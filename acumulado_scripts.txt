### SCRIPTS PYTHON CONSOLIDADOS



---
# Context Pack.py

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Genera un 'Context Pack' con todo lo necesario para que un proyecto nuevo
de ChatGPT conozca tu sistema sin partir de cero.

Crea una carpeta CONTEXT_PACK_YYYY-MM-DD con:
- repo_tree.txt                   (√°rbol de archivos)
- estructura_y_contenido_html.txt (todas las plantillas + contenido)
- endpoints_map.md                (urls ‚Üî views)
- estructura_actual_bd.csv        (esquema BD real via information_schema)
- vistas_fertilizantes.sql        (si existe, copia)
- acumulado_queries.txt           (si existe, copia)
- acumulado_scripts.txt           (contenido de /SCRIPTS/*.py)

Autor: Arturo (Sistema Fertilizantes)
"""

from pathlib import Path
import re
import shutil
import datetime as dt
import pandas as pd
from sqlalchemy import text

# === RUTAS PRINCIPALES ===
ROOT = Path("/Users/Arturo/AGRICULTURA/FERTILIZANTES/dashboard_web")
SCRIPTS_DIR = Path("/Users/Arturo/AGRICULTURA/FERTILIZANTES/SCRIPTS")
OUT = Path("/Users/Arturo/AGRICULTURA/FERTILIZANTES") / f"CONTEXT_PACK_{dt.date.today().isoformat()}"
OUT.mkdir(parents=True, exist_ok=True)

# Usa tu conexi√≥n ya estandarizada
from conexion import engine  # noqa: E402

def write_repo_tree():
    tree_file = OUT / "repo_tree.txt"
    lines = []
    for p in ROOT.rglob("*"):
        # Ignorar cach√©s y binarios pesados
        if any(part in p.parts for part in ["__pycache__", ".git", ".DS_Store", "env", "ENTORNO"]):
            continue
        rel = p.relative_to(ROOT)
        kind = "üìÅ" if p.is_dir() else "üìÑ"
        lines.append(f"{kind} {rel}")
    tree_file.write_text("\n".join(sorted(lines)), encoding="utf-8")

def dump_all_html():
    out_file = OUT / "estructura_y_contenido_html.txt"
    header = "üìÅ ESTRUCTURA DE ARCHIVOS HTML Y SU CONTENIDO\n\n"
    out_file.write_text(header, encoding="utf-8")
    html_glob = list(ROOT.rglob("fertilizantes/templates/**/*.html"))
    for html in sorted(html_glob):
        rel = html.relative_to(ROOT)
        try:
            content = html.read_text(encoding="utf-8", errors="ignore")
        except Exception:
            content = "(No se pudo leer con UTF-8)"
        block = (
            f"\n\nüîπ Ruta: {rel}\n"
            f"üìÑ Archivo: {html.name}\n"
            + "="*80 +
            f"\n{content}\n\n" + "="*80 + "\n"
        )
        with out_file.open("a", encoding="utf-8") as f:
            f.write(block)

def map_endpoints():
    """Extrae rutas de urls.py y mapea a views."""
    urls_py = ROOT / "dashboard" / "urls.py"
    fert_urls_py = ROOT / "fertilizantes" / "urls.py"
    views_py = ROOT / "fertilizantes" / "views.py"

    data = ["# Mapa de endpoints (urls ‚Üî views)\n"]
    for upath in [urls_py, fert_urls_py]:
        if upath.exists():
            txt = upath.read_text(encoding="utf-8", errors="ignore")
            data.append(f"\n## {upath.relative_to(ROOT)}\n")
            # patrones b√°sicos: path('ruta/', views.func, name='x')
            for m in re.finditer(r"path\(\s*['\"]([^'\"]+)['\"]\s*,\s*([a-zA-Z0-9_\.]+)", txt):
                ruta, destino = m.groups()
                data.append(f"- `{ruta}`  ‚Üí  `{destino}`")
    # Listado de vistas definidas
    if views_py.exists():
        txt = views_py.read_text(encoding="utf-8", errors="ignore")
        data.append(f"\n## {views_py.relative_to(ROOT)} (def vistas)\n")
        for m in re.finditer(r"^def\s+([a-zA-Z0-9_]+)\(", txt, flags=re.MULTILINE):
            data.append(f"- `def {m.group(1)}(...)`")

    (OUT / "endpoints_map.md").write_text("\n".join(data) + "\n", encoding="utf-8")

def copy_known_sql_and_queries():
    # Si tienes el archivo central de vistas, c√≤pialo
    for fname in ["vistas_fertilizantes.sql", "acumulado_queries.txt"]:
        src1 = ROOT / fname
        src2 = Path("/mnt/data") / fname  # por si lo mantienes fuera
        if src1.exists():
            shutil.copy2(src1, OUT / fname)
        elif src2.exists():
            shutil.copy2(src2, OUT / fname)

def dump_scripts_folder():
    out = OUT / "acumulado_scripts.txt"
    with out.open("w", encoding="utf-8") as f:
        f.write("### SCRIPTS PYTHON CONSOLIDADOS\n\n")
    if not SCRIPTS_DIR.exists():
        return
    for py in sorted(SCRIPTS_DIR.glob("*.py")):
        if py.name == "build_context_pack.py":
            continue
        content = py.read_text(encoding="utf-8", errors="ignore")
        block = f"\n\n---\n# {py.name}\n\n{content}\n"
        with out.open("a", encoding="utf-8") as f:
            f.write(block)

def export_db_schema():
    query = text("""
        SELECT
          table_name,
          column_name,
          data_type,
          is_nullable,
          COALESCE(character_maximum_length::text, '') AS char_len,
          ordinal_position
        FROM information_schema.columns
        WHERE table_schema='public'
        ORDER BY table_name, ordinal_position;
    """)
    df = pd.read_sql(query, engine)
    df.to_csv(OUT / "estructura_actual_bd.csv", index=False, encoding="utf-8")

def main():
    print(f"Generando Context Pack en: {OUT}")
    write_repo_tree()
    dump_all_html()
    map_endpoints()
    copy_known_sql_and_queries()
    dump_scripts_folder()
    export_db_schema()
    print("Listo ‚úÖ")

if __name__ == "__main__":
    main()


---
# Integrar_informe_2025.py

import xlwings as xw
from PIL import ImageGrab
from pptx import Presentation
from pptx.util import Cm
from pptx.enum.shapes import MSO_SHAPE_TYPE
import os
import time

# --- CONFIGURACIONES ---
ruta_excel = "/Users/Arturo/AGRICULTURA/INFORMES/INFORME_2025.xlsm"
ruta_pptx = "/Users/Arturo/AGRICULTURA/INFORMES/Avances Fertilizantes 2025.pptx"
carpeta_imgs = "/Users/Arturo/AGRICULTURA/INFORMES/Img_Temporales"
os.makedirs(carpeta_imgs, exist_ok=True)

# --- LISTA DE IM√ÅGENES DESDE EXCEL ---
imagenes_excel = [
    ("Avances Generales", "avance_nacional.png", "B4:Q17", 1, 1.15, 3.69, 31.57),
    ("En entregas", "en_entregas.png", "B3:M20", 5, 1.41, 3.34, 31.04),
    ("En entregas", "en_entregas_2.png", "B22:M38", 6, 1.41, 3.39, 31.04),
    ("Arranque Pr√≥ximo", "arranque_proximo.png", "B3:M6", 7, 1.41, 6.30, 31.04),
]

# --- LISTA DE IM√ÅGENES YA GENERADAS CON PYTHON ---
imagenes_python = [
    ("grafico_envios_diarios_powerbi.png", 2, 0.43, 3.15, 33),
    ("grafico_entregas_diarias.png", 3, 2.43, 2.83, 29),
    ("grafico_comparativo_abasto_entregas.png", 4, 2.93, 3.34, 28),
]

# --- PARTE 1: EXPORTAR IM√ÅGENES DESDE EXCEL ---
print("üì§ Exportando im√°genes desde Excel...")

app = xw.App(visible=False)
wb = xw.Book(ruta_excel)

def exportar_imagen(nombre_hoja, nombre_archivo, rango):
    try:
        macro = wb.macro("ExportarImagenDeHojaRango")
        macro(nombre_hoja, rango)

        time.sleep(1.5)
        imagen = ImageGrab.grabclipboard()
        if imagen:
            ruta_img = os.path.join(carpeta_imgs, nombre_archivo)
            imagen.save(ruta_img, "PNG")
            print(f"‚úÖ Imagen de '{nombre_hoja}' guardada como: {ruta_img}")
        else:
            print(f"‚ùå No se encontr√≥ imagen en portapapeles para '{nombre_hoja}'.")
    except Exception as e:
        print(f"‚ùå Error con la hoja '{nombre_hoja}': {e}")

for hoja, archivo, rango, _, _, _, _ in imagenes_excel:
    exportar_imagen(hoja, archivo, rango)

wb.close()
app.quit()

# --- PARTE 2: INSERTAR IM√ÅGENES EN POWERPOINT ---
print("\nüì• Insertando im√°genes en PowerPoint...")

if not os.path.exists(ruta_pptx):
    print("‚ùå No se encontr√≥ la presentaci√≥n.")
else:
    ppt = Presentation(ruta_pptx)

    # Combinar ambas listas de im√°genes
    imagenes_todas = [
        *[(archivo, slide, izquierda, arriba, ancho) for _, archivo, _, slide, izquierda, arriba, ancho in imagenes_excel],
        *imagenes_python
    ]

    for archivo, slide_idx, izquierda_cm, arriba_cm, ancho_cm in imagenes_todas:
        ruta_img = os.path.join(carpeta_imgs, archivo)

        if not os.path.exists(ruta_img):
            print(f"‚ùå No se encontr√≥ la imagen: {ruta_img}")
            continue

        if len(ppt.slides) <= slide_idx:
            print(f"‚ùå La presentaci√≥n no tiene un slide en el √≠ndice {slide_idx + 1}.")
            continue

        slide = ppt.slides[slide_idx]

        # Eliminar im√°genes anteriores del slide
        for shape in list(slide.shapes):
            if shape.shape_type == MSO_SHAPE_TYPE.PICTURE:
                sp = shape._element
                sp.getparent().remove(sp)

        # Insertar nueva imagen
        slide.shapes.add_picture(
            ruta_img,
            Cm(izquierda_cm),
            Cm(arriba_cm),
            width=Cm(ancho_cm)
        )

        print(f"‚úÖ Imagen '{archivo}' insertada en slide {slide_idx + 1}.")

    ppt.save(ruta_pptx)
    print("\nüíæ Presentaci√≥n actualizada correctamente.")



---
# Integrar_informe_2025_old.py

import xlwings as xw
from PIL import ImageGrab
from pptx import Presentation
from pptx.util import Cm
from pptx.enum.shapes import MSO_SHAPE_TYPE
import os
import time

# --- CONFIGURACIONES ---

ruta_excel = "/Users/Arturo/AGRICULTURA/INFORMES/INFORME_2025.xlsm"
ruta_pptx = "/Users/Arturo/AGRICULTURA/INFORMES/Avances Fertilizantes 2025.pptx"
carpeta_imgs = "/Users/Arturo/AGRICULTURA/INFORMES/Img_Temporales"
os.makedirs(carpeta_imgs, exist_ok=True)

# Lista de hojas, archivos, rangos y slides
hojas_con_info = [
    ("Avances Generales", "avance_nacional.png", "B4:Q17", 1),
    ("Guerrero Avances", "guerrero_avances.png", "B5:Q18", 9),
    ("Durango Avances", "durango_avances.png", "B5:Q18", 7),
    ("Michoac√°n Avances", "michoacan_avances.png", "B5:Q18", 11),
    ("Morelos Avances", "morelos_avances.png", "B5:Q18", 13),
    ("Tlaxcala Avances", "tlaxcala_avances.png", "B5:Q18", 15),
    ("Veracruz Avances", "veracruz_avances.png", "B5:Q18", 18),
    ("Chiapas Avances", "chiapas_avances.png", "B5:Q18", 20),
    ("Tabasco Avances", "tabasco_avances.png", "B5:Q18", 23),
    ("Colima Avances", "colima_avances.png", "B5:Q18", 24),
    ("Edomex Avances", "edomex_avances.png", "B5:Q18", 25),
    ("Cdmx Avances", "cdmx_avanes.png", "B5:Q18", 26),
    ("Guanajuato Avances", "guanajuato_avances.png", "B5:Q18", 27),
    ("Jalisco Avances", "jalisco_avances.png", "B5:Q18", 28),
    ("Puebla Avances", "puebla_avances.png", "B5:Q18", 29),
    ("Hidalgo Avances", "hidalgo_avances.png", "B5:Q18", 30),
    ("Campeche Avances", "campeche_avances.png", "B5:Q18", 31),
    ("Yucat√°n Avances", "yucatan_avances.png", "B5:Q18", 32),
    ("Quintana Roo Avances", "quintana_roo_avances.png", "B5:Q18", 33),
    ("Oaxaca Avances", "oaxaca_avances.png", "B5:Q18", 34)
]

# --- PARTE 1: Exportar im√°genes desde Excel ---

print("üì§ Exportando im√°genes desde Excel...")

app = xw.App(visible=False)
wb = xw.Book(ruta_excel)

def exportar_imagen(nombre_hoja, nombre_archivo, rango):
    try:
        macro = wb.macro("ExportarImagenDeHojaRango")
        macro(nombre_hoja, rango)

        time.sleep(1.5)
        imagen = ImageGrab.grabclipboard()
        if imagen:
            ruta_img = os.path.join(carpeta_imgs, nombre_archivo)
            imagen.save(ruta_img, "PNG")
            print(f"‚úÖ Imagen de '{nombre_hoja}' guardada como: {ruta_img}")
        else:
            print(f"‚ùå No se encontr√≥ imagen en portapapeles para '{nombre_hoja}'.")

    except Exception as e:
        print(f"‚ùå Error con la hoja '{nombre_hoja}': {e}")

for hoja, archivo, rango, _ in hojas_con_info:
    exportar_imagen(hoja, archivo, rango)

wb.close()
app.quit()

# --- PARTE 2: Insertar im√°genes en PowerPoint ---

print("\nüì• Insertando im√°genes en PowerPoint...")

if not os.path.exists(ruta_pptx):
    print("‚ùå No se encontr√≥ la presentaci√≥n.")
else:
    ppt = Presentation(ruta_pptx)

    for _, archivo, _, slide_idx in hojas_con_info:
        ruta_img = os.path.join(carpeta_imgs, archivo)

        if not os.path.exists(ruta_img):
            print(f"‚ùå No se encontr√≥ la imagen: {ruta_img}")
            continue

        if len(ppt.slides) <= slide_idx:
            print(f"‚ùå La presentaci√≥n no tiene un slide en el √≠ndice {slide_idx + 1}.")
            continue

        slide = ppt.slides[slide_idx]

        # Eliminar im√°genes anteriores del slide
        for shape in list(slide.shapes):
            if shape.shape_type == MSO_SHAPE_TYPE.PICTURE:
                sp = shape._element
                sp.getparent().remove(sp)

        # Insertar nueva imagen
        slide.shapes.add_picture(
            ruta_img,
            Cm(1.15),
            Cm(4.03),
            width=Cm(31.57),
            height=Cm(11)
        )

        print(f"‚úÖ Imagen '{archivo}' insertada en slide {slide_idx + 1}.")

    ppt.save(ruta_pptx)
    print("\nüíæ Presentaci√≥n actualizada correctamente.")



---
# actualizar_directorio_coz_rc_red_2025.py

#!/usr/bin/env python3
"""
actualizar_directorio_red.py
Complementa/directorio con:
  1) RESPONSABLE DE CEDA (uno por id_ceda_agricultura)
  2) COORDINADOR OPERATIVO DE ZONA (uno por estado+zona_operativa)
Flujo:
  - Borra del directorio ambos cargos
  - Vuelve a generarlos desde red_distribucion
Uso:
  $ python actualizar_directorio_red.py
Requiere: conexion.py (exporta `engine`)
"""
import pandas as pd
from sqlalchemy import text
from conexion import engine

# Consulta base sobre red_distribucion
Q = """
SELECT
  estado,
  coordinacion_estatal      AS unidad_operativa,
  zona_operativa,
  id_ceda_agricultura,
  responsable_cedas,
  rc_correo,
  rc_telefono,
  coordinador_operativo,
  coz_correo,
  coz_telefono
FROM red_distribucion;
"""

import pandas as pd

import pandas as pd

import pandas as pd

def build_responsables(df_red):
    # 1) Filtramos todos los CEDAs con id_ceda_agricultura (aunque responsable_cedas est√© vac√≠o)
    df = df_red.loc[
        df_red["id_ceda_agricultura"].notna(),
        [
            "estado",
            "unidad_operativa",   # tu alias de coordinacion_estatal
            "zona_operativa",
            "id_ceda_agricultura",
            "responsable_cedas",
            "rc_correo",
            "rc_telefono"
        ]
    ]

    # 2) A√±adimos las columnas fijas y mapeamos responsable_cedas ‚Üí nombre_completo, etc.
    df = df.assign(
        cargo="RESPONSABLE DE CEDA",
        curp=None,
        nombre_completo=df["responsable_cedas"].fillna(""),
        correo_electronico=df["rc_correo"].fillna(""),
        telefono=df["rc_telefono"].fillna(""),
        comentarios=None,
        fecha_actualizacion=pd.Timestamp.now()
    )

    # 3) Seleccionamos en una lista EXACTA las columnas finales en el orden deseado
    final_cols = [
        "estado",
        "unidad_operativa",
        "zona_operativa",
        "id_ceda_agricultura",
        "cargo",
        "curp",
        "nombre_completo",
        "correo_electronico",
        "telefono",
        "comentarios",
        "fecha_actualizacion"
    ]
    return df[final_cols]


def build_coordinadores(df_red):
    # Tomar una sola fila por (estado,zona_operativa)
    df = (
        df_red.dropna(subset=["coordinador_operativo"])
              .drop_duplicates(subset=["estado","zona_operativa"], keep="first")
    )
    return df.assign(
        unidad_operativa = df["unidad_operativa"],
        id_ceda_agricultura = "N/A",
        cargo="COORDINADOR OPERATIVO DE ZONA",
        curp=None,
        nombre_completo=df["coordinador_operativo"],
        correo_electronico=df["coz_correo"],
        telefono=df["coz_telefono"],
        comentarios=None,
        fecha_actualizacion=pd.Timestamp.now()
    )[
        ["estado","unidad_operativa","zona_operativa",
         "id_ceda_agricultura","cargo","curp",
         "nombre_completo","correo_electronico",
         "telefono","comentarios","fecha_actualizacion"]
    ]

def main():
    # 1) Leer tabla red_distribucion
    df_red = pd.read_sql(Q, engine)

    # 2) Generar DataFrames
    df_rc  = build_responsables(df_red)
    df_coz = build_coordinadores(df_red)
    df_all = pd.concat([df_rc, df_coz], ignore_index=True)

    with engine.begin() as conn:
        # A) Eliminar registros antiguos
        conn.execute(text("""
            DELETE FROM directorio
              WHERE cargo IN
                ('RESPONSABLE DE CEDA','COORDINADOR OPERATIVO DE ZONA');
        """))
        # B) Insertar los nuevos
        df_all.to_sql(
            "directorio",
            conn,
            if_exists="append",
            index=False,
            method="multi",
            chunksize=5_000
        )

    print(f"üîÑ Directorio red_distribucion actualizado: {len(df_all):,} filas agregadas.")

if __name__ == "__main__":
    main()



---
# actualizar_superficie_apoyada.py

# actualizar_superficie_apoyada.py

from sqlalchemy import text
from conexion import engine

def actualizar_superficie():
    print("üõ†Ô∏è Actualizando superficie_apoyada = 1 para CHIAPAS y OAXACA...\n")
    try:
        with engine.begin() as conn:
            result = conn.execute(text("""
                UPDATE derechohabientes
                SET superficie_apoyada = 1
                WHERE estado_predio_capturada IN ('CHIAPAS', 'OAXACA')
                  AND superficie_apoyada IS DISTINCT FROM 1;
            """))
            print(f"‚úÖ Registros actualizados: {result.rowcount}\n")
    except Exception as e:
        print(f"‚ùå Error al actualizar superficie_apoyada: {e}")

if __name__ == "__main__":
    actualizar_superficie()


---
# actualizar_todo.py

import os
import time
import subprocess

# Ruta a la carpeta de scripts
RUTA_SCRIPTS = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/SCRIPTS"

# Scripts de truncado (debe ir primero)
scripts_de_limpieza = [
    "truncar_tablas.py"
]

# Scripts de importaci√≥n en orden
scripts_importacion = [
    "importar_red.py",
    "importar_pedidos_desglosado.py",
    "importar_pedidos_sigap.py",
    "importar_fletes.py",
    "importar_transferencias.py",
    "importar_remanentes.py",
    "importar_incidencias.py",
    "importar_derechohabientes.py",
    "importar_inventarios_campo_2025_new.py"
]

# Scripts de exportaci√≥n
scripts_exportacion = [
    "exportar_entregas_td.py",
    "exportar_fletes_conteo_td.py",
    "exportar_fletes_fechas_td.py",
    "exportar_fletes_ton_td.py",
    "exportar_incidencias_td.py",
    "exportar_pedidos_sigap_td.py",
    "exportar_pedidos_td.py",
    "exportar_red_td.py",
    "exportar_remanentes_td.py",
    "exportar_transferencias_td.py",
    "exportar_abasto_x_estado_2025.py",
    "exportar_avances_2025_td.py",
    "exportar_entregas_diarias_2025.py",
    "exportar_envios_diarios_2025.py",
    "exportar_resumen_derechohabientes_apoyados_x_municipio.py",
    "exportar_abasto_y_remanente_x_dia_2025.py",
    "exportar_pedidos_por_dia_mas_remanentes.py",
    "exportar_abasto_y_remanente_por_dia_sin_transito_2025.py",
    "exportar_entregas_por_estado_y_genero.py",
    "exportar_entregas_semanales_2025.py",
    "exportar_pedidos_por_dia.py",
    "exportar_reporte_derechohabientes_bienestar_td.py",
    "grafico_Abasto Proyectado vs Real vs Pedido vs Entregado.py",
    "grafico_entregas_semanales_2025.py",
    "grafico_entregas_diarias.py",
    "grafico_envios_diarios.py",
    "grafico_recibido_vs_entregado.py",
    "exportar_reporte_fletes_bienestar.py",
    "exportar_entregas_x_estado_no_ceda_y_fecha_2025.py",
    "exportar_avance_operativo_ceda_2025.py"
]

# Combinaci√≥n de scripts: limpieza + importaci√≥n
scripts_preparacion = scripts_de_limpieza + scripts_importacion

# ------------------------------------------------------------------------
print("üöÄ Iniciando procedimiento completo de actualizaci√≥n...\n")
inicio = time.time()

errores = []

# Ejecutar scripts de limpieza e importaci√≥n
for script in scripts_preparacion:
    ruta_script = os.path.join(RUTA_SCRIPTS, script)
    print(f"‚ñ∂Ô∏è Ejecutando: {script}...")
    try:
        resultado = subprocess.run(
            ["/Users/Arturo/AGRICULTURA/FERTILIZANTES/ENTORNO/env/bin/python", ruta_script],
            capture_output=True,
            text=True
        )
        print(resultado.stdout)
        if resultado.stderr:
            print(f"‚ö†Ô∏è Advertencia en {script}:\n{resultado.stderr}")
            errores.append((script, resultado.stderr.strip()))
        print(f"‚úÖ Finalizado: {script}\n")
    except Exception as e:
        error_msg = f"{script}: {str(e)}"
        print(f"‚ùå Error al ejecutar {error_msg}")
        errores.append((script, str(e)))

# -----------------------------------------------------------
# CONEXI√ìN A LA BASE DE DATOS Y ACTUALIZACI√ìN DE DATOS
# -----------------------------------------------------------
from sqlalchemy import create_engine, text
import pandas as pd

print("üîÅ Conectando a la base de datos...\n")
engine = create_engine("postgresql://postgres:Art4125r0@localhost:5432/fertilizantes")

# Inicializar lista de errores si no existe
errores = []

# Actualizar superficie_apoyada = 1 para CHIAPAS y OAXACA
try:
    print("üõ†Ô∏è Actualizando superficie_apoyada = 1 para CHIAPAS y OAXACA...\n")
    with engine.begin() as conn:
        result = conn.execute(text("""
            UPDATE derechohabientes
            SET superficie_apoyada = 1
            WHERE estado_predio_capturada IN ('CHIAPAS', 'OAXACA')
              AND superficie_apoyada IS DISTINCT FROM 1;
        """))
        print(f"‚úÖ Registros actualizados (CHIAPAS y OAXACA): {result.rowcount}\n")
except Exception as e:
    print(f"‚ùå Error al actualizar superficie_apoyada para CHIAPAS y OAXACA: {e}")
    errores.append(("UPDATE superficie_apoyada CHIAPAS/OAXACA", str(e)))

# Actualizar superficie_apoyada para SINALOA usando tabla derechohabientes_padrones_2025
try:
    print("üõ†Ô∏è Actualizando superficie_apoyada desde padrones para SINALOA...\n")
    with engine.begin() as conn:
        result = conn.execute(text("""
            UPDATE derechohabientes d
            SET superficie_apoyada = p.superficie_apoyada
            FROM derechohabientes_padrones_2025 p
            WHERE d.estado_predio_capturada = 'SINALOA'
              AND d.acuse_estatal = p.acuse_estatal
              AND d.superficie_apoyada IS DISTINCT FROM p.superficie_apoyada;
        """))
        print(f"‚úÖ Registros actualizados (SINALOA): {result.rowcount}\n")
except Exception as e:
    print(f"‚ùå Error al actualizar superficie_apoyada para SINALOA: {e}")
    errores.append(("UPDATE superficie_apoyada SINALOA", str(e)))

# -----------------------------------------------------------
# REFRESCAR VISTAS MATERIALIZADAS
# -----------------------------------------------------------
vistas_materializadas = [
    "avance_operativo_ceda_2025",
    "entregas_diarias_x_estado_ceda_2025",
    "entregas_x_estado_no_ceda_y_fecha_2025",
    "inventario_acumulado_x_ceda_diario_2025",
    "inventario_remanente_x_ceda_diario_2025",
    "inventario_remanente_x_ceda_2025",
    "inventario_ceda_diario_2025_campo_sigap",
    "estadisticas_inventarios_campo",
]

try:
    print("üîÅ Refrescando vistas materializadas...\n")
    with engine.begin() as conn:
        for vista in vistas_materializadas:
            print(f"üîÅ Refrescando vista materializada {vista}...")
            conn.execute(text(f"REFRESH MATERIALIZED VIEW {vista};"))
            print(f"‚úÖ Vista {vista} actualizada correctamente.\n")
except Exception as e:
    print(f"‚ùå Error al refrescar vistas materializadas: {e}")
    errores.append(("REFRESH vistas materializadas", str(e)))

# Ejecutar scripts de exportaci√≥n y gr√°ficos
for script in scripts_exportacion:
    ruta_script = os.path.join(RUTA_SCRIPTS, script)
    print(f"‚ñ∂Ô∏è Ejecutando: {script}...")
    try:
        resultado = subprocess.run(
            ["/Users/Arturo/AGRICULTURA/FERTILIZANTES/ENTORNO/env/bin/python", ruta_script],
            capture_output=True,
            text=True
        )
        print(resultado.stdout)
        if resultado.stderr:
            print(f"‚ö†Ô∏è Advertencia en {script}:\n{resultado.stderr}")
            errores.append((script, resultado.stderr.strip()))
        print(f"‚úÖ Finalizado: {script}\n")
    except Exception as e:
        error_msg = f"{script}: {str(e)}"
        print(f"‚ùå Error al ejecutar {error_msg}")
        errores.append((script, str(e)))

fin = time.time()

# -----------------------------------------------------------
# RESUMEN FINAL
# -----------------------------------------------------------
print("\nüìã Resumen final de la ejecuci√≥n:\n")

if errores:
    print("‚ùå Se detectaron errores en los siguientes scripts:")
    for i, (script, msg) in enumerate(errores, 1):
        print(f"{i}. {script} ‚û§ {msg}")
else:
    print("‚úÖ Todos los scripts se ejecutaron correctamente.")

print(f"\n‚è±Ô∏è Tiempo total de ejecuci√≥n: {round(fin - inicio, 2)} segundos.")


---
# actualizar_todo_nvo.py

import os
import time
import subprocess

# Ruta a la carpeta de scripts
RUTA_SCRIPTS = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/SCRIPTS"

# Scripts de truncado (debe ir primero)
scripts_de_limpieza = [
    "truncar_tablas.py"
]

# Scripts de importaci√≥n en orden
scripts_importacion = [
    "importar_red.py",
    "importar_pedidos_desglosado.py",
    "importar_pedidos_sigap.py",
    "importar_fletes.py",
    "importar_transferencias.py",
    "importar_remanentes.py",
    "importar_incidencias.py",
    "importar_derechohabientes.py"
]

# Scripts de exportaci√≥n
scripts_exportacion = [
    "exportar_entregas_td.py",
    "exportar_fletes_conteo_td.py",
    "exportar_fletes_fechas_td.py",
    "exportar_fletes_ton_td.py",
    "exportar_incidencias_td.py",
    "exportar_pedidos_sigap_td.py",
    "exportar_pedidos_td.py",
    "exportar_red_td.py",
    "exportar_remanentes_td.py",
    "exportar_transferencias_td.py",
    "exportar_abasto_x_estado_2025.py",
    "exportar_avances_2025_td.py",
    "exportar_entregas_diarias_2025.py",
    "exportar_envios_diarios_2025.py",
    "exportar_resumen_derechohabientes_apoyados_x_municipio.py",
    "exportar_abasto_y_remanente_x_dia_2025.py",
    "exportar_pedidos_por_dia_mas_remanentes.py",
    "exportar_abasto_y_remanente_por_dia_sin_transito_2025.py",
    "exportar_entregas_por_estado_y_genero.py",
    "exportar_entregas_semanales_2025.py",
    "exportar_pedidos_por_dia.py",
    "exportar_reporte_derechohabientes_bienestar_td.py",
    "grafico_Abasto Proyectado vs Real vs Pedido vs Entregado.py",
    "grafico_entregas_semanales_2025.py",
    "grafico_envios_diarios.py",
    "grafico_recibido_vs_entregado.py",
    "exportar_reporte_fletes_bienestar.py",
    "exportar_entregas_x_estado_no_ceda_y_fecha_2025.py",
    "exportar_avance_operativo_ceda_2025.py"
]

# Combinaci√≥n de scripts: limpieza + importaci√≥n
scripts_preparacion = scripts_de_limpieza + scripts_importacion

# ------------------------------------------------------------------------
print("üöÄ Iniciando procedimiento completo de actualizaci√≥n...\n")
inicio = time.time()

errores = []

# Ejecutar scripts de limpieza e importaci√≥n
for script in scripts_preparacion:
    ruta_script = os.path.join(RUTA_SCRIPTS, script)
    print(f"‚ñ∂Ô∏è Ejecutando: {script}...")
    try:
        resultado = subprocess.run(
            ["/Users/Arturo/AGRICULTURA/FERTILIZANTES/ENTORNO/env/bin/python", ruta_script],
            capture_output=True,
            text=True
        )
        print(resultado.stdout)
        if resultado.stderr:
            print(f"‚ö†Ô∏è Advertencia en {script}:\n{resultado.stderr}")
            errores.append((script, resultado.stderr.strip()))
        print(f"‚úÖ Finalizado: {script}\n")
    except Exception as e:
        error_msg = f"{script}: {str(e)}"
        print(f"‚ùå Error al ejecutar {error_msg}")
        errores.append((script, str(e)))

# -----------------------------------------------------------
# CONEXI√ìN A LA BASE DE DATOS Y ACTUALIZACI√ìN DE DATOS
# -----------------------------------------------------------
from sqlalchemy import create_engine, text

print("üîÅ Conectando a la base de datos...\n")
engine = create_engine("postgresql://postgres:Art4125r0@localhost:5432/fertilizantes")

# Actualizar superficie_apoyada para CHIAPAS y OAXACA
try:
    print("üõ†Ô∏è Actualizando superficie_apoyada = 1 para CHIAPAS y OAXACA...\n")
    with engine.begin() as conn:
        result = conn.execute(text("""
            UPDATE derechohabientes
            SET superficie_apoyada = 1
            WHERE estado_predio_capturada IN ('CHIAPAS', 'OAXACA')
              AND superficie_apoyada IS DISTINCT FROM 1;
        """))
        print(f"‚úÖ Registros actualizados: {result.rowcount}\n")
except Exception as e:
    print(f"‚ùå Error al actualizar superficie_apoyada: {e}")
    errores.append(("UPDATE superficie_apoyada", str(e)))

# -----------------------------------------------------------
# REFRESCAR VISTAS MATERIALIZADAS
# -----------------------------------------------------------
vistas_materializadas = [
    "avance_operativo_ceda_2025",
    "entregas_diarias_x_estado_ceda_2025",
    "entregas_x_estado_no_ceda_y_fecha_2025",
    "inventario_acumulado_x_ceda_diario_2025",
    "inventario_remanente_x_ceda_diario_2025",
    "inventario_remanente_x_ceda_2025",
]

try:
    print("üîÅ Refrescando vistas materializadas...\n")
    with engine.begin() as conn:
        for vista in vistas_materializadas:
            print(f"üîÅ Refrescando vista materializada {vista}...")
            conn.execute(text(f"REFRESH MATERIALIZED VIEW {vista};"))
            print(f"‚úÖ Vista {vista} actualizada correctamente.\n")
except Exception as e:
    print(f"‚ùå Error al refrescar vistas materializadas: {e}")
    errores.append(("REFRESH vistas materializadas", str(e)))

# Ejecutar scripts de exportaci√≥n y gr√°ficos
for script in scripts_exportacion:
    ruta_script = os.path.join(RUTA_SCRIPTS, script)
    print(f"‚ñ∂Ô∏è Ejecutando: {script}...")
    try:
        resultado = subprocess.run(
            ["/Users/Arturo/AGRICULTURA/FERTILIZANTES/ENTORNO/env/bin/python", ruta_script],
            capture_output=True,
            text=True
        )
        print(resultado.stdout)
        if resultado.stderr:
            print(f"‚ö†Ô∏è Advertencia en {script}:\n{resultado.stderr}")
            errores.append((script, resultado.stderr.strip()))
        print(f"‚úÖ Finalizado: {script}\n")
    except Exception as e:
        error_msg = f"{script}: {str(e)}"
        print(f"‚ùå Error al ejecutar {error_msg}")
        errores.append((script, str(e)))

fin = time.time()

# -----------------------------------------------------------
# RESUMEN FINAL
# -----------------------------------------------------------
print("\nüìã Resumen final de la ejecuci√≥n:\n")

if errores:
    print("‚ùå Se detectaron errores en los siguientes scripts:")
    for i, (script, msg) in enumerate(errores, 1):
        print(f"{i}. {script} ‚û§ {msg}")
else:
    print("‚úÖ Todos los scripts se ejecutaron correctamente.")

print(f"\n‚è±Ô∏è Tiempo total de ejecuci√≥n: {round(fin - inicio, 2)} segundos.")



---
# actualizar_todo_v2.py

import os
import time
import subprocess

# Ruta a la carpeta de scripts
RUTA_SCRIPTS = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/SCRIPTS"

# Scripts de truncado (debe ir primero)
scripts_de_limpieza = [
    "truncar_tablas.py"
]

# Scripts de importaci√≥n en orden
scripts_importacion = [
    "importar_red.py",
    "importar_pedidos_desglosado.py",
    "importar_pedidos_sigap.py",
    "importar_fletes.py",
    "importar_transferencias.py",
    "importar_remanentes.py",
    "importar_incidencias.py",
    "importar_derechohabientes.py"
]

# Scripts de exportaci√≥n
scripts_exportacion = [
    "exportar_entregas_td.py",
    "exportar_fletes_conteo_td.py",
    "exportar_fletes_fechas_td.py",
    "exportar_fletes_ton_td.py",
    "exportar_incidencias_td.py",
    "exportar_pedidos_sigap_td.py",
    "exportar_pedidos_td.py",
    "exportar_red_td.py",
    "exportar_remanentes_td.py",
    "exportar_transferencias_td.py",
    "exportar_abasto_x_estado_2025.py",
    "exportar_avances_2025_td.py",
    "exportar_entregas_diarias_2025.py",
    "exportar_envios_diarios_2025.py",
    "exportar_resumen_derechohabientes_apoyados_x_municipio.py",
    "exportar_abasto_y_remanente_x_dia_2025.py",
    "exportar_pedidos_por_dia_mas_remanentes.py",
    "exportar_abasto_y_remanente_por_dia_sin_transito_2025.py",
    "exportar_entregas_por_estado_y_genero.py",
    "exportar_entregas_semanales_2025.py",
    "exportar_pedidos_por_dia.py",
    "exportar_reporte_derechohabientes_bienestar_td.py",
    "grafico_Abasto Proyectado vs Real vs Pedido vs Entregado.py",
    "grafico_entregas_semanales_2025.py",
    "grafico_envios_diarios.py",
    "grafico_recibido_vs_entregado.py",
    "exportar_reporte_fletes_bienestar.py",
    "exportar_entregas_x_estado_no_ceda_y_fecha_2025.py",
    "exportar_avance_operativo_ceda_2025.py"
]

# Combinaci√≥n de scripts: limpieza + importaci√≥n
scripts_preparacion = scripts_de_limpieza + scripts_importacion

# ------------------------------------------------------------------------
print("üöÄ Iniciando procedimiento completo de actualizaci√≥n...\n")
inicio = time.time()

errores = []

# Ejecutar scripts de limpieza e importaci√≥n
for script in scripts_preparacion:
    ruta_script = os.path.join(RUTA_SCRIPTS, script)
    print(f"‚ñ∂Ô∏è Ejecutando: {script}...")
    try:
        resultado = subprocess.run(
            ["/Users/Arturo/AGRICULTURA/FERTILIZANTES/ENTORNO/env/bin/python", ruta_script],
            capture_output=True,
            text=True
        )
        print(resultado.stdout)
        if resultado.stderr:
            print(f"‚ö†Ô∏è Advertencia en {script}:\n{resultado.stderr}")
            errores.append((script, resultado.stderr.strip()))
        print(f"‚úÖ Finalizado: {script}\n")
    except Exception as e:
        error_msg = f"{script}: {str(e)}"
        print(f"‚ùå Error al ejecutar {error_msg}")
        errores.append((script, str(e)))

# -----------------------------------------------------------
# REFRESCAR VISTAS MATERIALIZADAS
# -----------------------------------------------------------
from sqlalchemy import create_engine, text

vistas_materializadas = [
    "avance_operativo_ceda_2025",
    "entregas_diarias_x_estado_ceda_2025",
    "entregas_x_estado_no_ceda_y_fecha_2025",
    "inventario_acumulado_x_ceda_diario_2025",
    "inventario_remanente_x_ceda_diario_2025",
    "inventario_remanente_x_ceda_2025",
]

try:
    print("üîÅ Refrescando vistas materializadas...\n")
    engine = create_engine("postgresql://postgres:Art4125r0@localhost:5432/fertilizantes")
    with engine.begin() as conn:
        for vista in vistas_materializadas:
            print(f"üîÅ Refrescando vista materializada {vista}...")
            conn.execute(text(f"REFRESH MATERIALIZED VIEW {vista};"))
            print(f"‚úÖ Vista {vista} actualizada correctamente.\n")
except Exception as e:
    print(f"‚ùå Error al refrescar vistas materializadas: {e}")
    errores.append(("REFRESH vistas materializadas", str(e)))

# Ejecutar scripts de exportaci√≥n y gr√°ficos
for script in scripts_exportacion:
    ruta_script = os.path.join(RUTA_SCRIPTS, script)
    print(f"‚ñ∂Ô∏è Ejecutando: {script}...")
    try:
        resultado = subprocess.run(
            ["/Users/Arturo/AGRICULTURA/FERTILIZANTES/ENTORNO/env/bin/python", ruta_script],
            capture_output=True,
            text=True
        )
        print(resultado.stdout)
        if resultado.stderr:
            print(f"‚ö†Ô∏è Advertencia en {script}:\n{resultado.stderr}")
            errores.append((script, resultado.stderr.strip()))
        print(f"‚úÖ Finalizado: {script}\n")
    except Exception as e:
        error_msg = f"{script}: {str(e)}"
        print(f"‚ùå Error al ejecutar {error_msg}")
        errores.append((script, str(e)))

fin = time.time()

# -----------------------------------------------------------
# RESUMEN FINAL
# -----------------------------------------------------------
print("\nüìã Resumen final de la ejecuci√≥n:\n")

if errores:
    print("‚ùå Se detectaron errores en los siguientes scripts:")
    for i, (script, msg) in enumerate(errores, 1):
        print(f"{i}. {script} ‚û§ {msg}")
else:
    print("‚úÖ Todos los scripts se ejecutaron correctamente.")

print(f"\n‚è±Ô∏è Tiempo total de ejecuci√≥n: {round(fin - inicio, 2)} segundos.")



---
# acuses_no_sincronizados.py

import os
import pandas as pd
import glob
from datetime import datetime

# Rutas
carpeta_derechohabientes = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/derechohabientes/"
archivo_no_sincronizados = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/CORRECCIONES DERECHOHABIENTES/derechohabientes_no_sincronizados_nacional.xlsx"
carpeta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/CORRECCIONES DERECHOHABIENTES/"

# 1Ô∏è‚É£ Combinar archivos derechohabientes
archivos_csv = glob.glob(os.path.join(carpeta_derechohabientes, "*.CSV"))

df_derechohabientes = pd.concat(
    (
        pd.read_csv(archivo, encoding="utf-8", delimiter=",", dtype={12: str})
        .rename(columns=lambda col: col.lower().strip())
        for archivo in archivos_csv
    ),
    ignore_index=True
)

# Asegurar columna clave en min√∫sculas
if "acuse_estatal" not in df_derechohabientes.columns:
    raise ValueError("‚ùå No se encontr√≥ la columna 'acuse_estatal' en los derechohabientes.")

acuses_bd = set(df_derechohabientes["acuse_estatal"].dropna().astype(str).str.strip())

# 2Ô∏è‚É£ Leer archivo de no sincronizados
df_no_sync = pd.read_excel(archivo_no_sincronizados, dtype=str)
df_no_sync.columns = df_no_sync.columns.str.strip().str.lower()

# Buscar la columna correcta por nombre o posici√≥n
if "acuse_estatal" in df_no_sync.columns:
    acuses_archivo = df_no_sync["acuse_estatal"].astype(str).str.strip()
else:
    acuses_archivo = df_no_sync.iloc[:, 3].astype(str).str.strip()  # columna D es la 4ta (√≠ndice 3)

# 3Ô∏è‚É£ Identificar los acuses NO presentes
acuses_faltantes = acuses_archivo[~acuses_archivo.isin(acuses_bd)].drop_duplicates().reset_index(drop=True)

# 4Ô∏è‚É£ Exportar resultado
timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M")
nombre_salida = f"acuses no sincronizados sigap_{timestamp}.csv"
ruta_salida = os.path.join(carpeta_salida, nombre_salida)

acuses_faltantes.to_csv(ruta_salida, index=False, header=["acuse_estatal"], encoding="utf-8")

print(f"‚úÖ Acuses faltantes guardados en: {ruta_salida}")
print(f"üìä Total identificados: {len(acuses_faltantes)}")


---
# analizar_archivo_derechohabientes_coregidos_2025.py

import pandas as pd

archivo = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/derechohabientes_corregidos_2025.csv"

df = pd.read_csv(archivo, encoding="utf-8", dtype=str)
df.columns = df.columns.str.lower().str.strip()

if "fecha_entrega" in df.columns:
    print("üóìÔ∏è Primeros valores de 'fecha_entrega':")
    print(df["fecha_entrega"].dropna().head(10))
else:
    print("‚ùå La columna 'fecha_entrega' no se encuentra en el archivo.")



---
# analizar_archivo_fletes-corregidos.py

import pandas as pd

# Ruta del archivo de corregidos
archivo_corregidos = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/fletes_corregidos.csv"

# Leer el archivo como texto
df = pd.read_csv(archivo_corregidos, dtype=str, encoding="utf-8")

# Revisar los primeros valores de las columnas de fecha
columnas_fecha = ["fecha_de_salida", "fecha_de_llegada", "fecha_de_entrega"]

for col in columnas_fecha:
    if col in df.columns:
        print(f"\nüïí Primeros valores de la columna '{col}':")
        print(df[col].dropna().head(10))



---
# analizar_archivo_fletes.py

import pandas as pd
import glob
import os

# ‚úÖ Definir correctamente la ruta
ruta_archivo = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP"

# ‚úÖ Buscar archivo CSV espec√≠fico basado en el patr√≥n de nombre
archivos_csv = glob.glob(os.path.join(ruta_archivo, "*FERTILIZANTES-FLETES-NACIONAL-ANUAL*.csv"))

if len(archivos_csv) == 0:
    raise FileNotFoundError("No se encontr√≥ ning√∫n archivo con ese patr√≥n.")
else:
    archivo_csv = archivos_csv[0]

print(f"üìÇ Archivo analizado: {archivo_csv}")

# ‚úÖ Leer archivo CSV
df = pd.read_csv(archivo_csv, dtype=str)

# ‚úÖ Funci√≥n robusta para detectar tipos de dato predominantes
def detectar_tipo_dato(serie):
    if serie.dropna().str.match(r'^\d+$').all():
        return 'INTEGER'
    elif serie.dropna().str.match(r'^\d+\.\d+$').all():
        return 'NUMERIC(10,2)'
    elif pd.to_datetime(serie.dropna(), format='%Y-%m-%d %H:%M:%S', errors='coerce').notna().all():
        return 'TIMESTAMP'
    elif pd.to_datetime(serie.dropna(), format='%Y-%m-%d', errors='coerce').notna().all():
        return 'DATE'
    else:
        return 'TEXT'

# ‚úÖ Aplicar funci√≥n para cada columna
tipos_datos = df.apply(detectar_tipo_dato)

# ‚úÖ Longitudes m√°ximas de texto por columna
longitudes_maximas = df.apply(lambda col: col.dropna().map(lambda x: len(str(x))).max())

# ‚úÖ Conteo claro de valores nulos por columna
valores_nulos = df.isnull().sum()

# ‚úÖ Conteo claro de valores √∫nicos por columna
valores_unicos = df.nunique()

# ‚úÖ Consolidar resultados claramente en DataFrame final
resultado = pd.DataFrame({
    "Columna": df.columns,
    "Tipo de Dato Sugerido": tipos_datos,
    "Longitud M√°xima": longitudes_maximas,
    "Valores Nulos": valores_nulos,
    "Valores √önicos": valores_unicos
})

print(resultado)

# ‚úÖ Guardar resultados claramente en CSV
ruta_resultado = os.path.join(ruta_archivo, "analisis_detallado_fletes.csv")
resultado.to_csv(ruta_resultado, index=False)

print(f"‚úÖ Resultados guardados correctamente en: {ruta_resultado}")


---
# analizar_archivo_incidentes.py

import pandas as pd
import glob
import os

# ‚úÖ Definir correctamente la ruta
ruta_archivo = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP"

# ‚úÖ Buscar archivo CSV espec√≠fico basado en el patr√≥n de nombre
archivos_csv = glob.glob(os.path.join(ruta_archivo, "*-INCIDENTES-NACIONAL-ANUAL_*.csv"))

if len(archivos_csv) == 0:
    raise FileNotFoundError("No se encontr√≥ ning√∫n archivo con ese patr√≥n.")
else:
    archivo_csv = archivos_csv[0]

print(f"üìÇ Archivo analizado: {archivo_csv}")

# ‚úÖ Leer archivo CSV
df = pd.read_csv(archivo_csv, dtype=str)

# ‚úÖ Funci√≥n robusta para detectar tipos de dato predominantes
def detectar_tipo_dato(serie):
    if serie.dropna().str.match(r'^\d+$').all():
        return 'INTEGER'
    elif serie.dropna().str.match(r'^\d+\.\d+$').all():
        return 'NUMERIC(10,2)'
    elif pd.to_datetime(serie.dropna(), format='%Y-%m-%d %H:%M:%S', errors='coerce').notna().all():
        return 'TIMESTAMP'
    elif pd.to_datetime(serie.dropna(), format='%Y-%m-%d', errors='coerce').notna().all():
        return 'DATE'
    else:
        return 'TEXT'

# ‚úÖ Aplicar funci√≥n para cada columna
tipos_datos = df.apply(detectar_tipo_dato)

# ‚úÖ Longitudes m√°ximas de texto por columna
longitudes_maximas = df.apply(lambda col: col.dropna().map(lambda x: len(str(x))).max())

# ‚úÖ Conteo claro de valores nulos por columna
valores_nulos = df.isnull().sum()

# ‚úÖ Conteo claro de valores √∫nicos por columna
valores_unicos = df.nunique()

# ‚úÖ Consolidar resultados claramente en DataFrame final
resultado = pd.DataFrame({
    "Columna": df.columns,
    "Tipo de Dato Sugerido": tipos_datos,
    "Longitud M√°xima": longitudes_maximas,
    "Valores Nulos": valores_nulos,
    "Valores √önicos": valores_unicos
})

print(resultado)

# ‚úÖ Guardar resultados claramente en CSV
ruta_resultado = os.path.join(ruta_archivo, "analisis_detallado_fletes.csv")
resultado.to_csv(ruta_resultado, index=False)

print(f"‚úÖ Resultados guardados correctamente en: {ruta_resultado}")


---
# analizar_archivo_pedidos_desglosados.py

import pandas as pd
import glob
import os

# ‚úÖ Definir correctamente la ruta
ruta_archivo = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP"

# ‚úÖ Buscar archivo CSV espec√≠fico basado en el patr√≥n de nombre
archivos_csv = glob.glob(os.path.join(ruta_archivo, "*-PEDIDOS DESGLOSE-*.csv"))

if len(archivos_csv) == 0:
    raise FileNotFoundError("No se encontr√≥ ning√∫n archivo con ese patr√≥n.")
else:
    archivo_csv = archivos_csv[0]

print(f"üìÇ Archivo analizado: {archivo_csv}")

# ‚úÖ Leer archivo CSV
df = pd.read_csv(archivo_csv, dtype=str)

# ‚úÖ Funci√≥n robusta para detectar tipos de dato predominantes
def detectar_tipo_dato(serie):
    if serie.dropna().str.match(r'^\d+$').all():
        return 'INTEGER'
    elif serie.dropna().str.match(r'^\d+\.\d+$').all():
        return 'NUMERIC(10,2)'
    elif pd.to_datetime(serie.dropna(), format='%Y-%m-%d %H:%M:%S', errors='coerce').notna().all():
        return 'TIMESTAMP'
    elif pd.to_datetime(serie.dropna(), format='%Y-%m-%d', errors='coerce').notna().all():
        return 'DATE'
    else:
        return 'TEXT'

# ‚úÖ Aplicar funci√≥n para cada columna
tipos_datos = df.apply(detectar_tipo_dato)

# ‚úÖ Longitudes m√°ximas de texto por columna
longitudes_maximas = df.apply(lambda col: col.dropna().map(lambda x: len(str(x))).max())

# ‚úÖ Conteo claro de valores nulos por columna
valores_nulos = df.isnull().sum()

# ‚úÖ Conteo claro de valores √∫nicos por columna
valores_unicos = df.nunique()

# ‚úÖ Consolidar resultados claramente en DataFrame final
resultado = pd.DataFrame({
    "Columna": df.columns,
    "Tipo de Dato Sugerido": tipos_datos,
    "Longitud M√°xima": longitudes_maximas,
    "Valores Nulos": valores_nulos,
    "Valores √önicos": valores_unicos
})

print(resultado)

# ‚úÖ Guardar resultados claramente en CSV
ruta_resultado = os.path.join(ruta_archivo, "analisis_detallado_pedidos_desglosado.csv")
resultado.to_csv(ruta_resultado, index=False)

print(f"‚úÖ Resultados guardados correctamente en: {ruta_resultado}")


---
# analizar_archivo_pedidos_sigap.py

import pandas as pd
import glob
import os

# ‚úÖ Definir correctamente la ruta
ruta_archivo = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP"

# ‚úÖ Buscar archivo CSV espec√≠fico basado en el patr√≥n de nombre
archivos_csv = glob.glob(os.path.join(ruta_archivo, "*SEGUIMIENTO CANTIDADES-NACIONAL-ANUAL_*.csv"))

if len(archivos_csv) == 0:
    raise FileNotFoundError("No se encontr√≥ ning√∫n archivo con ese patr√≥n.")
else:
    archivo_csv = archivos_csv[0]

print(f"üìÇ Archivo analizado: {archivo_csv}")

# ‚úÖ Leer archivo CSV
df = pd.read_csv(archivo_csv, dtype=str)

# ‚úÖ Funci√≥n robusta para detectar tipos de dato predominantes
def detectar_tipo_dato(serie):
    if serie.dropna().str.match(r'^\d+$').all():
        return 'INTEGER'
    elif serie.dropna().str.match(r'^\d+\.\d+$').all():
        return 'NUMERIC(10,2)'
    elif pd.to_datetime(serie.dropna(), format='%Y-%m-%d %H:%M:%S', errors='coerce').notna().all():
        return 'TIMESTAMP'
    elif pd.to_datetime(serie.dropna(), format='%Y-%m-%d', errors='coerce').notna().all():
        return 'DATE'
    else:
        return 'TEXT'

# ‚úÖ Aplicar funci√≥n para cada columna
tipos_datos = df.apply(detectar_tipo_dato)

# ‚úÖ Longitudes m√°ximas de texto por columna
longitudes_maximas = df.apply(lambda col: col.dropna().map(lambda x: len(str(x))).max())

# ‚úÖ Conteo claro de valores nulos por columna
valores_nulos = df.isnull().sum()

# ‚úÖ Conteo claro de valores √∫nicos por columna
valores_unicos = df.nunique()

# ‚úÖ Consolidar resultados claramente en DataFrame final
resultado = pd.DataFrame({
    "Columna": df.columns,
    "Tipo de Dato Sugerido": tipos_datos,
    "Longitud M√°xima": longitudes_maximas,
    "Valores Nulos": valores_nulos,
    "Valores √önicos": valores_unicos
})

print(resultado)

# ‚úÖ Guardar resultados claramente en CSV
ruta_resultado = os.path.join(ruta_archivo, "analisis_detallado_fletes.csv")
resultado.to_csv(ruta_resultado, index=False)

print(f"‚úÖ Resultados guardados correctamente en: {ruta_resultado}")


---
# analizar_archivo_seguimiento_pedidos.py

import pandas as pd
import glob
import os

# ‚úÖ Definir correctamente la ruta
ruta_archivo = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP"

# ‚úÖ Buscar archivo CSV espec√≠fico basado en el patr√≥n de nombre
archivos_csv = glob.glob(os.path.join(ruta_archivo, "*-PEDIDOS SEGUIMIENTO CANTIDADES-*.csv"))

if len(archivos_csv) == 0:
    raise FileNotFoundError("No se encontr√≥ ning√∫n archivo con ese patr√≥n.")
else:
    archivo_csv = archivos_csv[0]

print(f"üìÇ Archivo analizado: {archivo_csv}")

# ‚úÖ Leer archivo CSV
df = pd.read_csv(archivo_csv, dtype=str)

# ‚úÖ Funci√≥n robusta para detectar tipos de dato predominantes
def detectar_tipo_dato(serie):
    if serie.dropna().str.match(r'^\d+$').all():
        return 'INTEGER'
    elif serie.dropna().str.match(r'^\d+\.\d+$').all():
        return 'NUMERIC(10,2)'
    elif pd.to_datetime(serie.dropna(), format='%Y-%m-%d %H:%M:%S', errors='coerce').notna().all():
        return 'TIMESTAMP'
    elif pd.to_datetime(serie.dropna(), format='%Y-%m-%d', errors='coerce').notna().all():
        return 'DATE'
    else:
        return 'TEXT'

# ‚úÖ Aplicar funci√≥n para cada columna
tipos_datos = df.apply(detectar_tipo_dato)

# ‚úÖ Longitudes m√°ximas de texto por columna
longitudes_maximas = df.apply(lambda col: col.dropna().map(lambda x: len(str(x))).max())

# ‚úÖ Conteo claro de valores nulos por columna
valores_nulos = df.isnull().sum()

# ‚úÖ Conteo claro de valores √∫nicos por columna
valores_unicos = df.nunique()

# ‚úÖ Consolidar resultados claramente en DataFrame final
resultado = pd.DataFrame({
    "Columna": df.columns,
    "Tipo de Dato Sugerido": tipos_datos,
    "Longitud M√°xima": longitudes_maximas,
    "Valores Nulos": valores_nulos,
    "Valores √önicos": valores_unicos
})

print(resultado)

# ‚úÖ Guardar resultados claramente en CSV
ruta_resultado = os.path.join(ruta_archivo, "analisis_detallado_seguimiento_pedidos.csv")
resultado.to_csv(ruta_resultado, index=False)

print(f"‚úÖ Resultados guardados correctamente en: {ruta_resultado}")


---
# analizar_archivo_transferencias.py

import pandas as pd
import glob
import os

# ‚úÖ Definir correctamente la ruta
ruta_archivo = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP"

# ‚úÖ Buscar archivo CSV espec√≠fico basado en el patr√≥n de nombre
archivos_csv = glob.glob(os.path.join(ruta_archivo, "*_2025_TR*.csv"))

if len(archivos_csv) == 0:
    raise FileNotFoundError("No se encontr√≥ ning√∫n archivo con ese patr√≥n.")
else:
    archivo_csv = archivos_csv[0]

print(f"üìÇ Archivo analizado: {archivo_csv}")

# ‚úÖ Leer archivo CSV
df = pd.read_csv(archivo_csv, dtype=str)

# ‚úÖ Funci√≥n robusta para detectar tipos de dato predominantes
def detectar_tipo_dato(serie):
    if serie.dropna().str.match(r'^\d+$').all():
        return 'INTEGER'
    elif serie.dropna().str.match(r'^\d+\.\d+$').all():
        return 'NUMERIC(10,2)'
    elif pd.to_datetime(serie.dropna(), format='%Y-%m-%d %H:%M:%S', errors='coerce').notna().all():
        return 'TIMESTAMP'
    elif pd.to_datetime(serie.dropna(), format='%Y-%m-%d', errors='coerce').notna().all():
        return 'DATE'
    else:
        return 'TEXT'

# ‚úÖ Aplicar funci√≥n para cada columna
tipos_datos = df.apply(detectar_tipo_dato)

# ‚úÖ Longitudes m√°ximas de texto por columna
longitudes_maximas = df.apply(lambda col: col.dropna().map(lambda x: len(str(x))).max())

# ‚úÖ Conteo claro de valores nulos por columna
valores_nulos = df.isnull().sum()

# ‚úÖ Conteo claro de valores √∫nicos por columna
valores_unicos = df.nunique()

# ‚úÖ Consolidar resultados claramente en DataFrame final
resultado = pd.DataFrame({
    "Columna": df.columns,
    "Tipo de Dato Sugerido": tipos_datos,
    "Longitud M√°xima": longitudes_maximas,
    "Valores Nulos": valores_nulos,
    "Valores √önicos": valores_unicos
})

print(resultado)

# ‚úÖ Guardar resultados claramente en CSV
ruta_resultado = os.path.join(ruta_archivo, "analisis_detallado_transferencias.csv")
resultado.to_csv(ruta_resultado, index=False)

print(f"‚úÖ Resultados guardados correctamente en: {ruta_resultado}")


---
# analizar_contenido_csv.py

import pandas as pd

# Ruta al archivo
ruta_csv = "/Users/Arturo/AGRICULTURA/2024/beneficiarios_2024.csv"

# Mostrar columnas y contar registros sin cargar todo en memoria
def analizar_csv(ruta):
    # Leer solo la primera fila para obtener columnas
    columnas = pd.read_csv(ruta, nrows=0).columns.tolist()
    
    # Contar n√∫mero de registros (sin encabezado)
    with open(ruta, 'r', encoding='utf-8') as f:
        total_lineas = sum(1 for _ in f)
        total_registros = total_lineas - 1  # Restamos encabezado

    print("üîπ Nombres de las columnas:")
    for col in columnas:
        print(f"- {col}")

    print(f"\nüîπ Total de registros (sin contar encabezado): {total_registros:,}")

# Ejecutar an√°lisis
analizar_csv(ruta_csv)



---
# analizar_expedientes.py

import os
import fitz
import pytesseract
from PIL import Image, ImageEnhance, ImageFilter
import io
import numpy as np
import pandas as pd

RUTA_EXPEDIENTES = "/Users/Arturo/AGRICULTURA/EXPEDIENTES"
GRUPOS = ["PERSONAL SEGALMEX", "PROPUESTAS DE ALTAS"]

# Definici√≥n de claves por tipo de documento
DOCUMENTOS = {
    "CURP": ["curp"],
    "INE": ["ine", "identificacion", "credencial"],
    "RFC": ["rfc", "situacion", "fiscal"],
    "CV": ["cv", "curriculum"],
    "COMPROBANTE DE DOMICILIO": ["domicilio", "recibo"],
    "CONSTANCIA SITUACI√ìN FISCAL": ["situaci√≥n fiscal", "r√©gimen fiscal", "sat"],
    "ACTA DE NACIMIENTO": ["nacimiento", "registro civil"],
    "ESTADO DE CUENTA": ["estado de cuenta", "clabe", "banco", "banamex", "bbva", "santander"],
    "CONSTANCIA DE NO INHABILITACI√ìN": ["inhabilitacion", "funci√≥n p√∫blica"],
    "COMPROBANTE DE ESTUDIOS": ["estudios", "titulo", "cedula"],
    "HOJA √öNICA DE SERVICIO": ["hoja √∫nica", "servicio"],
    "CARTILLA SMN": ["cartilla", "smn"]
}

# OCR mejorado
def aplicar_ocr(pixmap):
    img_data = pixmap.tobytes("png")
    img = Image.open(io.BytesIO(img_data)).convert("L")
    img = img.filter(ImageFilter.MedianFilter())
    img = ImageEnhance.Contrast(img).enhance(2.0)
    img_np = np.array(img)
    texto = pytesseract.image_to_string(img_np, lang='spa', config='--psm 6')
    return texto.lower()

# Extracci√≥n de texto del PDF
def extraer_texto(path):
    texto_total = ""
    try:
        with fitz.open(path) as doc:
            for page in doc:
                texto = page.get_text().lower().strip()
                if len(texto) < 30:
                    pix = page.get_pixmap(dpi=400)
                    texto = aplicar_ocr(pix)
                texto_total += texto + "\n"
    except:
        pass
    return texto_total

# Clasificaci√≥n por contenido
def clasificar_por_contenido(texto):
    resultados = set()
    for doc, claves in DOCUMENTOS.items():
        if any(clave in texto for clave in claves):
            resultados.add(doc)
    return resultados

# Clasificaci√≥n por nombre de archivo
def clasificar_por_nombre(nombre_archivo):
    nombre = nombre_archivo.lower()
    resultados = set()
    for doc, claves in DOCUMENTOS.items():
        if any(clave in nombre for clave in claves):
            resultados.add(doc)
    return resultados

# Recorrido y an√°lisis
checklist = []

for grupo in GRUPOS:
    ruta_grupo = os.path.join(RUTA_EXPEDIENTES, grupo)
    if not os.path.isdir(ruta_grupo):
        continue

    for persona in os.listdir(ruta_grupo):
        ruta_persona = os.path.join(ruta_grupo, persona)
        if not os.path.isdir(ruta_persona):
            continue

        documentos_detectados = {}
        observaciones = []

        archivos = [f for f in os.listdir(ruta_persona) if f.lower().endswith(".pdf")]

        for archivo in archivos:
            ruta_pdf = os.path.join(ruta_persona, archivo)
            texto = extraer_texto(ruta_pdf)

            detectados_contenido = clasificar_por_contenido(texto)
            detectados_nombre = clasificar_por_nombre(archivo)

            for doc in detectados_contenido:
                documentos_detectados[doc] = "‚úîÔ∏è OCR"
            for doc in detectados_nombre:
                if doc not in documentos_detectados:
                    documentos_detectados[doc] = "‚úîÔ∏è nombre"
                elif documentos_detectados[doc] == "‚úîÔ∏è nombre" and doc in detectados_contenido:
                    documentos_detectados[doc] = "‚úîÔ∏è OCR"

        fila = {
            "Grupo": grupo,
            "Persona": persona,
        }

        for doc in DOCUMENTOS.keys():
            fila[doc] = documentos_detectados.get(doc, "‚ùå")

        checklist.append(fila)

# Exportar a Excel
df = pd.DataFrame(checklist)
salida = os.path.join(RUTA_EXPEDIENTES, "CHECKLIST_RESULTADO_FINAL.xlsx")
df.to_excel(salida, index=False)
print(f"‚úÖ Checklist generado correctamente:\n{salida}")



---
# analizar_full_derechohabientes.py

import os
import pandas as pd
import unicodedata
import re

# Ruta base
ruta_base = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/U_TEMPORAL/"
archivos = [
    "1051-FERTILIZANTES-FULL-NACIONAL-SEGUNDO CORTE_2025-05-09 18_26_46_.csv",
    "1051-FERTILIZANTES-FULL-NACIONAL-PRIMER CORTE_2025-05-09 16_30_38_.csv",
    "1051-FERTILIZANTES-FULL-NACIONAL-TERCER CORTE_2025-05-10 12_44_16_.csv",
    "1051-FERTILIZANTES-FULL-NACIONAL-CUARTO CORTE_2025-05-10 16_56_37_.csv",
    "1051-FERTILIZANTES-FULL-NACIONAL-QUINTO CORTE_2025-05-10 17_01_10_.csv",
    "1051-FERTILIZANTES-FULL-NACIONAL-SEXTO CORTE_2025-05-10 17_05_41_.csv",
    "1051-FERTILIZANTES-FULL-SINALOA-ANUAL_2025-05-10 17_06_24_.csv",
]

# Limpieza de nombres de columnas
def limpiar_columna(nombre):
    nombre = unicodedata.normalize('NFKD', nombre)
    nombre = ''.join(c for c in nombre if not unicodedata.combining(c))
    nombre = nombre.lower().replace(" ", "_")
    nombre = re.sub(r"[^a-z0-9_]", "", nombre)  # eliminar s√≠mbolos raros
    return nombre

# Leer archivos y concatenar
df_total = pd.DataFrame()
for archivo in archivos:
    path = os.path.join(ruta_base, archivo)
    df = pd.read_csv(path, dtype=str, encoding="utf-8")
    if "Estatus Solicitud" in df.columns:
        df = df.rename(columns={"Estatus Solicitud": "estatus_solicitud_pago"})
    df_total = pd.concat([df_total, df], ignore_index=True)

# Normalizar columnas
df_total.columns = [limpiar_columna(col) for col in df_total.columns]

# Funci√≥n para detectar tipo SQL
def detectar_tipo(columna, nombre):
    datos = columna.dropna().astype(str).str.strip()
    longitud_max = int(datos.str.len().max())

    campos_texto = [
        "nombre", "apellido", "estado", "municipio", "localidad", "sexo",
        "cultivo", "estatus", "regimen", "etapa", "cader", "ddr", "cuadernillo"
    ]
    if any(p in nombre for p in campos_texto):
        return f"VARCHAR({max(longitud_max, 10)})"
    
    if "curp" in nombre:
        return "VARCHAR(18)"
    if "clave" in nombre or "cdf_entrega" in nombre or "acuse" in nombre:
        return f"VARCHAR({max(longitud_max, 10)})"
    
    if datos.str.fullmatch(r"\d{4}-\d{2}-\d{2}").all():
        return "DATE"

    contiene_letras = datos.str.contains(r"[A-Za-z√Å√â√ç√ì√ö√ú√ë√°√©√≠√≥√∫√º√±]", regex=True).any()
    if contiene_letras:
        return f"VARCHAR({longitud_max})"

    try:
        numeros = pd.to_numeric(datos, errors='coerce')
        if numeros.dropna().apply(float.is_integer).all():
            return "INTEGER"
        elif not numeros.dropna().empty:
            return "FLOAT"
    except:
        pass

    return f"VARCHAR({longitud_max})"

# Detectar tipo por columna
tipos_sql = {col: detectar_tipo(df_total[col], col) for col in df_total.columns}

# Generar SQL
nombre_tabla = "full_derechohabientes_2025"
columnas_sql = [f'    "{col}" {tipo}' for col, tipo in tipos_sql.items()]
sql_create = f"""CREATE TABLE {nombre_tabla} (
{',\n'.join(columnas_sql)},
    PRIMARY KEY ("acuse_estatal")
);
"""

# Relaciones e √≠ndices
sql_relaciones = f"""
-- Relaciones
ALTER TABLE {nombre_tabla}
    ADD CONSTRAINT fk_derechohabientes FOREIGN KEY ("acuse_estatal") REFERENCES derechohabientes("acuse_estatal"),
    ADD CONSTRAINT fk_red_distribucion FOREIGN KEY ("cdf_entrega") REFERENCES red_distribucion("id_ceda_agricultura");

-- √çndices
CREATE INDEX idx_curp_appmovil ON {nombre_tabla}("curp_appmovil");
CREATE INDEX idx_curp_renapo ON {nombre_tabla}("curp_renapo");
CREATE INDEX idx_acuse_estatal ON {nombre_tabla}("acuse_estatal");
CREATE INDEX idx_cdf_entrega ON {nombre_tabla}("cdf_entrega");
"""

# Relaci√≥n opcional con dim_fecha
sql_fecha = ""
if "fecha_entrega" in df_total.columns:
    sql_fecha = f"""
-- Relaci√≥n opcional con dim_fecha
-- ALTER TABLE {nombre_tabla}
--     ADD CONSTRAINT fk_fecha_entrega FOREIGN KEY (fecha_entrega) REFERENCES dim_fecha(fecha);
"""
else:
    sql_fecha = "-- No se detect√≥ columna 'fecha_entrega'."

# Guardar SQL final
ruta_sql = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/SCRIPTS/sql_crear_tabla_full_derechohabientes_2025.sql"
with open(ruta_sql, "w", encoding="utf-8") as f:
    f.write("-- SQL generado autom√°ticamente\n\n")
    f.write(sql_create + "\n")
    f.write(sql_relaciones + "\n")
    f.write(sql_fecha + "\n")

print(f"‚úÖ SQL generado y guardado en: {ruta_sql}")
    


---
# analizar_longitud_y_tipo_de_datos.py

import pandas as pd

# Ruta del archivo CSV a revisar
ruta_csv = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/1051-FERTILIZANTES-FLETES-NACIONAL-ANUAL_2025-03-09 18_52_01_.csv"

# Cargar el archivo CSV (leer todo como texto para evitar conversiones autom√°ticas)
df = pd.read_csv(ruta_csv, dtype=str)

# Funci√≥n para detectar tipo de dato predominante
def detectar_tipo_dato(serie):
    try:
        # Convertir a n√∫meros enteros y verificar si no hay decimales
        if serie.dropna().str.isnumeric().all():
            return "INTEGER"
        # Convertir a float, si todos los valores pueden convertirse es un NUMERIC
        serie_float = pd.to_numeric(serie.dropna(), errors='coerce')
        if serie_float.notna().all():
            return "NUMERIC(10,2)"  # Asumimos 10 d√≠gitos totales, 2 decimales
        # Detectar fechas
        serie_fecha = pd.to_datetime(serie.dropna(), errors='coerce')
        if serie_fecha.notna().all():
            return "DATE"
    except:
        pass
    return "TEXT"

# Calcular la longitud m√°xima de cada columna
longitudes = df.applymap(lambda x: len(str(x)) if pd.notna(x) else 0).max()

# Detectar el tipo de dato predominante en cada columna
tipos_dato = df.apply(detectar_tipo_dato)

# Crear un DataFrame con los resultados
resultado = pd.DataFrame({
    "Columna": df.columns,
    "Longitud M√°xima": longitudes.values,
    "Tipo de Dato Sugerido": tipos_dato.values
})

# Mostrar los resultados
print(resultado)

# Guardar los resultados en un archivo CSV
resultado.to_csv("/Users/Arturo/AGRICULTURA/FERTILIZANTES/SCRIPTS/analisis_fletes.csv", index=False)


---
# analizar_padrones_2025.py

import os
import pandas as pd
import unicodedata
import re

# üìÅ Ruta base
base_path = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/derechohabientes_padrones"
output_csv = os.path.join(base_path, "derechohabientes_padrones_2025.csv")
duplicados_csv = os.path.join(base_path, "duplicados.csv")

# üîç Buscar todos los archivos .xlsx dentro de las subcarpetas
excel_files = []
for root, dirs, files in os.walk(base_path):
    for file in files:
        if file.endswith(".xlsx") and not file.startswith("~$"):
            excel_files.append(os.path.join(root, file))

print(f"üìä Se detectaron {len(excel_files)} archivos Excel.")

# üß© Unir todos los archivos en un solo DataFrame, agregando nombre de archivo
df_total = pd.DataFrame()
for archivo in excel_files:
    df = pd.read_excel(archivo, dtype=str)
    df["archivo_origen"] = os.path.basename(archivo)
    df_total = pd.concat([df_total, df], ignore_index=True)

# üßº Limpiar nombres de columnas
def limpiar_columna(col):
    col = unicodedata.normalize('NFKD', col)
    col = ''.join(c for c in col if not unicodedata.combining(c))
    col = col.strip().lower().replace(" ", "_")
    col = re.sub(r"[^a-z0-9_]", "", col)
    return col

df_total.columns = [limpiar_columna(c) for c in df_total.columns]

# üîÅ Verificar duplicados por acuse_estatal y guardar duplicados
if "acuse_estatal" not in df_total.columns:
    print("‚ùå ERROR: No se encontr√≥ la columna 'acuse_estatal'.")
    exit()

df_total["__orden__"] = df_total.index  # para conservar el √∫ltimo
duplicados = df_total[df_total.duplicated("acuse_estatal", keep=False)]
duplicados.drop(columns=["__orden__"]).to_csv(duplicados_csv, index=False, encoding="utf-8")
print(f"‚ö†Ô∏è Se encontraron {len(duplicados)} registros duplicados por 'acuse_estatal'. Se guardaron en:\n{duplicados_csv}")

# üßπ Eliminar duplicados, conservar el √∫ltimo registro
df_total = df_total.sort_values("__orden__").drop(columns=["__orden__"])
df_total = df_total.drop_duplicates("acuse_estatal", keep="last")

# üíæ Guardar el archivo combinado final (sin duplicados)
df_total.to_csv(output_csv, index=False, encoding="utf-8")
print(f"‚úÖ Archivo final sin duplicados guardado en:\n{output_csv}")

# üß† Inferencia de tipos de datos para SQL
def infer_sql_type(serie):
    datos = serie.dropna().astype(str).str.strip()

    if datos.empty:
        return "VARCHAR(1)"

    longitud_max = datos.str.len().max()
    if pd.isna(longitud_max):
        return "VARCHAR(1)"

    longitud_max = int(longitud_max)

    if datos.str.fullmatch(r"\d{4}-\d{2}-\d{2}").all():
        return "DATE"
    if datos.str.contains(r"[A-Za-z√Å√â√ç√ì√ö√ú√ë√°√©√≠√≥√∫√º√±]", regex=True).any():
        return f"VARCHAR({max(longitud_max, 1)})"
    try:
        numeros = pd.to_numeric(datos, errors="coerce")
        if numeros.dropna().apply(float.is_integer).all():
            return "INTEGER"
        elif not numeros.dropna().empty:
            return "FLOAT"
    except:
        pass
    return f"VARCHAR({max(longitud_max, 1)})"

# üßæ Generar SQL CREATE TABLE
tipos_sql = {col: infer_sql_type(df_total[col]) for col in df_total.columns}
nombre_tabla = "derechohabientes_padrones_2025"
columnas_sql = [f'    "{col}" {tipo}' for col, tipo in tipos_sql.items()]
columnas_sql.append('    PRIMARY KEY ("acuse_estatal")')

sql = f"""CREATE TABLE {nombre_tabla} (
{',\n'.join(columnas_sql)}
);
"""

print("\nüìÑ Sentencia SQL para crear la tabla:")
print(sql)



---
# checar_fechas.py

import pandas as pd, glob, os, unicodedata, re

# === Ajusta SOLO si tu ruta cambia ===
ruta_base = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/"

# --- Localizar archivo ---
file_path = glob.glob(os.path.join(ruta_base, "*PEDIDOS DESGLOSE-NACIONAL-ANUAL*"))[0]

# --- Cargar CSV ---
df_pedidos = pd.read_csv(file_path, encoding="utf-8", delimiter=",", header=0)

# --- Normalizar nombres de columnas (tal como en tu script) ---
def normalizar_columna(col):
    col = col.strip().lower()
    col = ''.join(c for c in unicodedata.normalize('NFD', col) if unicodedata.category(c) != 'Mn')
    col = re.sub(r'[^a-z0-9]+', '_', col)
    col = re.sub(r'[_]+', '_', col).strip('_')
    return col
df_pedidos.columns = [normalizar_columna(c) for c in df_pedidos.columns]

# --- Mostrar resultados ---
print("Columnas:", df_pedidos.columns.tolist())
print("Primeros 10 valores de 'fecha':", df_pedidos["fecha"].head(10).tolist())


---
# clasificar_documento.py

# Versi√≥n mejorada del script con OCR avanzado, detecci√≥n de firmas y clasificaci√≥n ampliada

import os
import fitz  # PyMuPDF
import pytesseract
from PIL import Image, ImageEnhance, ImageFilter
import io
import numpy as np
import pandas as pd

# Configuraci√≥n base
RUTA_EXPEDIENTES = "/Users/Arturo/AGRICULTURA/EXPEDIENTES"
GRUPOS = ["PERSONAL SEGALMEX", "PROPUESTAS DE ALTAS"]

# Palabras clave ampliadas por tipo de documento
CLAVES_DOCUMENTOS = {
    "ACTA DE NACIMIENTO": ["registro civil", "acta de nacimiento", "oficial√≠a", "libro"],
    "CURP": ["clave √∫nica de registro", "curp", "gobierno de m√©xico"],
    "INE": ["instituto nacional electoral", "credencial para votar", "vigencia", "electoral"],
    "RFC": ["registro federal de contribuyentes", "sat"],
    "CONSTANCIA SITUACI√ìN FISCAL": ["situaci√≥n fiscal", "r√©gimen fiscal", "c√≥digo postal", "sat"],
    "COMPROBANTE DE DOMICILIO": ["recibo", "cfe", "luz", "agua", "gas", "servicio", "importe"],
    "CONSTANCIA DE NO INHABILITACI√ìN": ["funci√≥n p√∫blica", "no se encuentra inhabilitado", "inhabilitaci√≥n"],
    "ESTADO DE CUENTA": [
        "estado de cuenta", "saldo", "clabe", "cuenta", "movimientos",
        "banamex", "bbva", "santander", "banorte", "scotiabank",
        "n√∫mero de cuenta", "gastos", "detalle de movimientos", "banco", "pago de n√≥mina"
    ],
    "CV": [
        "curriculum vitae", "curr√≠culum", "datos personales", "experiencia laboral",
        "formaci√≥n acad√©mica", "educaci√≥n", "perfil profesional", "objetivo",
        "referencias", "escolaridad", "periodo laboral"
    ],
    "COMPROBANTE DE ESTUDIOS": [
        "t√≠tulo", "certificado", "licenciatura", "ingenier√≠a", "universidad", "bachillerato", "c√©dula"
    ],
    "CARTILLA SMN": ["servicio militar nacional", "cartilla", "defensa nacional"],
    "HOJAS √öNICAS DE SERVICIO": ["hoja √∫nica de servicio", "fecha de ingreso", "plaza", "n√≥mina"]
}

# Aplicar OCR mejorado con Pillow + Tesseract + DPI alto
def aplicar_ocr_con_mejoras(pixmap):
    img_data = pixmap.tobytes("png")
    img = Image.open(io.BytesIO(img_data)).convert('L')  # Escala de grises
    img = img.filter(ImageFilter.MedianFilter())
    enhancer = ImageEnhance.Contrast(img)
    img = enhancer.enhance(2.0)
    img_np = np.array(img)
    texto = pytesseract.image_to_string(img_np, lang='spa', config='--psm 6')
    return texto

# Extraer texto de PDF, aplicar OCR si no hay texto
def extraer_texto_pdf(path):
    try:
        doc = fitz.open(path)
        texto_total = ""
        for i, page in enumerate(doc):
            texto = page.get_text().strip()
            if len(texto) < 30:
                pix = page.get_pixmap(dpi=400)
                texto += aplicar_ocr_con_mejoras(pix)
            texto_total += texto + "\n"
        doc.close()
        return texto_total.strip()
    except Exception:
        return ""

# Clasificar documento seg√∫n su contenido textual
def clasificar_documento(texto):
    texto_limpio = texto.lower()
    coincidencias = {}
    for tipo, claves in CLAVES_DOCUMENTOS.items():
        score = sum(1 for palabra in claves if palabra in texto_limpio)
        if score > 0:
            coincidencias[tipo] = score
    if coincidencias:
        tipo_detectado = max(coincidencias, key=coincidencias.get)
        return tipo_detectado, coincidencias[tipo_detectado]
    return "NO IDENTIFICADO", 0

# Recolectar resultados
resultados = []

for grupo in GRUPOS:
    ruta_grupo = os.path.join(RUTA_EXPEDIENTES, grupo)
    if not os.path.isdir(ruta_grupo):
        continue

    for persona in os.listdir(ruta_grupo):
        ruta_persona = os.path.join(ruta_grupo, persona)
        if not os.path.isdir(ruta_persona):
            continue

        for archivo in os.listdir(ruta_persona):
            if archivo.lower().endswith(".pdf"):
                ruta_archivo = os.path.join(ruta_persona, archivo)
                texto = extraer_texto_pdf(ruta_archivo)
                tipo, score = clasificar_documento(texto)
                resultados.append({
                    "Grupo": grupo,
                    "Persona": persona,
                    "Archivo": archivo,
                    "Tipo Detectado": tipo,
                    "Coincidencias": score
                })

# Exportar resultados a Excel
archivo_salida = os.path.join(RUTA_EXPEDIENTES, "clasificacion_resultados.xlsx")
pd.DataFrame(resultados).to_excel(archivo_salida, index=False)
print(f"\n‚úÖ Clasificaci√≥n completada. Resultados guardados en:\n{archivo_salida}")



---
# conceder_acceso_a_excel.py

import subprocess

# Llama AppleScript que intenta controlar Excel
applescript = '''
tell application "Microsoft Excel"
    activate
end tell
'''

subprocess.run(["osascript", "-e", applescript])



---
# conexion.py

from sqlalchemy import create_engine
import psycopg2

# Configuraci√≥n de la base de datos
DB_USER = "postgres"
DB_PASSWORD = "Art4125r0"
DB_HOST = "localhost"
DB_PORT = "5432"
DB_NAME = "fertilizantes"

# Crear conexi√≥n SQLAlchemy para pandas/to_sql
engine = create_engine(
    f"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}"
)

# Crear conexi√≥n psycopg2 para COPY
psycopg_conn = psycopg2.connect(
    dbname=DB_NAME,
    user=DB_USER,
    password=DB_PASSWORD,
    host=DB_HOST,
    port=DB_PORT
)


---
# convertir_word_en_pdf.py


#!/usr/bin/env python3
'''
fast_docx_pandoc_pdf.py
-----------------------------------------
Conversi√≥n ultra‚Äër√°pida de un DOCX (contratos.docx)
a PDF utilizando Pandoc + WeasyPrint.

Requisitos previos (una sola vez):
  brew install pandoc weasyprint

C√≥mo usar:
  python fast_docx_pandoc_pdf.py
     ‚Üí crea contratos.pdf junto al DOCX original
'''
import subprocess
from pathlib import Path
import sys
import shlex

DOCX = Path('/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/contratos.docx')
HTML = DOCX.with_suffix('.html')
PDF  = DOCX.with_suffix('.pdf')

def run(cmd: str):
    '''Ejecuta comando shell mostrando salida en vivo.'''
    print(f"‚öôÔ∏è  {cmd}")
    result = subprocess.run(shlex.split(cmd), capture_output=True, text=True)
    if result.stdout:
        print(result.stdout)
    if result.stderr:
        print(result.stderr, file=sys.stderr)
    if result.returncode != 0:
        sys.exit(f"üí• Error ({result.returncode}) al ejecutar: {cmd}")

def main():
    if not DOCX.exists():
        sys.exit(f"‚ùå No se encontr√≥ {DOCX}")

    # 1. DOCX ‚Üí HTML v√≠a Pandoc
    run(f"pandoc {DOCX} -o {HTML} --from=docx --to=html")

    # 2. HTML ‚Üí PDF v√≠a WeasyPrint
    run(f"weasyprint {HTML} {PDF}")

    # 3. Opcional: eliminar HTML temporal (comenta si lo quieres conservar)
    HTML.unlink(missing_ok=True)

    print(f"‚úÖ PDF generado en: {PDF}")

if __name__ == '__main__':
    main()



---
# correccion_toneladas_derechohabientes.py

#!/usr/bin/env python3
import subprocess
from sqlalchemy import text
from conexion import engine, DB_NAME

# Paso 1: Corregir los datos calculando toneladas entregadas
update_query = """
UPDATE derechohabientes
SET 
    ton_dap_entregada = ROUND(((dap_anio_actual + dap_remanente) * 0.025)::numeric, 3),
    ton_urea_entregada = ROUND(((urea_anio_actual + urea_remanente) * 0.025)::numeric, 3)
WHERE ton_dap_entregada = 0 AND ton_urea_entregada = 0;
"""

print(f"üõ†Ô∏è Corrigiendo valores nulos en 'ton_dap_entregada' y 'ton_urea_entregada' de la tabla 'derechohabientes' en la base '{DB_NAME}'...")

with engine.begin() as conn:
    result = conn.execute(text(update_query))
    print(f"‚úÖ Se actualizaron {result.rowcount} registros.")

# Paso 2: Actualizar vistas materializadas
vistas_materializadas = [
    "avance_operativo_ceda_2025",
    "entregas_diarias_x_estado_ceda_2025",
    "entregas_x_estado_no_ceda_y_fecha_2025",
    "inventario_acumulado_x_ceda_diario_2025",
    "inventario_remanente_x_ceda_diario_2025",
    "inventario_remanente_x_ceda_2025",
]

print("üîÑ Actualizando vistas materializadas...")
with engine.begin() as conn:
    for vista in vistas_materializadas:
        print(f"üîÅ REFRESH MATERIALIZED VIEW {vista}...")
        conn.execute(text(f"REFRESH MATERIALIZED VIEW {vista};"))
print("‚úÖ Vistas actualizadas.")

# Paso 3: Ejecutar scripts de exportaci√≥n y gr√°ficos
scripts_exportacion = [
    "exportar_entregas_td.py",
    "exportar_remanentes_td.py",
    "exportar_avances_2025_td.py",
    "exportar_entregas_diarias_2025.py",
    "exportar_resumen_derechohabientes_apoyados_x_municipio.py",
    "exportar_abasto_y_remanente_por_dia_sin_transito_2025.py",
    "exportar_entregas_por_estado_y_genero.py",
    "exportar_entregas_semanales_2025.py",
    "exportar_reporte_derechohabientes_bienestar_td.py",
    "grafico_Abasto Proyectado vs Real vs Pedido vs Entregado.py",
    "grafico_entregas_semanales_2025.py",
    "grafico_entregas_diarias.py",
    "grafico_recibido_vs_entregado.py",
    "exportar_entregas_x_estado_no_ceda_y_fecha_2025.py",
    "exportar_avance_operativo_ceda_2025.py"
]

print("üì§ Ejecutando scripts de exportaci√≥n y generaci√≥n de gr√°ficos...")
for script in scripts_exportacion:
    ruta_script = f"/Users/Arturo/AGRICULTURA/FERTILIZANTES/SCRIPTS/{script}"
    print(f"‚ñ∂Ô∏è Ejecutando: {script}")
    result = subprocess.run(["python3", ruta_script], capture_output=True, text=True)
    if result.returncode == 0:
        print(f"‚úÖ {script} ejecutado correctamente.")
    else:
        print(f"‚ùå Error en {script}:\n{result.stderr}")

print("üéâ Proceso completo.")


---
# corregir_toneladas_2025.py

#!/usr/bin/env python3
from conexion import engine
import pandas as pd
from sqlalchemy import text

print("üîÑ Comparando y actualizando registros entre derechohabientes y derechohabientes_padrones_2025...")

# Consulta para detectar registros distintos
query = """
SELECT
    d.acuse_estatal,
    p.dap_ton,
    p.urea_ton
FROM derechohabientes d
JOIN derechohabientes_padrones_2025 p ON d.acuse_estatal = p.acuse_estatal
WHERE d.ton_dap_entregada IS DISTINCT FROM p.dap_ton
   OR d.ton_urea_entregada IS DISTINCT FROM p.urea_ton;
"""

df = pd.read_sql(query, engine)

if df.empty:
    print("‚úÖ No hay registros que necesiten actualizaci√≥n.")
else:
    print(f"‚ö†Ô∏è  Se encontraron {len(df)} registros con diferencias. Procediendo con la actualizaci√≥n...")

    with engine.begin() as conn:  # begin() incluye autom√°ticamente commit
        for _, row in df.iterrows():
            acuse = row['acuse_estatal']
            dap_ton = row['dap_ton'] or 0
            urea_ton = row['urea_ton'] or 0
            dap_actual = dap_ton / 0.025
            urea_actual = urea_ton / 0.025

            update_stmt = text("""
                UPDATE derechohabientes
                SET
                    ton_dap_entregada = :dap_ton,
                    ton_urea_entregada = :urea_ton,
                    dap_anio_actual = :dap_actual,
                    urea_anio_actual = :urea_actual
                WHERE acuse_estatal = :acuse_estatal;
            """)
            conn.execute(update_stmt, {
                'dap_ton': dap_ton,
                'urea_ton': urea_ton,
                'dap_actual': dap_actual,
                'urea_actual': urea_actual,
                'acuse_estatal': acuse
            })

    print("‚úÖ Actualizaci√≥n completada con √©xito.")


---
# derechohabientes_no_sincronizados.py

import pandas as pd
from sqlalchemy import text
from conexion import engine

# -------------------------------
# 1. Leer datos desde el Excel
# -------------------------------
ruta_excel = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/CORRECCIONES DERECHOHABIENTES/derechohabientes_no_sincronizados_nacional.xlsx"
hoja = "Detalles depurado"

# Leer columnas: C (ID_CEDA_AGRICULTURA), D (Acuse_estatal), G (Fecha_Entrega)
df_excel = pd.read_excel(ruta_excel, sheet_name=hoja, usecols="C,D,G")
df_excel = df_excel.dropna(subset=["Acuse_estatal"])

# Normalizar
df_excel["Acuse_estatal"] = df_excel["Acuse_estatal"].astype(str).str.strip()
df_excel["ID_CEDA_AGRICULTURA"] = df_excel["ID_CEDA_AGRICULTURA"].astype(str).str.strip()
df_excel["Fecha_Entrega"] = pd.to_datetime(df_excel["Fecha_Entrega"], errors="coerce")

# Crear diccionarios para mapear datos
map_cdf = dict(zip(df_excel["Acuse_estatal"], df_excel["ID_CEDA_AGRICULTURA"]))
map_fecha = dict(zip(df_excel["Acuse_estatal"], df_excel["Fecha_Entrega"]))

# Armar lista de acuses
acuses = df_excel["Acuse_estatal"].unique().tolist()
acuses_str = ",".join(f"'{a}'" for a in acuses)

# -------------------------------
# 2. Consulta SQL con filtro
# -------------------------------
query = f"""
SELECT
    f.clave_estado_predio_capturada,
    f.estado_predio_capturada,
    f.clave_municipio_predio_capturada,
    f.municipio_predio_capturada,
    f.clave_localidad_predio AS clave_localidad_predio_capturada,
    f.localidad_predio AS localidad_predio_capturada,
    f.id_solicitud AS id_nu_solicitud,
    f.cdf_entrega,
    NULL AS id_cdf_entrega,
    f.acuse_estatal,
    f.curp_appmovil AS curp_solicitud,
    f.curp_renapo AS curp_renapo,
    f.curp_historica,
    f.primer_apellido AS sn_primer_apellido,
    f.segundo_apellido AS sn_segundo_apellido,
    f.nombre AS ln_nombre,
    f.es_pob_indigena,
    f.cultivo,
    d.dap_ton AS ton_dap_entregada,
    d.urea_ton AS ton_urea_entregada,
    f.fecha_entrega,
    f.id_persona AS folio_persona,
    f.cuadernillo,
    f.nombre_ddr AS nombre_ddr,
    f.clave_ddr AS clave_ddr,
    f.nombre_cader_ventanilla,
    f.clave_cader_ventanilla,
    ROUND(COALESCE(d.dap_ton, 0) / 0.025)::INT AS dap_anio_actual,
    ROUND(COALESCE(d.urea_ton, 0) / 0.025)::INT AS urea_anio_actual,
    0 AS dap_remanente,
    0 AS urea_remanente,
    f.superficie_apoyada,
    f.id_persona AS id_beneficiarios_fertilizantes,
    NULL AS fecha_creacion,
    NULL AS id_nu_taxonomia
FROM full_derechohabientes_2025 f
LEFT JOIN derechohabientes_padrones_2025 d
    ON f.acuse_estatal = d.acuse_estatal
WHERE f.acuse_estatal IN ({acuses_str})
"""

# -------------------------------
# 3. Ejecutar consulta
# -------------------------------
df = pd.read_sql_query(text(query), engine)

# üîÑ Reemplazar datos con los del Excel usando map
df["cdf_entrega"] = df["acuse_estatal"].map(map_cdf)
df["fecha_entrega"] = df["acuse_estatal"].map(map_fecha)

# -------------------------------
# 4. Exportar
# -------------------------------
ruta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/CORRECCIONES DERECHOHABIENTES/derechohabientes_no_sincronizados_importar.csv"
df.to_csv(ruta_salida, index=False, encoding="utf-8-sig")

print(f"‚úÖ Exportado {len(df)} registros a:\n{ruta_salida}")


---
# derechohabientes_tabasco_2025_sexo.py

# archivo: derechohabientes_tabasco_2025_sexo.py

import pandas as pd
import os
from conexion import engine

# Ruta de salida
ruta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS"
nombre_archivo = "derechohabientes_apoyados_tabasco.csv"
ruta_completa = os.path.join(ruta_salida, nombre_archivo)

# Consulta: obtener todos los campos de la tabla derechohabientes para Tabasco
query = """
SELECT dh.*
FROM derechohabientes dh
JOIN red_distribucion rd
  ON dh.cdf_entrega = rd.id_ceda_agricultura
WHERE UPPER(rd.estado) = 'TABASCO';
"""

# Leer el resultado
df = pd.read_sql(query, engine)

# Crear columna 'sexo' a partir de CURP (posici√≥n 11 = √≠ndice 10)
df["sexo"] = df["curp_solicitud"].str.upper().str[10]

# Exportar archivo
df.to_csv(ruta_completa, index=False, encoding="utf-8")
print(f"‚úÖ Archivo generado exitosamente: {ruta_completa}")



---
# dim_fechas.py

import pandas as pd
from sqlalchemy import create_engine, text

# Configuraci√≥n de la conexi√≥n a PostgreSQL
DB_USER = "postgres"
DB_PASSWORD = "Art4125r0"
DB_HOST = "localhost"
DB_PORT = "5432"
DB_NAME = "fertilizantes"

# Crear conexi√≥n a PostgreSQL
engine = create_engine(f'postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}')

# Generar fechas para el a√±o 2025
fechas = pd.date_range(start="2025-01-01", end="2025-12-31", freq='D')
df = pd.DataFrame({'fecha': fechas})
df['anio'] = df['fecha'].dt.year
df['mes'] = df['fecha'].dt.month
df['semana'] = df['fecha'].dt.isocalendar().week
df['dia'] = df['fecha'].dt.day

# Diccionarios de traducci√≥n manual
meses = {
    1: 'Enero', 2: 'Febrero', 3: 'Marzo', 4: 'Abril',
    5: 'Mayo', 6: 'Junio', 7: 'Julio', 8: 'Agosto',
    9: 'Septiembre', 10: 'Octubre', 11: 'Noviembre', 12: 'Diciembre'
}
dias = {
    0: 'Lunes', 1: 'Martes', 2: 'Mi√©rcoles', 3: 'Jueves',
    4: 'Viernes', 5: 'S√°bado', 6: 'Domingo'
}

# Aplicar traducciones
df['nombre_mes'] = df['mes'].map(meses)
df['nombre_dia'] = df['fecha'].dt.weekday.map(dias)
df['es_fin_de_semana'] = df['nombre_dia'].isin(['S√°bado', 'Domingo'])

# Crear tabla dim_fecha en PostgreSQL
create_table_sql = """
CREATE TABLE IF NOT EXISTS dim_fecha (
    fecha DATE PRIMARY KEY,
    anio INTEGER,
    mes INTEGER,
    nombre_mes TEXT,
    semana INTEGER,
    dia INTEGER,
    nombre_dia TEXT,
    es_fin_de_semana BOOLEAN
);
"""

with engine.connect() as conn:
    conn.execute(text(create_table_sql))
    print("‚úÖ Tabla dim_fecha creada correctamente.")
    
    # Borrar datos anteriores (si existen)
    conn.execute(text("DELETE FROM dim_fecha;"))
    print("üßπ Datos anteriores eliminados.")

# Insertar datos nuevos
df.to_sql('dim_fecha', engine, index=False, if_exists='append')
print("üöÄ Tabla dim_fecha poblada con fechas del 2025.")



---
# dividir_padron_2024.py

import pandas as pd
import os

# Ruta del archivo original
ruta_csv = "/Users/Arturo/AGRICULTURA/2024/beneficiarios_2024.csv"

# Contar total de registros (sin encabezado)
with open(ruta_csv, 'r', encoding='utf-8') as f:
    total_lineas = sum(1 for _ in f)
    total_registros = total_lineas - 1

# Calcular tama√±o por parte
registros_por_parte = total_registros // 4

# Crear carpeta de salida
carpeta_salida = "/Users/Arturo/AGRICULTURA/2024/beneficiarios_dividido"
os.makedirs(carpeta_salida, exist_ok=True)

# Variables de control
parte = 1
registros_acumulados = 0
registros_deseados = registros_por_parte
df_parte = []

# Leer por bloques
chunk_size = 100_000  # Ajusta seg√∫n tu RAM
for chunk in pd.read_csv(ruta_csv, chunksize=chunk_size, encoding='utf-8'):
    df_parte.append(chunk)
    registros_acumulados += len(chunk)

    # Si es una de las tres primeras partes o ya es la √∫ltima
    if registros_acumulados >= registros_deseados or parte == 4:
        df_total = pd.concat(df_parte, ignore_index=True)
        nombre_archivo = f"beneficiarios_2024_parte_{parte}.csv"
        ruta_salida = os.path.join(carpeta_salida, nombre_archivo)
        df_total.to_csv(ruta_salida, index=False, encoding='utf-8-sig')
        print(f"‚úÖ Parte {parte} guardada con {len(df_total):,} registros.")
        
        parte += 1
        df_parte = []
        registros_acumulados = 0
        
        # Las siguientes solo necesitan exactamente una parte m√°s
        if parte == 4:
            registros_deseados = total_registros  # lo que sobre




---
# errores_toneladas_2025.py

import os
import pandas as pd
import glob
import time
from sqlalchemy import text
from conexion import engine

# Iniciar medici√≥n de tiempo
inicio = time.time()

# 1Ô∏è‚É£ Leer y combinar todos los archivos CSV en derechohabientes
carpeta_derechohabientes = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/derechohabientes/"
archivos_csv = glob.glob(os.path.join(carpeta_derechohabientes, "*.csv"))

RENOMBRAR_COLUMNAS = {
    "urea_25_kg_anio_actual": "urea_anio_actual",
    "dap_25_kg_anio_actual": "dap_anio_actual",
    "dap_remanente_25_kg": "dap_remanente",
    "urea_remanente_25_kg": "urea_remanente"
}

df_list = []
for archivo in archivos_csv:
    df_temp = pd.read_csv(archivo, encoding="utf-8", delimiter=",", dtype={12: str})
    df_temp.columns = df_temp.columns.str.lower().str.strip()
    df_temp = df_temp.rename(columns=RENOMBRAR_COLUMNAS)
    df_temp['archivo_origen'] = os.path.basename(archivo)  # solo el nombre del archivo
    df_list.append(df_temp)

df_derechohabientes = pd.concat(df_list, ignore_index=True)

df_derechohabientes.columns = df_derechohabientes.columns.str.lower().str.strip()
print(f"üìÇ Se han combinado {len(archivos_csv)} archivos de la carpeta derechohabientes.")

# 2Ô∏è‚É£ Eliminar registros marcados con errores
archivo_eliminar = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/derechohabientes_eliminar_2025.xlsx"
df_eliminar = pd.read_excel(archivo_eliminar, dtype=str)
df_eliminar.columns = df_eliminar.columns.str.lower().str.strip()

if "acuse_estatal" not in df_eliminar.columns:
    print("‚ùå ERROR: Falta columna 'acuse_estatal' en archivo de eliminaci√≥n.")
    exit()

antes_eliminar = len(df_derechohabientes)
df_derechohabientes = df_derechohabientes[~df_derechohabientes["acuse_estatal"].isin(df_eliminar["acuse_estatal"])]
print(f"‚úÖ Se eliminaron {antes_eliminar - len(df_derechohabientes)} registros.")

# 3Ô∏è‚É£ Agregar registros corregidos
archivo_corregidos = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/derechohabientes_corregidos_2025.csv"
if os.path.exists(archivo_corregidos):
    df_corregidos = pd.read_csv(archivo_corregidos, encoding="utf-8", delimiter=",")
    df_corregidos.columns = df_corregidos.columns.str.lower().str.strip()

    if "fecha_entrega" in df_corregidos.columns:
        df_corregidos["fecha_entrega"] = pd.to_datetime(
            df_corregidos["fecha_entrega"],
            format="%d/%m/%y",
            errors="coerce"
        ).dt.strftime("%Y-%m-%d")

    df_derechohabientes = pd.concat([df_derechohabientes, df_corregidos], ignore_index=True)
    print("‚úÖ Se combin√≥ el archivo derechohabientes_corregidos_2025.csv.")
else:
    print("‚ö†Ô∏è No se encontr√≥ archivo de registros corregidos, se omiti√≥.")

# 4Ô∏è‚É£ Conversi√≥n de tipos
columnas_integer = [
    "clave_estado_predio_capturada", "clave_municipio_predio_capturada", "clave_localidad_predio_capturada",
    "id_nu_solicitud", "id_cdf_entrega", "folio_persona", "clave_ddr", "clave_cader_ventanilla",
    "superficie_apoyada"
]
columnas_integer = [col for col in columnas_integer if col in df_derechohabientes.columns]
for col in columnas_integer:
    df_derechohabientes[col] = pd.to_numeric(df_derechohabientes[col], errors='coerce').astype("Int64")

columnas_int_con_decimales = ["dap_anio_actual", "urea_anio_actual", "dap_remanente", "urea_remanente"]
columnas_int_con_decimales = [col for col in columnas_int_con_decimales if col in df_derechohabientes.columns]
for col in columnas_int_con_decimales:
    df_derechohabientes[col] = pd.to_numeric(df_derechohabientes[col], errors='coerce').fillna(0).astype("Int64")

df_derechohabientes[["ton_dap_entregada", "ton_urea_entregada"]] = df_derechohabientes[
    ["ton_dap_entregada", "ton_urea_entregada"]
].apply(pd.to_numeric, errors='coerce').fillna(0)

if "fecha_entrega" in df_derechohabientes.columns:
    df_derechohabientes["fecha_entrega"] = pd.to_datetime(
        df_derechohabientes["fecha_entrega"],
        errors="coerce"
    ).dt.strftime("%Y-%m-%d")

# 5Ô∏è‚É£ Comparar contra padrones existentes en PostgreSQL
try:
    print("üîé Comparando contra la tabla derechohabientes_padrones_2025...")

    query_padron = """
        SELECT acuse_estatal, dap_ton, urea_ton
        FROM derechohabientes_padrones_2025
    """
    df_padron = pd.read_sql(query_padron, engine)

    # Extraer columnas necesarias del DataFrame combinado
    df_comparacion = df_derechohabientes[[
        'acuse_estatal',
        'ton_dap_entregada',
        'ton_urea_entregada',
        'dap_anio_actual',
        'urea_anio_actual',
        'dap_remanente',
        'urea_remanente',
        'archivo_origen'
    ]].copy()

    # Combinar con padr√≥n y detectar diferencias
    df_dif = df_comparacion.merge(df_padron, on='acuse_estatal', how='inner')

    df_dif = df_dif[
        (df_dif['ton_dap_entregada'] != df_dif['dap_ton']) |
        (df_dif['ton_urea_entregada'] != df_dif['urea_ton'])
    ]

    ruta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/dif_toneladas_entregadas_previas.csv"
    if df_dif.empty:
        print("‚úÖ No se encontraron diferencias en toneladas respecto al padr√≥n.")
    else:
        df_dif.to_csv(ruta_salida, index=False, encoding='utf-8-sig')
        print(f"‚ö†Ô∏è Diferencias encontradas: {len(df_dif)} registros exportados a:")
        print(f"üìÅ {ruta_salida}")

except Exception as e:
    print(f"‚ùå Error al comparar contra padr√≥n: {e}")

# Tiempo total
fin = time.time()
print(f"‚è±Ô∏è Tiempo total del proceso: {round(fin - inicio, 2)} segundos.")


---
# estructura_archivos_sistema_fertilizantes.py

import os

# Ruta base del proyecto
base_dir = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/dashboard_web"
# Archivo de salida en el escritorio
output_file = "/Users/Arturo/Desktop/estructura_y_contenido_html.txt"

with open(output_file, 'w', encoding='utf-8') as out:
    out.write("üìÅ ESTRUCTURA DE ARCHIVOS HTML Y SU CONTENIDO\n\n")
    for root, dirs, files in os.walk(base_dir):
        for file in files:
            if file.endswith(".html"):
                filepath = os.path.join(root, file)
                relative_path = os.path.relpath(filepath, base_dir)
                out.write(f"\nüîπ Ruta: {relative_path}\n")
                out.write(f"üìÑ Archivo: {file}\n")
                out.write("=" * 80 + "\n")
                try:
                    with open(filepath, 'r', encoding='utf-8') as html_file:
                        content = html_file.read()
                        out.write(content + "\n")
                except Exception as e:
                    out.write(f"[ERROR AL LEER ARCHIVO]: {e}\n")
                out.write("=" * 80 + "\n\n")

print(f"‚úÖ Archivo generado: {output_file}")



---
# exportar_abasto_x_estado_2025.py

import pandas as pd
from conexion import engine  # Aseg√∫rate de que 'conexion.py' tenga el objeto engine definido

# Nombre de la vista
vista_sql = "entradas_por_estado_td"

# Ruta de salida
ruta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/entradas_por_estado_td.csv"

try:
    print(f"üì§ Exportando vista '{vista_sql}' desde PostgreSQL...")

    # Leer los datos desde la vista
    df = pd.read_sql(f"SELECT * FROM {vista_sql};", engine)

    # Exportar a CSV con codificaci√≥n UTF-8 con BOM (compatible con Excel)
    df.to_csv(ruta_salida, index=False, encoding='utf-8-sig')

    print(f"‚úÖ Vista '{vista_sql}' exportada correctamente a:\n{ruta_salida}")

except Exception as e:
    print(f"‚ùå Error al exportar la vista '{vista_sql}': {e}")



---
# exportar_abasto_y_remanente_por_dia_sin_transito_2025.py

import pandas as pd
from conexion import engine  # Conexi√≥n centralizada con SQLAlchemy

# Nombre de la vista
vista_sql = "abasto_y_remanente_x_dia_sin_transito_2025"

# Ruta de salida
ruta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/abasto_y_remanente_x_dia_sin_transito_2025.csv"

try:
    print(f"üì§ Exportando vista '{vista_sql}' desde PostgreSQL...")

    # Leer los datos desde la vista
    df = pd.read_sql(f"SELECT * FROM {vista_sql};", engine)

    # Exportar a CSV en UTF-8 con BOM para Excel
    df.to_csv(ruta_salida, index=False, encoding='utf-8-sig')

    print(f"‚úÖ Vista '{vista_sql}' exportada correctamente a:\n{ruta_salida}")

except Exception as e:
    print(f"‚ùå Error al exportar la vista '{vista_sql}': {e}")



---
# exportar_abasto_y_remanente_x_dia_2025.py

import pandas as pd
from sqlalchemy import text
from conexion import engine, DB_NAME

# Ruta de salida
ruta_csv = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/abasto_y_remanente_x_dia_2025.csv"

# Exportar la vista a CSV
try:
    with engine.begin() as conn:
        df = pd.read_sql(text("SELECT * FROM abasto_y_remanente_x_dia_2025"), conn)
        df.to_csv(ruta_csv, index=False, encoding='utf-8-sig')
    print(f"‚úÖ Vista 'abasto_y_remanente_x_dia_2025' exportada exitosamente a: {ruta_csv}")
except Exception as e:
    print(f"‚ùå Error durante la exportaci√≥n: {e}")



---
# exportar_abasto_y_remanente_x_estado_2025.py

import pandas as pd
from conexion import engine  # Conexi√≥n centralizada con SQLAlchemy

estados = ["GUERRERO", "TLAXCALA", "MORELOS", "MICHOAC√ÅN", "DURANGO", "CHIAPAS"]
vista_sql = "abasto_y_remanente_x_estado_2025"
ruta_base = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS"

for estado in estados:
    try:
        print(f"üì§ Exportando '{vista_sql}' para {estado}...")

        query = f"SELECT * FROM {vista_sql} WHERE estado = '{estado}';"
        df = pd.read_sql(query, engine)

        ruta_salida = f"{ruta_base}/abasto_{estado.lower()}.csv"
        df.to_csv(ruta_salida, index=False, encoding="utf-8-sig")

        print(f"‚úÖ Exportado correctamente: {ruta_salida}")

    except Exception as e:
        print(f"‚ùå Error al exportar {estado}: {e}")



---
# exportar_apoyados_nacional_2025_coordinador.py

import pandas as pd
from conexion import engine  # Asumiendo conexi√≥n SQLAlchemy centralizada

query = """
SELECT
f.id_persona as id_suri,
f.nombre,
f.primer_apellido as "primer apellido",
f.segundo_apellido as "segundo apellido",
f.curp_renapo as curp,
f.clave_estado_predio_capturada as cve_estado,
f.estado_predio_capturada as estado,
f.clave_municipio_predio_capturada as cve_municipio,
f.municipio_predio_capturada as municipio,
'N/A' as bancarizado,
'APOYADO' as estatus,
'Programa de Fertilizantes' as programa
FROM full_derechohabientes_2025 f
LEFT JOIN derechohabientes d
ON f.acuse_estatal=d.acuse_estatal
WHERE d.fecha_entrega >'2025-03-31' AND d.fecha_entrega <'2025-05-01';
"""

df = pd.read_sql(query, engine)

# Exportar respetando acentos y letra √±
df.to_csv('/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/derechohabientes_apoyados_nacional_abril_2025.csv',
          index=False,
          encoding='utf-8-sig')  # ‚úÖ UTF-8 con BOM para compatibilidad Excel



---
# exportar_apoyados_nacional_2025_municipio_loc_coordinador.py

import pandas as pd
from conexion import engine  # Conexi√≥n SQLAlchemy centralizada

# Consulta SQL
query = """
SELECT
d.clave_estado_predio_capturada as "Clave de estado",
d.estado_predio_capturada as "Nombre de estado",
d.clave_municipio_predio_capturada as "Clave de municipio",
d.municipio_predio_capturada as "Nombre de municipio",
d.localidad_predio_capturada as "Nombre de localidad",
d.nombre_cader_ventanilla as "Nombre de CADER",
CASE
    WHEN d.sn_segundo_apellido IS NULL OR TRIM(d.sn_segundo_apellido) = '' THEN
      d.ln_nombre || ' ' || d.sn_primer_apellido
    ELSE
      d.ln_nombre || ' ' || d.sn_primer_apellido || ' ' || d.sn_segundo_apellido
  END AS "Nombre de Productor",
'N/A' as "Grupo de cultivos",
d.curp_renapo as "CURP",
d.cultivo as "Cultivo",
'N/A' as "Estrato",
f.regimen_hidrico as "R√©gimen H√≠drico",
d.superficie_apoyada as "Superficie (Hect√°reas)",
'N/A' as "Colmenas"
FROM
derechohabientes d
LEFT JOIN
full_derechohabientes_2025 f
ON d.acuse_estatal=f.acuse_estatal;
"""

# Leer datos
print("üì• Ejecutando consulta SQL...")
df = pd.read_sql(query, engine)
total_registros = len(df)
mitad = total_registros // 2

# Dividir en dos partes iguales
df1 = df.iloc[:mitad]
df2 = df.iloc[mitad:]

# Rutas de salida
ruta_1 = '/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/derechohabientes_apoyados_nacional__2025_parte1.csv'
ruta_2 = '/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/derechohabientes_apoyados_nacional__2025_parte2.csv'

# Exportar
print(f"üì§ Exportando {len(df1)} registros a: {ruta_1}")
df1.to_csv(ruta_1, index=False, encoding='utf-8-sig')

print(f"üì§ Exportando {len(df2)} registros a: {ruta_2}")
df2.to_csv(ruta_2, index=False, encoding='utf-8-sig')

print("‚úÖ Exportaci√≥n completada en 2 partes iguales.")



---
# exportar_apoyados_nacional_2025_municipio_loc_coordinador_2.py

import pandas as pd
from conexion import engine  # Conexi√≥n SQLAlchemy centralizada

# Consulta SQL
query = """
SELECT
d.clave_estado_predio_capturada as "Clave de estado",
d.estado_predio_capturada as "Estado",
d.clave_municipio_predio_capturada as "Clave de municipio",
d.municipio_predio_capturada as "Municipio",
d.curp_renapo as "CURP",
CASE
    WHEN d.sn_segundo_apellido IS NULL OR TRIM(d.sn_segundo_apellido) = '' THEN
      d.ln_nombre || ' ' || d.sn_primer_apellido
    ELSE
      d.ln_nombre || ' ' || d.sn_primer_apellido || ' ' || d.sn_segundo_apellido
  END AS "Nombre de completo"
FROM
derechohabientes d
LEFT JOIN
full_derechohabientes_2025 f
ON d.acuse_estatal=f.acuse_estatal;
"""

# Leer datos
print("üì• Ejecutando consulta SQL...")
df = pd.read_sql(query, engine)
total_registros = len(df)
mitad = total_registros // 2

# Dividir en dos partes iguales
df1 = df.iloc[:mitad]
df2 = df.iloc[mitad:]

# Rutas de salida
ruta_1 = '/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/derechohabientes_apoyados_nacional__03062025_parte1.csv'
ruta_2 = '/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/derechohabientes_apoyados_nacional__03062025_parte2.csv'

# Exportar
print(f"üì§ Exportando {len(df1)} registros a: {ruta_1}")
df1.to_csv(ruta_1, index=False, encoding='utf-8-sig')

print(f"üì§ Exportando {len(df2)} registros a: {ruta_2}")
df2.to_csv(ruta_2, index=False, encoding='utf-8-sig')

print("‚úÖ Exportaci√≥n completada en 2 partes iguales.")



---
# exportar_apoyados_tlaxcala_2025.py

import pandas as pd
from conexion import engine  # Asumiendo conexi√≥n SQLAlchemy centralizada

query = """
SELECT
  id_persona as id_sur,
  nombre,
  primer_apellido as "primer apellido",
  segundo_apellido as "segundo apellido",
  curp_renapo as curp,
  clave_estado_predio_capturada as cve_estado,
  estado_predio_capturada as estado,
  clave_municipio_predio_capturada as cve_municipio,
  municipio_predio_capturada as municipio,
  'N/A' as bancarizado,
  'APOYADO' as estatus,
  'Programa de Fertilizantes' as programa
FROM full_derechohabientes_2025
WHERE estado_predio_capturada = 'TLAXCALA'
  AND fecha_entrega > '2025-03-31'
  AND fecha_entrega < '2025-05-01';
"""

df = pd.read_sql(query, engine)

# Exportar respetando acentos y letra √±
df.to_csv('/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/derechohabientes_tlaxcala.csv',
          index=False,
          encoding='utf-8-sig')  # ‚úÖ UTF-8 con BOM para compatibilidad Excel



---
# exportar_avance_operativo_ceda_2025.py

import pandas as pd
from conexion import engine  # Usa la conexi√≥n centralizada

# Leer datos desde la vista entregas_diarias_2025
consulta = "SELECT * FROM avance_operativo_ceda_2025"
df = pd.read_sql(consulta, engine)

# Exportar a CSV
ruta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/avance_operativo_ceda_2025.csv"
df.to_csv(ruta_salida, index=False, encoding='utf-8-sig')

print(f"‚úÖ Archivo exportado exitosamente a: {ruta_salida}")



---
# exportar_avances_2025_td.py

import pandas as pd
from conexion import engine  # Usa tu engine centralizado con SQLAlchemy

# Nombre de la vista
vista_sql = "avances_2025"

# Ruta de salida
ruta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/avances_2025.csv"

try:
    print(f"üì§ Exportando vista '{vista_sql}' desde PostgreSQL...")
    
    # Leer los datos desde la vista
    df = pd.read_sql(f"SELECT * FROM {vista_sql};", engine)

    # Exportar a CSV con codificaci√≥n UTF-8 con BOM para compatibilidad con Excel
    df.to_csv(ruta_salida, index=False, encoding='utf-8-sig')

    print(f"‚úÖ Vista '{vista_sql}' exportada correctamente a:\n{ruta_salida}")

except Exception as e:
    print(f"‚ùå Error al exportar la vista '{vista_sql}': {e}")



---
# exportar_desempenÃÉo_estado.py

import subprocess

archivo_excel = "/Users/Arturo/AGRICULTURA/SEGUIMIENTO 2025/Desempe√±o entregas Nacional.xlsm"
macro = "ExportarPDFsPorEstado"

apple_script = f'''
tell application "Microsoft Excel"
    activate
    open POSIX file "{archivo_excel}"
    run VBScript "{macro}"
end tell
'''

subprocess.run(["osascript", "-e", apple_script])
print("üü¢ Proceso iniciado. Espera a que Excel exporte los PDFs.")



---
# exportar_entregas_diarias_2025.py

import pandas as pd
from conexion import engine  # Usa la conexi√≥n centralizada

# Leer datos desde la vista entregas_diarias_2025
consulta = "SELECT * FROM entregas_diarias_2025"
df = pd.read_sql(consulta, engine)

# Exportar a CSV
ruta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/entregas_diarias_2025.csv"
df.to_csv(ruta_salida, index=False, encoding='utf-8-sig')

print(f"‚úÖ Archivo exportado exitosamente a: {ruta_salida}")



---
# exportar_entregas_diarias_x_estado_2025.py

import pandas as pd
import unicodedata
from conexion import engine  # Conexi√≥n centralizada con SQLAlchemy

# Lista de estados que deseas exportar
estados = ["GUERRERO", "MICHOAC√ÅN", "MORELOS", "TLAXCALA", "DURANGO", "CHIAPAS"]
vista_sql = "entregas_diarias_x_estado_2025"
ruta_base = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS"

def normalizar_nombre(nombre):
    # Elimina acentos y convierte a min√∫sculas
    return unicodedata.normalize('NFKD', nombre).encode('ASCII', 'ignore').decode().lower()

for estado in estados:
    try:
        print(f"üì§ Exportando entregas para {estado}...")

        # Consulta con filtro por estado
        query = f"""
        SELECT * FROM {vista_sql}
        WHERE estado = '{estado}'
        ORDER BY fecha;
        """

        # Leer y exportar
        df = pd.read_sql(query, engine)
        nombre_archivo = f"entregas_{normalizar_nombre(estado)}.csv"
        ruta_salida = f"{ruta_base}/{nombre_archivo}"
        df.to_csv(ruta_salida, index=False, encoding="utf-8-sig")

        print(f"‚úÖ Exportado correctamente: {ruta_salida}")

    except Exception as e:
        print(f"‚ùå Error al exportar {estado}: {e}")



---
# exportar_entregas_por_estado_y_genero.py

import pandas as pd
from conexion import engine  # Usa la conexi√≥n centralizada

# Leer datos desde la vista entregas_diarias_2025
consulta = "SELECT * FROM entregas_por_estado_y_genero"
df = pd.read_sql(consulta, engine)

# Exportar a CSV
ruta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/entregas_por_estado_y_genero.csv"
df.to_csv(ruta_salida, index=False, encoding='utf-8-sig')

print(f"‚úÖ Archivo exportado exitosamente a: {ruta_salida}")



---
# exportar_entregas_semanales_2025.py

import pandas as pd
from conexion import engine  # Usa la conexi√≥n centralizada

# Leer datos desde la vista entregas_diarias_2025
consulta = "SELECT * FROM entregas_semanales_2025"
df = pd.read_sql(consulta, engine)

# Exportar a CSV
ruta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/entregas_semanales_2025.csv"
df.to_csv(ruta_salida, index=False, encoding='utf-8-sig')

print(f"‚úÖ Archivo exportado exitosamente a: {ruta_salida}")



---
# exportar_entregas_td.py

import pandas as pd
from conexion import engine  # ‚úÖ Usamos engine desde archivo centralizado

# Consulta a la vista entregas_td
query = "SELECT * FROM entregas_td;"

# Leer los datos en un DataFrame
df = pd.read_sql(query, engine)

# Ruta de salida para el archivo CSV
ruta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/entregas_td.csv"

# Exportar a CSV con codificaci√≥n UTF-8
df.to_csv(ruta_salida, index=False, encoding='utf-8-sig')

print("‚úÖ Archivo 'entregas_td.csv' exportado correctamente.")



---
# exportar_entregas_x_estado_no_ceda_y_fecha_2025.py

import pandas as pd
from sqlalchemy import text
from conexion import engine, DB_NAME

# Ruta de salida
ruta_csv = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/entregas_x_estado_no_ceda_y_fecha_2025.csv"

# Exportar la vista a CSV
try:
    with engine.begin() as conn:
        df = pd.read_sql(text("SELECT * FROM entregas_x_estado_no_ceda_y_fecha_2025"), conn)
        df.to_csv(ruta_csv, index=False, encoding='utf-8-sig')
    print(f"‚úÖ Vista 'entregas_x_estado_no_ceda_y_fecha_2025' exportada exitosamente a: {ruta_csv}")
except Exception as e:
    print(f"‚ùå Error durante la exportaci√≥n: {e}")



---
# exportar_envios_diarios_2025.py

import pandas as pd
from conexion import engine  # Conexi√≥n centralizada con SQLAlchemy

# Nombre de la vista
vista_sql = "envios_diarios_2025"

# Ruta de salida
ruta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/envios_diarios_2025.csv"

try:
    print(f"üì§ Exportando vista '{vista_sql}' desde PostgreSQL...")

    # Leer los datos desde la vista
    df = pd.read_sql(f"SELECT * FROM {vista_sql};", engine)

    # Exportar a CSV en UTF-8 con BOM para Excel
    df.to_csv(ruta_salida, index=False, encoding='utf-8-sig')

    print(f"‚úÖ Vista '{vista_sql}' exportada correctamente a:\n{ruta_salida}")

except Exception as e:
    print(f"‚ùå Error al exportar la vista '{vista_sql}': {e}")



---
# exportar_envios_diarios_x_estado_2025.py

import pandas as pd
from conexion import engine

estados = ["VERACRUZ", "CHIAPAS", "TABASCO", "SINALOA"]
vista_sql = "envios_diarios_x_estado_2025"
ruta_base = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS"

for estado in estados:
    try:
        print(f"üì§ Exportando '{vista_sql}' para {estado}...")

        query = f"SELECT * FROM {vista_sql} WHERE estado = '{estado}';"
        df = pd.read_sql(query, engine)

        ruta_salida = f"{ruta_base}/envios_{estado.lower()}.csv"
        df.to_csv(ruta_salida, index=False, encoding="utf-8-sig")

        print(f"‚úÖ Exportado correctamente: {ruta_salida}")

    except Exception as e:
        print(f"‚ùå Error al exportar {estado}: {e}")



---
# exportar_estructura_bd_fertilizantes.py

import pandas as pd
from conexion import engine  # Usamos tu conexi√≥n centralizada

# Consulta para obtener la estructura de columnas, tipos, llaves primarias y for√°neas
query = """
SELECT 
    c.table_name,
    CASE 
        WHEN t.relkind = 'r' THEN 'tabla'
        WHEN t.relkind = 'v' THEN 'vista'
        WHEN t.relkind = 'm' THEN 'vista materializada'
        ELSE t.relkind
    END AS tipo_estructura,
    c.column_name,
    c.data_type,
    c.character_maximum_length,
    c.numeric_precision,
    c.numeric_scale,
    c.is_nullable,
    CASE WHEN pk.column_name IS NOT NULL THEN 'S√≠' ELSE '' END AS es_llave_primaria,
    fk.constraint_name AS llave_foranea,
    fk.foreign_table_name AS tabla_referida,
    fk.foreign_column_name AS columna_referida
FROM information_schema.columns c
JOIN pg_class t ON t.relname = c.table_name
LEFT JOIN (
    SELECT 
        tc.table_name, 
        kcu.column_name
    FROM information_schema.table_constraints tc
    JOIN information_schema.key_column_usage kcu 
        ON tc.constraint_name = kcu.constraint_name
    WHERE tc.constraint_type = 'PRIMARY KEY'
) pk ON c.table_name = pk.table_name AND c.column_name = pk.column_name
LEFT JOIN (
    SELECT
        kcu.table_name,
        kcu.column_name,
        ccu.table_name AS foreign_table_name,
        ccu.column_name AS foreign_column_name,
        rc.constraint_name
    FROM information_schema.referential_constraints rc
    JOIN information_schema.key_column_usage kcu ON rc.constraint_name = kcu.constraint_name
    JOIN information_schema.constraint_column_usage ccu ON rc.unique_constraint_name = ccu.constraint_name
) fk ON c.table_name = fk.table_name AND c.column_name = fk.column_name
WHERE c.table_schema = 'public'
ORDER BY tipo_estructura, c.table_name, c.ordinal_position;
"""

# Ejecutar la consulta y guardar en DataFrame
df = pd.read_sql(query, engine)

# Ruta de salida
ruta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/estructura_actual_bd.csv"
df.to_csv(ruta_salida, index=False, encoding='utf-8-sig')

print("‚úÖ Estructura exportada en:", ruta_salida)



---
# exportar_estructura_tablas_y_vistas.py

import os
import pandas as pd
from conexion import engine  # Aseg√∫rate de que engine est√© correctamente configurado

# Consulta para obtener estructura de tablas y vistas
query = """
SELECT 
    c.table_schema,
    c.table_name,
    c.column_name,
    c.data_type,
    c.character_maximum_length,
    CASE WHEN tc.constraint_type = 'PRIMARY KEY' THEN 'YES' ELSE 'NO' END AS is_primary_key,
    CASE WHEN tc.constraint_type = 'FOREIGN KEY' THEN 'YES' ELSE 'NO' END AS is_foreign_key,
    kcu2.table_name AS referenced_table,
    kcu2.column_name AS referenced_column
FROM information_schema.columns c
LEFT JOIN information_schema.key_column_usage kcu
    ON c.table_name = kcu.table_name
    AND c.column_name = kcu.column_name
    AND c.table_schema = kcu.table_schema
LEFT JOIN information_schema.table_constraints tc
    ON kcu.constraint_name = tc.constraint_name
    AND kcu.table_schema = tc.table_schema
LEFT JOIN information_schema.referential_constraints rc
    ON tc.constraint_name = rc.constraint_name
LEFT JOIN information_schema.key_column_usage kcu2
    ON rc.unique_constraint_name = kcu2.constraint_name
    AND kcu.ordinal_position = kcu2.ordinal_position
WHERE c.table_schema = 'public'
ORDER BY c.table_name, c.ordinal_position;
"""

# Ruta de guardado
output_path = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/QUERIES/estructura_bd_fertilizantes.csv"

# Ejecutar y guardar
df = pd.read_sql(query, engine)
df.to_csv(output_path, index=False, encoding="utf-8")

print(f"‚úÖ Estructura exportada correctamente a: {output_path}")



---
# exportar_fletes_conteo_td.py

import pandas as pd
from conexion import engine  # ‚úÖ Usamos el engine desde el archivo de conexi√≥n centralizado

# Consulta a la vista fletes_conteo_td
query = "SELECT * FROM fletes_conteo_td;"

# Leer datos con pandas
df = pd.read_sql(query, engine)

# Ruta de salida para el archivo CSV
ruta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/fletes_conteo_td.csv"

# Exportar a CSV con codificaci√≥n UTF-8
df.to_csv(ruta_salida, index=False, encoding='utf-8-sig')

print("‚úÖ Archivo 'fletes_conteo_td.csv' exportado correctamente.")



---
# exportar_fletes_conteo_td_old.py

import pandas as pd
from sqlalchemy import create_engine

# Configuraci√≥n de la conexi√≥n a PostgreSQL
DB_USER = "postgres"
DB_PASSWORD = "Art4125r0"
DB_HOST = "localhost"
DB_PORT = "5432"
DB_NAME = "fertilizantes"

# Crear la conexi√≥n
engine = create_engine(f'postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}')

# Consulta a la vista fletes_conteo_td
query = "SELECT * FROM fletes_conteo_td;"

# Leer datos con pandas
df = pd.read_sql(query, engine)

# Ruta de salida para el archivo CSV
ruta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/fletes_conteo_td.csv"

# Exportar a CSV con codificaci√≥n UTF-8
df.to_csv(ruta_salida, index=False, encoding='utf-8-sig')

print("‚úÖ Archivo 'fletes_conteo_td.csv' exportado correctamente.")



---
# exportar_fletes_fechas_td.py

import pandas as pd
from conexion import engine  # ‚úÖ Conexi√≥n centralizada

# Consulta a la vista fletes_fechas_td
query = "SELECT * FROM fletes_fechas_td;"

# Leer datos con pandas
df = pd.read_sql(query, engine)

# Ruta de salida para el archivo CSV
ruta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/fletes_fechas_td.csv"

# Exportar a CSV con codificaci√≥n UTF-8
df.to_csv(ruta_salida, index=False, encoding='utf-8-sig')

print("‚úÖ Archivo 'fletes_fechas_td.csv' exportado correctamente.")



---
# exportar_fletes_ton_td.py

import pandas as pd
from conexion import engine  # ‚úÖ Usando conexi√≥n centralizada

# Consulta a la vista fletes_ton_td
query = "SELECT * FROM fletes_ton_td;"

# Leer datos con pandas
df = pd.read_sql(query, engine)

# Ruta destino del archivo CSV
ruta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/fletes_ton_td.csv"

# Guardar como CSV con codificaci√≥n UTF-8
df.to_csv(ruta_salida, index=False, encoding='utf-8-sig')

print("‚úÖ Archivo 'fletes_ton_td.csv' exportado correctamente.")



---
# exportar_incidencias_td.py

import pandas as pd
from conexion import engine  # ‚úÖ Usando conexi√≥n centralizada

# Consulta a la vista incidencias_td
query = "SELECT * FROM incidencias_td;"

# Leer los datos en un DataFrame
df = pd.read_sql(query, engine)

# Ruta de salida para el archivo CSV
ruta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/incidencias_td.csv"

# Exportar a CSV con codificaci√≥n UTF-8
df.to_csv(ruta_salida, index=False, encoding='utf-8-sig')

print("‚úÖ Archivo 'incidencias_td.csv' exportado correctamente.")



---
# exportar_multiples_imagenes_informe.py

import xlwings as xw
from PIL import ImageGrab
import os
import time

# Rutas
ruta_excel = "/Users/Arturo/AGRICULTURA/INFORMES/INFORME_2025.xlsm"
carpeta_imgs = "/Users/Arturo/AGRICULTURA/INFORMES/Img_Temporales"

# Lista de hojas y nombres de imagen que queremos generar (solo las primeras 3 para prueba)
hojas_a_exportar = [
    ("Durango Avances", "grafico_3.png"),
    ("Michoac√°n Avances", "grafico_4.png"),
    ("Morelos Avances", "grafico_5.png"),
]

# Crear carpeta si no existe
os.makedirs(carpeta_imgs, exist_ok=True)

# Iniciar Excel
app = xw.App(visible=False)
wb = xw.Book(ruta_excel)

def exportar_imagen(nombre_hoja, nombre_archivo):
    try:
        sht = wb.sheets[nombre_hoja]

        # Limpiar im√°genes anteriores en la hoja
        for shape in sht.api.Shapes:
            if shape.Type == 13:  # msoPicture = 13
                shape.Delete()

        # Copiar el rango como imagen
        sht.range("B5:Q18").api.CopyPicture(Appearance=1, Format=-4147)
        time.sleep(1)
        sht.api.Paste()
        time.sleep(1)
        sht.api.Pictures(sht.api.Pictures().Count).Copy()

        imagen = ImageGrab.grabclipboard()
        if imagen:
            ruta_img = os.path.join(carpeta_imgs, nombre_archivo)
            imagen.save(ruta_img, "PNG")
            print(f"‚úÖ Imagen de '{nombre_hoja}' guardada como: {ruta_img}")
        else:
            print(f"‚ùå No se encontr√≥ imagen en portapapeles para '{nombre_hoja}'.")

    except Exception as e:
        print(f"‚ùå Error en hoja '{nombre_hoja}': {e}")

# Ejecutar el proceso para las hojas seleccionadas
for hoja, archivo in hojas_a_exportar:
    exportar_imagen(hoja, archivo)

wb.close()
app.quit()



---
# exportar_multiples_imagenes_informe_2.py

import xlwings as xw
from PIL import ImageGrab
import os
import time

ruta_excel = "/Users/Arturo/AGRICULTURA/INFORMES/INFORME_2025.xlsm"
carpeta_imgs = "/Users/Arturo/AGRICULTURA/INFORMES/Img_Temporales"

# Lista de hojas y nombres de imagen que queremos generar
hojas_a_exportar = [
    ("Durango Avances", "grafico_3.png"),
    ("Michoac√°n Avances", "grafico_4.png"),
    ("Morelos Avances", "grafico_5.png"),
]

os.makedirs(carpeta_imgs, exist_ok=True)

app = xw.App(visible=False)
wb = xw.Book(ruta_excel)

def exportar_imagen_via_macro(nombre_hoja, nombre_archivo):
    try:
        macro = wb.macro("ExportarImagenDeHoja")
        macro(nombre_hoja)

        time.sleep(1.5)
        imagen = ImageGrab.grabclipboard()
        if imagen:
            ruta_img = os.path.join(carpeta_imgs, nombre_archivo)
            imagen.save(ruta_img, "PNG")
            print(f"‚úÖ Imagen de '{nombre_hoja}' guardada como: {ruta_img}")
        else:
            print(f"‚ùå No se encontr√≥ imagen en portapapeles para '{nombre_hoja}'.")

    except Exception as e:
        print(f"‚ùå Error con la hoja '{nombre_hoja}': {e}")

for hoja, archivo in hojas_a_exportar:
    exportar_imagen_via_macro(hoja, archivo)

wb.close()
app.quit()



---
# exportar_pedidos_por_dia.py

import pandas as pd
from sqlalchemy import text
from conexion import engine, DB_NAME

# Nombre de la vista y ruta del archivo CSV
vista = "pedidos_por_dia_2025"
ruta_salida = f"/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/{vista}.csv"

# Exportar a CSV
try:
    with engine.connect() as conn:
        df = pd.read_sql(text(f"SELECT * FROM {vista}"), conn)
        df.to_csv(ruta_salida, index=False, encoding='utf-8-sig')
    print(f"‚úÖ Vista '{vista}' exportada correctamente a:\n{ruta_salida}")
except Exception as e:
    print(f"‚ùå Error al exportar la vista '{vista}': {e}")



---
# exportar_pedidos_por_dia_mas_remanentes.py

import pandas as pd
from sqlalchemy import text
from conexion import engine, DB_NAME

# Nombre de la vista y ruta del archivo CSV
vista = "pedidos_y_remanentes_por_dia_2025"
ruta_salida = f"/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/{vista}.csv"

# Exportar a CSV
try:
    with engine.connect() as conn:
        df = pd.read_sql(text(f"SELECT * FROM {vista}"), conn)
        df.to_csv(ruta_salida, index=False, encoding='utf-8-sig')
    print(f"‚úÖ Vista '{vista}' exportada correctamente a:\n{ruta_salida}")
except Exception as e:
    print(f"‚ùå Error al exportar la vista '{vista}': {e}")



---
# exportar_pedidos_sigap_td.py

import pandas as pd
from conexion import engine  # ‚úÖ Conexi√≥n centralizada

# Consulta SQL
query = """
SELECT
    rd.id_ceda_agricultura,
    COALESCE(ps.dap_por_suministrar, 3) AS dap_atender_sigap,
    COALESCE(ps.urea_por_suministrar, 3) AS urea_atender_sigap,
    COALESCE(ps.catr_dap, 3) AS cap_dap_disp_sigap,
    COALESCE(ps.catr_urea, 3) AS cap_urea_disp_sigap
FROM red_distribucion rd
LEFT JOIN pedidos_sigap ps
    ON rd.id_ceda_agricultura = ps.folio_cdf
ORDER BY rd.id_ceda_agricultura;
"""

# Leer los datos en un DataFrame
df = pd.read_sql(query, engine)

# Ruta del archivo CSV
ruta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/pedidos_sigap_td.csv"

# Exportar a CSV con codificaci√≥n UTF-8
df.to_csv(ruta_salida, index=False, encoding='utf-8-sig')

print("‚úÖ Archivo 'pedidos_sigap_td.csv' exportado correctamente.")



---
# exportar_pedidos_td.py

import pandas as pd
from conexion import engine  # ‚úÖ Conexi√≥n centralizada

# Consulta a la vista pedidos_td
query = "SELECT * FROM pedidos_td;"

# Leer datos con pandas
df = pd.read_sql(query, engine)

# Ruta destino del archivo CSV
ruta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/pedidos_td.csv"

# Guardar como CSV con codificaci√≥n UTF-8
df.to_csv(ruta_salida, index=False, encoding='utf-8-sig')

print("‚úÖ Archivo 'pedidos_td.csv' exportado correctamente.")



---
# exportar_proyeccion_abasto_x_dia_2025.py

import pandas as pd
from conexion import engine
import os

# Consulta
query = "SELECT * FROM proyeccion_abasto_x_dia_2025"

# Ruta de salida
ruta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/proyeccion_abasto_x_dia_2025.csv"

# Exportaci√≥n
try:
    df = pd.read_sql(query, con=engine)
    df.to_csv(ruta_salida, index=False, encoding='utf-8-sig')
    print(f"‚úÖ Exportaci√≥n exitosa a: {ruta_salida}")
except Exception as e:
    print(f"‚ùå Error durante la exportaci√≥n: {e}")



---
# exportar_red_td.py

import pandas as pd
from conexion import engine  # ‚úÖ Importaci√≥n del motor desde archivo centralizado

# Consulta a la vista red_td
query = "SELECT * FROM red_td;"

# Leer datos con pandas
df = pd.read_sql(query, engine)

# Ruta destino del archivo CSV
ruta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/red_td.csv"

# Guardar como CSV con codificaci√≥n UTF-8
df.to_csv(ruta_salida, index=False, encoding='utf-8-sig')

print("‚úÖ Archivo 'red_td.csv' exportado correctamente.")



---
# exportar_remanentes_td.py

import pandas as pd
from conexion import engine  # ‚úÖ Usamos el motor centralizado

# Consulta a la vista remanentes_td
query = "SELECT * FROM remanentes_td;"

# Leer datos con pandas
df = pd.read_sql(query, engine)

# Ruta destino del archivo CSV
ruta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/remanentes_td.csv"

# Guardar como CSV con codificaci√≥n UTF-8
df.to_csv(ruta_salida, index=False, encoding='utf-8-sig')

print("‚úÖ Archivo 'remanentes_td.csv' exportado correctamente.")



---
# exportar_reporte_derechohabientes_bienestar_td.py

import pandas as pd
from datetime import datetime
from conexion import engine  # Usa la conexi√≥n centralizada

# Leer datos desde la vista
consulta = "SELECT * FROM reporte_derechohabientes_bienestar_td"
df = pd.read_sql(consulta, engine)

# Obtener fecha en formato mmddaaaa
fecha_actual = datetime.now().strftime("%d%m%Y")  # Resultado: "18072025"

# Definir ruta base (sin extensi√≥n)
ruta_base = f"/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/Fertilizantes - seguimiento beneficiarios - {fecha_actual}"

# Calcular cortes para 5 partes equilibradas
total = len(df)
base = total // 5
resto = total % 5

# Distribuir cortes con el resto para equilibrio
cortes = [0]
for i in range(5):
    incremento = base + (1 if i < resto else 0)
    cortes.append(cortes[-1] + incremento)

# Exportar cada parte a XLSX
for i in range(5):
    parte_df = df.iloc[cortes[i]:cortes[i+1]]
    nombre_archivo = f"{ruta_base}_parte{i+1}.csv"
    # parte_df.to_excel(nombre_archivo, index=False, engine='openpyxl')
    # Si prefieres CSV, comenta la l√≠nea anterior y descomenta esta:
    parte_df.to_csv(nombre_archivo.replace('.xlsx', '.csv'), index=False, encoding='utf-8-sig')

# Mostrar resumen
print("‚úÖ Archivos exportados exitosamente:")
for i in range(5):
    print(f"  üìÅ Parte {i+1}: {ruta_base}_parte{i+1}.csv ({cortes[i+1] - cortes[i]} registros)")


---
# exportar_reporte_fletes_bienestar.py

import pandas as pd
from conexion import engine  # Usa la conexi√≥n centralizada

# Leer datos desde la vista reporte_derechohabientes_bienestar_td
consulta = "SELECT * FROM reporte_fletes_bienestar"
df = pd.read_sql(consulta, engine)

# Exportar a CSV
ruta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/reporte_fletes_bienestar.csv"
df.to_csv(ruta_salida, index=False, encoding='utf-8-sig')

print(f"‚úÖ Archivo exportado exitosamente a: {ruta_salida}")



---
# exportar_resumen_derechohabientes_apoyados_x_municipio.py

import pandas as pd
from sqlalchemy import text
from conexion import engine, DB_NAME

# Nombre de la vista y ruta del archivo CSV
vista = "resumen_derechohabientes_apoyados_x_municipio"
ruta_salida = f"/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/{vista}.csv"

# Exportar a CSV
try:
    with engine.connect() as conn:
        df = pd.read_sql(text(f"SELECT * FROM {vista}"), conn)
        df.to_csv(ruta_salida, index=False, encoding='utf-8-sig')
    print(f"‚úÖ Vista '{vista}' exportada correctamente a:\n{ruta_salida}")
except Exception as e:
    print(f"‚ùå Error al exportar la vista '{vista}': {e}")



---
# exportar_sql_vistas_fertilizantes.py

import os

# Ruta de tu carpeta QUERIES
carpeta_queries = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/QUERIES"
archivo_salida = os.path.join(carpeta_queries, "vistas_fertilizantes.sql")

# Buscar todos los archivos .txt que empiezan con "vista"
archivos_vistas = sorted([
    f for f in os.listdir(carpeta_queries)
    if f.lower().startswith("vista") and f.endswith(".txt")
])

# Unir contenido de todos los archivos en uno solo
with open(archivo_salida, 'w', encoding='utf-8') as salida:
    for archivo in archivos_vistas:
        ruta = os.path.join(carpeta_queries, archivo)
        with open(ruta, 'r', encoding='utf-8') as f:
            salida.write(f"-- {archivo} --\n")
            salida.write(f.read())
            salida.write(f"\n-- Fin de {archivo} --\n\n")

print(f"‚úÖ Archivo generado: {archivo_salida}")



---
# exportar_transferencias_td.py

import pandas as pd
from conexion import engine  # ‚úÖ Conexi√≥n centralizada

# Consulta a la vista transferencias_td
query = "SELECT * FROM transferencias_td;"

# Leer datos con pandas
df = pd.read_sql(query, engine)

# Ruta destino del archivo CSV
ruta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/transferencias_td.csv"

# Guardar como CSV con codificaci√≥n UTF-8
df.to_csv(ruta_salida, index=False, encoding='utf-8-sig')

print("‚úÖ Archivo 'transferencias_td.csv' exportado correctamente.")



---
# grafico_Abasto Proyectado vs Real vs Pedido vs Entregado.py

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
import matplotlib.dates as mdates
from datetime import datetime, timedelta

# üìÅ Rutas
base_dir = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/"
salida_img = "/Users/Arturo/AGRICULTURA/INFORMES/Img_Temporales/grafico_comparativo_acumulado_2025.png"

archivos = {
    "proyeccion": "proyeccion_abasto_x_dia_2025.csv",
    "abasto": "abasto_y_remanente_x_dia_2025.csv",
    "entregas": "entregas_diarias_2025.csv",
    "pedidos": "pedidos_y_remanentes_por_dia_2025.csv"
}

def cargar_y_acumular(ruta, columna_valor, nuevo_nombre):
    df = pd.read_csv(ruta)
    df['fecha'] = pd.to_datetime(df['fecha'])
    df = df[df[columna_valor] > 0].sort_values('fecha')
    df[nuevo_nombre] = df[columna_valor].cumsum()
    return df[['fecha', nuevo_nombre]]

df_lineas = {
    "Abasto Proyectado": cargar_y_acumular(base_dir + archivos["proyeccion"], "abasto_proy_2025_ton", "Abasto Proyectado"),
    "Abasto Real": cargar_y_acumular(base_dir + archivos["abasto"], "total_recibido_2025_ton", "Abasto Real"),
    "Entregado": cargar_y_acumular(base_dir + archivos["entregas"], "total_ton_entregada", "Entregado"),
    "Remanente + Pedido": cargar_y_acumular(base_dir + archivos["pedidos"], "pedido_total_ton", "Remanente + Pedido")
}

colores = {
    "Abasto Proyectado": "#888888",
    "Remanente + Pedido": "#b68a1e",
    "Abasto Real": "#1e5b4f",
    "Entregado": "#662e3c"
}

# üìê Figura de 27 cm de ancho (‚âà10.63 in)
fig = plt.figure(figsize=(10.63, 6.1))
ax = fig.add_axes([0.07, 0.1, 0.73, 0.8])  # M√°s espacio para gr√°fico

# Ocultar los bordes superior y derecho
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)

valores_finales = {}
for nombre, df_l in df_lineas.items():
    color = colores[nombre]
    plt.plot(df_l["fecha"], df_l[nombre], label=nombre, color=color, linewidth=2.5, marker='o', markersize=3)
    plt.fill_between(df_l["fecha"], 0, df_l[nombre], color=color, alpha=0.15)

    punto_final = df_l.iloc[-1]
    valores_finales[nombre] = punto_final[nombre]

    etiqueta = f"{int(punto_final[nombre] / 1000):,} mil".replace(",", ".")
    plt.text(punto_final["fecha"], punto_final[nombre], etiqueta,
             color=color, fontsize=9, ha='center', va='bottom', fontweight='bold')

meses_es = {
    1: "ene", 2: "feb", 3: "mar", 4: "abr", 5: "may", 6: "jun",
    7: "jul", 8: "ago", 9: "sep", 10: "oct", 11: "nov", 12: "dic"
}
def formato_fecha_es(fecha, _):
    if isinstance(fecha, (int, float)):
        fecha = mdates.num2date(fecha)
    return f"{meses_es[fecha.month]}\n{fecha.year}"

ax.xaxis.set_major_locator(mdates.MonthLocator())
ax.xaxis.set_major_formatter(FuncFormatter(formato_fecha_es))
plt.xticks(rotation=0)
plt.xlabel("Fecha", fontsize=12)
plt.ylabel("Toneladas de Fertilizante", fontsize=12)
plt.grid(True, linestyle="--", alpha=0.5)
ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f"{int(x / 1000):,} mil".replace(",", ".")))
ax.set_facecolor("white")
plt.gcf().patch.set_facecolor("none")

plt.legend(loc="upper center", bbox_to_anchor=(0.5, 1.12), ncol=4, fontsize=9, frameon=False)

ayer = datetime.today().date() - timedelta(days=1)
valores_ayer = {}

for nombre, df_l in df_lineas.items():
    df_l["fecha"] = pd.to_datetime(df_l["fecha"]).dt.date
    df_filtrado = df_l[df_l["fecha"] <= ayer]
    valores_ayer[nombre] = df_filtrado.iloc[-1][nombre] if not df_filtrado.empty else 0

remanente_y_pedido = valores_ayer.get("Remanente + Pedido", 0)
abasto_y_remanente = valores_ayer.get("Abasto Real", 0)
proyectado = valores_ayer.get("Abasto Proyectado", 0)
entregado = valores_ayer.get("Entregado", 0)

brechas = {
    "Remanente y Pedido": remanente_y_pedido,
    "Abasto y Remanente": abasto_y_remanente,
    "[Abasto Proyectado]": proyectado,
    "Entregado": entregado,
    "Abasto Real - Proyectado": abasto_y_remanente - proyectado,
    "Entregado - Proyectado": entregado - proyectado,
    "Pedidos por Atender": remanente_y_pedido - abasto_y_remanente,
    "Inventarios m√°s tr√°nsito": abasto_y_remanente - entregado
}

# üìä Panel ampliado
panel_x = 1.035
top_y = 0.92
linea = 0

def draw_panel_text(text, size=10, color="black", bold=False, box=False, bg="#444444", fg="white", extra_line_gap=0):
    global linea
    y = top_y - 0.043 * linea
    weight = 'bold' if bold else 'normal'
    if box:
        plt.text(panel_x, y, text, fontsize=size, fontweight=weight, color=fg,
                 transform=ax.transAxes, verticalalignment='top',
                 bbox=dict(facecolor=bg, edgecolor='none', boxstyle='round,pad=0.3'))
    else:
        plt.text(panel_x, y, text, fontsize=size, fontweight=weight, color=color,
                 transform=ax.transAxes, verticalalignment='top')
    linea += 1 + extra_line_gap

draw_panel_text("Estad√≠sticas al d√≠a de\nAyer (ton.)", bold=True, box=True, extra_line_gap=1)

for clave in ["Remanente y Pedido", "Abasto y Remanente", "[Abasto Proyectado]", "Entregado"]:
    draw_panel_text(f"{brechas[clave]:,.2f}".replace(",", "X").replace(".", ",").replace("X", "."), bold=True)
    draw_panel_text(clave, size=9)

draw_panel_text("Brechas entre L√≠neas\nal d√≠a de Ayer (ton.)", bold=True, box=True,
                bg="#b68a1e", fg="white", extra_line_gap=1)

for clave in ["Abasto Real - Proyectado", "Entregado - Proyectado", "Pedidos por Atender", "Inventarios m√°s tr√°nsito"]:
    draw_panel_text(f"{brechas[clave]:,.2f}".replace(",", "X").replace(".", ",").replace("X", "."), bold=True)
    draw_panel_text(clave, size=9)

# üíæ Exportar sin recorte
plt.savefig(salida_img, dpi=300, transparent=True)
plt.close()
print(f"‚úÖ Gr√°fico final ajustado y guardado en: {salida_img}")



---
# grafico_Abasto_vs_Entregas_chiapas.py

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
from datetime import datetime

# üìå Configuraci√≥n
estado = "CHIAPAS"  # Cambia el estado aqu√≠ (debe ir con acento si lo lleva en datos)
nombre_archivo = estado.lower().replace("√°", "a").replace("√©", "e").replace("√≠", "i").replace("√≥", "o").replace("√∫", "u").replace("√±", "n")

ruta_abasto = f"/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/abasto_{nombre_archivo}.csv"
ruta_entregas = f"/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/entregas_{nombre_archivo}.csv"
ruta_salida = f"/Users/Arturo/AGRICULTURA/INFORMES/Img_Temporales/grafico_abasto_{nombre_archivo}.png"

# üìÖ Diccionarios para etiquetas
DIAS = {0: "lun", 1: "mar", 2: "mi√©", 3: "jue", 4: "vier", 5: "s√°b", 6: "dom"}
MESES = {1: "ene", 2: "feb", 3: "mar", 4: "abr", 5: "may", 6: "jun", 7: "jul", 8: "ago", 9: "sep", 10: "oct", 11: "nov", 12: "dic"}

# üß™ Cargar datos
df_abasto = pd.read_csv(ruta_abasto)
df_entregas = pd.read_csv(ruta_entregas)

df_abasto['fecha'] = pd.to_datetime(df_abasto['fecha'])
df_entregas['fecha'] = pd.to_datetime(df_entregas['fecha'])

df = pd.merge(
    df_abasto[['fecha', 'total_recibido_2025_ton']],
    df_entregas[['fecha', 'total_ton_entregada']],
    on='fecha', how='outer'
).fillna(0).sort_values('fecha')

# üìÖ Agregar info de semana
df['dia_semana'] = df['fecha'].dt.dayofweek
df['semana'] = df['fecha'].dt.isocalendar().week

# üìå Etiquetas X
etiquetas_fecha = df[df['dia_semana'] == 4]['fecha'].tolist()
etiquetas_formato = [f"{DIAS[f.weekday()]}\n{f.day:02d}-{MESES[f.month]}" for f in etiquetas_fecha]

# üî∫ D√≠as para anotar
lunes_jueves = df[df['dia_semana'].isin([0, 1, 2, 3])]
idx_max = (lunes_jueves.assign(suma=lunes_jueves['total_recibido_2025_ton'] + lunes_jueves['total_ton_entregada'])
            .groupby('semana')['suma'].idxmax())
fechas_anotar = sorted(set(etiquetas_fecha + df.loc[idx_max, 'fecha'].tolist()))

# üé® Crear gr√°fico
fig, ax = plt.subplots(figsize=(12.6, 6.5))
ax.set_facecolor('white')
plt.gcf().patch.set_facecolor('none')

# √Åreas y l√≠neas
plt.fill_between(df['fecha'], df['total_recibido_2025_ton'], color='#1e5b4f', alpha=0.25)
plt.plot(df['fecha'], df['total_recibido_2025_ton'], color='#1e5b4f', linewidth=2.5, marker='o', label='Recibido (ton)')

plt.fill_between(df['fecha'], df['total_ton_entregada'], color='#a57f2c', alpha=0.25)
plt.plot(df['fecha'], df['total_ton_entregada'], color='#a57f2c', linewidth=2.5, marker='o', label='Entregado (ton)')

# L√≠neas viernes
for f in etiquetas_fecha:
    plt.axvline(x=f, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)

# Anotar valores clave
for _, fila in df[df['fecha'].isin(fechas_anotar)].iterrows():
    y = max(fila['total_recibido_2025_ton'], fila['total_ton_entregada'])
    if y <= 10000:
        plt.text(fila['fecha'], y + 300, f"{int(y):,}", fontsize=8, ha='center', va='bottom', rotation=90)

# KPIs
recibido = int(df['total_recibido_2025_ton'].sum())
entregado = int(df['total_ton_entregada'].sum())
inventario = recibido - entregado

plt.text(0.50, 1.13, f"{recibido:,}", transform=ax.transAxes, ha='left', fontsize=13, color='#1e5b4f', weight='bold')
plt.text(0.50, 1.08, "Recibido Total (ton)", transform=ax.transAxes, ha='left', fontsize=10.5)

plt.text(0.70, 1.13, f"{entregado:,}", transform=ax.transAxes, ha='left', fontsize=13, color='#a57f2c', weight='bold')
plt.text(0.70, 1.08, "Entregado Total (ton)", transform=ax.transAxes, ha='left', fontsize=10.5)

plt.text(0.90, 1.13, f"{inventario:,}", transform=ax.transAxes, ha='left', fontsize=13, color='#c60052', weight='bold')
plt.text(0.90, 1.08, "Inventarios (ton)", transform=ax.transAxes, ha='left', fontsize=10.5)

# Estilo y ejes
plt.xlabel("Fecha")
plt.ylabel("Toneladas")
plt.xlim(df['fecha'].min(), df['fecha'].max())
ax.get_yaxis().set_major_formatter(FuncFormatter(lambda x, _: f"{int(x):,}"))
ax.set_xticks(etiquetas_fecha)
ax.set_xticklabels(etiquetas_formato, fontsize=8, ha='center')

plt.legend(loc='upper left', bbox_to_anchor=(0, 1.13), fontsize=9)
plt.tight_layout()
plt.savefig(ruta_salida, dpi=300, bbox_inches='tight', transparent=True)
plt.close()

print(f"‚úÖ Gr√°fico generado: {ruta_salida}")



---
# grafico_Abasto_vs_Entregas_durango.py

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
from datetime import datetime

# üìå Configuraci√≥n
estado = "DURANGO"  # Cambia el estado aqu√≠ (debe ir con acento si lo lleva en datos)
nombre_archivo = estado.lower().replace("√°", "a").replace("√©", "e").replace("√≠", "i").replace("√≥", "o").replace("√∫", "u").replace("√±", "n")

ruta_abasto = f"/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/abasto_{nombre_archivo}.csv"
ruta_entregas = f"/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/entregas_{nombre_archivo}.csv"
ruta_salida = f"/Users/Arturo/AGRICULTURA/INFORMES/Img_Temporales/grafico_abasto_{nombre_archivo}.png"

# üìÖ Diccionarios para etiquetas
DIAS = {0: "lun", 1: "mar", 2: "mi√©", 3: "jue", 4: "vier", 5: "s√°b", 6: "dom"}
MESES = {1: "ene", 2: "feb", 3: "mar", 4: "abr", 5: "may", 6: "jun", 7: "jul", 8: "ago", 9: "sep", 10: "oct", 11: "nov", 12: "dic"}

# üß™ Cargar datos
df_abasto = pd.read_csv(ruta_abasto)
df_entregas = pd.read_csv(ruta_entregas)

df_abasto['fecha'] = pd.to_datetime(df_abasto['fecha'])
df_entregas['fecha'] = pd.to_datetime(df_entregas['fecha'])

df = pd.merge(
    df_abasto[['fecha', 'total_recibido_2025_ton']],
    df_entregas[['fecha', 'total_ton_entregada']],
    on='fecha', how='outer'
).fillna(0).sort_values('fecha')

# üìÖ Agregar info de semana
df['dia_semana'] = df['fecha'].dt.dayofweek
df['semana'] = df['fecha'].dt.isocalendar().week

# üìå Etiquetas X
etiquetas_fecha = df[df['dia_semana'] == 4]['fecha'].tolist()
etiquetas_formato = [f"{DIAS[f.weekday()]}\n{f.day:02d}-{MESES[f.month]}" for f in etiquetas_fecha]

# üî∫ D√≠as para anotar
lunes_jueves = df[df['dia_semana'].isin([0, 1, 2, 3])]
idx_max = (lunes_jueves.assign(suma=lunes_jueves['total_recibido_2025_ton'] + lunes_jueves['total_ton_entregada'])
            .groupby('semana')['suma'].idxmax())
fechas_anotar = sorted(set(etiquetas_fecha + df.loc[idx_max, 'fecha'].tolist()))

# üé® Crear gr√°fico
fig, ax = plt.subplots(figsize=(12.6, 6.5))
ax.set_facecolor('white')
plt.gcf().patch.set_facecolor('none')

# √Åreas y l√≠neas
plt.fill_between(df['fecha'], df['total_recibido_2025_ton'], color='#1e5b4f', alpha=0.25)
plt.plot(df['fecha'], df['total_recibido_2025_ton'], color='#1e5b4f', linewidth=2.5, marker='o', label='Recibido (ton)')

plt.fill_between(df['fecha'], df['total_ton_entregada'], color='#a57f2c', alpha=0.25)
plt.plot(df['fecha'], df['total_ton_entregada'], color='#a57f2c', linewidth=2.5, marker='o', label='Entregado (ton)')

# L√≠neas viernes
for f in etiquetas_fecha:
    plt.axvline(x=f, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)

# Anotar valores clave
for _, fila in df[df['fecha'].isin(fechas_anotar)].iterrows():
    y = max(fila['total_recibido_2025_ton'], fila['total_ton_entregada'])
    if y <= 10000:
        plt.text(fila['fecha'], y + 300, f"{int(y):,}", fontsize=8, ha='center', va='bottom', rotation=90)

# KPIs
recibido = int(df['total_recibido_2025_ton'].sum())
entregado = int(df['total_ton_entregada'].sum())
inventario = recibido - entregado

plt.text(0.50, 1.13, f"{recibido:,}", transform=ax.transAxes, ha='left', fontsize=13, color='#1e5b4f', weight='bold')
plt.text(0.50, 1.08, "Recibido Total (ton)", transform=ax.transAxes, ha='left', fontsize=10.5)

plt.text(0.70, 1.13, f"{entregado:,}", transform=ax.transAxes, ha='left', fontsize=13, color='#a57f2c', weight='bold')
plt.text(0.70, 1.08, "Entregado Total (ton)", transform=ax.transAxes, ha='left', fontsize=10.5)

plt.text(0.90, 1.13, f"{inventario:,}", transform=ax.transAxes, ha='left', fontsize=13, color='#c60052', weight='bold')
plt.text(0.90, 1.08, "Inventarios (ton)", transform=ax.transAxes, ha='left', fontsize=10.5)

# Estilo y ejes
plt.xlabel("Fecha")
plt.ylabel("Toneladas")
plt.xlim(df['fecha'].min(), df['fecha'].max())
ax.get_yaxis().set_major_formatter(FuncFormatter(lambda x, _: f"{int(x):,}"))
ax.set_xticks(etiquetas_fecha)
ax.set_xticklabels(etiquetas_formato, fontsize=8, ha='center')

plt.legend(loc='upper left', bbox_to_anchor=(0, 1.13), fontsize=9)
plt.tight_layout()
plt.savefig(ruta_salida, dpi=300, bbox_inches='tight', transparent=True)
plt.close()

print(f"‚úÖ Gr√°fico generado: {ruta_salida}")



---
# grafico_Abasto_vs_Entregas_guerrero.py

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
from datetime import datetime

# üìå Estado
estado = "GUERRERO"
nombre_archivo = estado.lower().replace("√°", "a").replace("√©", "e").replace("√≠", "i") \
                               .replace("√≥", "o").replace("√∫", "u").replace("√±", "n")

# üìÇ Rutas
ruta_abasto = f"/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/abasto_{nombre_archivo}.csv"
ruta_entregas = f"/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/entregas_{nombre_archivo}.csv"
ruta_salida = f"/Users/Arturo/AGRICULTURA/INFORMES/Img_Temporales/grafico_abasto_{nombre_archivo}.png"

# üìÖ Diccionarios etiquetas
DIAS = {0: "lun", 1: "mar", 2: "mi√©", 3: "jue", 4: "vier", 5: "s√°b", 6: "dom"}
MESES = {1: "ene", 2: "feb", 3: "mar", 4: "abr", 5: "may", 6: "jun",
         7: "jul", 8: "ago", 9: "sep", 10: "oct", 11: "nov", 12: "dic"}

# üß™ Cargar datos
df_abasto = pd.read_csv(ruta_abasto)
df_entregas = pd.read_csv(ruta_entregas)

df_abasto['fecha'] = pd.to_datetime(df_abasto['fecha'])
df_entregas['fecha'] = pd.to_datetime(df_entregas['fecha'])

# ‚ùó Excluir datos vac√≠os
df_abasto = df_abasto[df_abasto['total_recibido_2025_ton'] > 0]
df_entregas = df_entregas[df_entregas['total_ton_entregada'] > 0]

# üîó Unir datos
df = pd.merge(
    df_abasto[['fecha', 'total_recibido_2025_ton']],
    df_entregas[['fecha', 'total_ton_entregada']],
    on='fecha', how='outer'
).fillna(0).sort_values('fecha')

df['dia_semana'] = df['fecha'].dt.dayofweek
df['semana'] = df['fecha'].dt.isocalendar().week

# üìå Etiquetas X (viernes)
etiquetas_fecha = df[df['dia_semana'] == 4]['fecha'].tolist()
etiquetas_formato = [f"{DIAS[f.weekday()]}\n{f.day:02d}-{MESES[f.month]}" for f in etiquetas_fecha]

# üî∫ D√≠as con valor alto de lunes a jueves
lunes_jueves = df[df['dia_semana'].isin([0, 1, 2, 3])]
idx_max = (lunes_jueves.assign(suma=lunes_jueves['total_recibido_2025_ton'] + lunes_jueves['total_ton_entregada'])
           .groupby('semana')['suma'].idxmax())
fechas_anotar = sorted(set(etiquetas_fecha + df.loc[idx_max, 'fecha'].tolist()))

# üé® Crear gr√°fico
fig, ax = plt.subplots(figsize=(12.6, 6.5))
ax.set_facecolor('white')
plt.gcf().patch.set_facecolor('none')

# √Åreas y l√≠neas
plt.fill_between(df['fecha'], df['total_recibido_2025_ton'], color='#1e5b4f', alpha=0.25)
plt.plot(df['fecha'], df['total_recibido_2025_ton'], color='#1e5b4f', linewidth=2.5, marker='o', label='Recibido (ton)')

plt.fill_between(df['fecha'], df['total_ton_entregada'], color='#a57f2c', alpha=0.25)
plt.plot(df['fecha'], df['total_ton_entregada'], color='#a57f2c', linewidth=2.5, marker='o', label='Entregado (ton)')

# L√≠neas viernes
for f in etiquetas_fecha:
    plt.axvline(x=f, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)

# Anotar valores relevantes (si no exceden 6000)
for _, fila in df[df['fecha'].isin(fechas_anotar)].iterrows():
    y = max(fila['total_recibido_2025_ton'], fila['total_ton_entregada'])
    if y <= 6000:
        plt.text(fila['fecha'], y + 300, f"{int(y):,}", fontsize=8, ha='center', va='bottom', rotation=90)

# Anotar valor m√°ximo
fila_max = df.loc[(df['total_recibido_2025_ton'] + df['total_ton_entregada']).idxmax()]
valor_max = max(fila_max['total_recibido_2025_ton'], fila_max['total_ton_entregada'])
if valor_max <= 6000:
    plt.annotate(
        f"{int(valor_max):,}",
        xy=(fila_max['fecha'], 5800), xytext=(fila_max['fecha'], 6000),
        textcoords='data',
        arrowprops=dict(facecolor='black', arrowstyle='->', lw=0.8),
        ha='center', fontsize=9, color='black'
    )

# KPIs
recibido = int(df['total_recibido_2025_ton'].sum())
entregado = int(df['total_ton_entregada'].sum())
inventario = recibido - entregado

plt.text(0.50, 1.13, f"{recibido:,}", transform=ax.transAxes, ha='left', fontsize=13, color='#1e5b4f', weight='bold')
plt.text(0.50, 1.08, "Recibido Total (ton)", transform=ax.transAxes, ha='left', fontsize=10.5)

plt.text(0.70, 1.13, f"{entregado:,}", transform=ax.transAxes, ha='left', fontsize=13, color='#a57f2c', weight='bold')
plt.text(0.70, 1.08, "Entregado Total (ton)", transform=ax.transAxes, ha='left', fontsize=10.5)

plt.text(0.90, 1.13, f"{inventario:,}", transform=ax.transAxes, ha='left', fontsize=13, color='#c60052', weight='bold')
plt.text(0.90, 1.08, "Inventarios (ton)", transform=ax.transAxes, ha='left', fontsize=10.5)

# Ejes
plt.xlabel("Fecha")
plt.ylabel("Toneladas")
plt.xlim(df['fecha'].min(), df['fecha'].max())
plt.ylim([0, 6000])
ax.get_yaxis().set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))
ax.set_xticks(etiquetas_fecha)
ax.set_xticklabels(etiquetas_formato, fontsize=8, ha='center')
plt.yticks(color='black')
ax.spines['top'].set_visible(False)
ax.yaxis.grid(True, linestyle='--', linewidth=0.5, alpha=0.5)

# Leyenda y guardar
plt.legend(loc='upper left', bbox_to_anchor=(0, 1.13), fontsize=9)
plt.tight_layout()
plt.savefig(ruta_salida, dpi=300, bbox_inches='tight', transparent=True)
plt.close()

print(f"‚úÖ Gr√°fico de GUERRERO con l√≠mite 6,000 generado en: {ruta_salida}")



---
# grafico_Abasto_vs_Entregas_michoacan.py

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
from datetime import datetime

# üìå Configuraci√≥n
estado = "MICHOAC√ÅN"  # Cambia el estado aqu√≠ (debe ir con acento si lo lleva en datos)
nombre_archivo = estado.lower().replace("√°", "a").replace("√©", "e").replace("√≠", "i").replace("√≥", "o").replace("√∫", "u").replace("√±", "n")

ruta_abasto = f"/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/abasto_{nombre_archivo}.csv"
ruta_entregas = f"/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/entregas_{nombre_archivo}.csv"
ruta_salida = f"/Users/Arturo/AGRICULTURA/INFORMES/Img_Temporales/grafico_abasto_{nombre_archivo}.png"

# üìÖ Diccionarios para etiquetas
DIAS = {0: "lun", 1: "mar", 2: "mi√©", 3: "jue", 4: "vier", 5: "s√°b", 6: "dom"}
MESES = {1: "ene", 2: "feb", 3: "mar", 4: "abr", 5: "may", 6: "jun", 7: "jul", 8: "ago", 9: "sep", 10: "oct", 11: "nov", 12: "dic"}

# üß™ Cargar datos
df_abasto = pd.read_csv(ruta_abasto)
df_entregas = pd.read_csv(ruta_entregas)

df_abasto['fecha'] = pd.to_datetime(df_abasto['fecha'])
df_entregas['fecha'] = pd.to_datetime(df_entregas['fecha'])

df = pd.merge(
    df_abasto[['fecha', 'total_recibido_2025_ton']],
    df_entregas[['fecha', 'total_ton_entregada']],
    on='fecha', how='outer'
).fillna(0).sort_values('fecha')

# üìÖ Agregar info de semana
df['dia_semana'] = df['fecha'].dt.dayofweek
df['semana'] = df['fecha'].dt.isocalendar().week

# üìå Etiquetas X
etiquetas_fecha = df[df['dia_semana'] == 4]['fecha'].tolist()
etiquetas_formato = [f"{DIAS[f.weekday()]}\n{f.day:02d}-{MESES[f.month]}" for f in etiquetas_fecha]

# üî∫ D√≠as para anotar
lunes_jueves = df[df['dia_semana'].isin([0, 1, 2, 3])]
idx_max = (lunes_jueves.assign(suma=lunes_jueves['total_recibido_2025_ton'] + lunes_jueves['total_ton_entregada'])
            .groupby('semana')['suma'].idxmax())
fechas_anotar = sorted(set(etiquetas_fecha + df.loc[idx_max, 'fecha'].tolist()))

# üé® Crear gr√°fico
fig, ax = plt.subplots(figsize=(12.6, 6.5))
ax.set_facecolor('white')
plt.gcf().patch.set_facecolor('none')

# √Åreas y l√≠neas
plt.fill_between(df['fecha'], df['total_recibido_2025_ton'], color='#1e5b4f', alpha=0.25)
plt.plot(df['fecha'], df['total_recibido_2025_ton'], color='#1e5b4f', linewidth=2.5, marker='o', label='Recibido (ton)')

plt.fill_between(df['fecha'], df['total_ton_entregada'], color='#a57f2c', alpha=0.25)
plt.plot(df['fecha'], df['total_ton_entregada'], color='#a57f2c', linewidth=2.5, marker='o', label='Entregado (ton)')

# L√≠neas viernes
for f in etiquetas_fecha:
    plt.axvline(x=f, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)

# Anotar valores clave
for _, fila in df[df['fecha'].isin(fechas_anotar)].iterrows():
    y = max(fila['total_recibido_2025_ton'], fila['total_ton_entregada'])
    if y <= 10000:
        plt.text(fila['fecha'], y + 300, f"{int(y):,}", fontsize=8, ha='center', va='bottom', rotation=90)

# KPIs
recibido = int(df['total_recibido_2025_ton'].sum())
entregado = int(df['total_ton_entregada'].sum())
inventario = recibido - entregado

plt.text(0.50, 1.13, f"{recibido:,}", transform=ax.transAxes, ha='left', fontsize=13, color='#1e5b4f', weight='bold')
plt.text(0.50, 1.08, "Recibido Total (ton)", transform=ax.transAxes, ha='left', fontsize=10.5)

plt.text(0.70, 1.13, f"{entregado:,}", transform=ax.transAxes, ha='left', fontsize=13, color='#a57f2c', weight='bold')
plt.text(0.70, 1.08, "Entregado Total (ton)", transform=ax.transAxes, ha='left', fontsize=10.5)

plt.text(0.90, 1.13, f"{inventario:,}", transform=ax.transAxes, ha='left', fontsize=13, color='#c60052', weight='bold')
plt.text(0.90, 1.08, "Inventarios (ton)", transform=ax.transAxes, ha='left', fontsize=10.5)

# Estilo y ejes
plt.xlabel("Fecha")
plt.ylabel("Toneladas")
plt.xlim(df['fecha'].min(), df['fecha'].max())
ax.get_yaxis().set_major_formatter(FuncFormatter(lambda x, _: f"{int(x):,}"))
ax.set_xticks(etiquetas_fecha)
ax.set_xticklabels(etiquetas_formato, fontsize=8, ha='center')

plt.legend(loc='upper left', bbox_to_anchor=(0, 1.13), fontsize=9)
plt.tight_layout()
plt.savefig(ruta_salida, dpi=300, bbox_inches='tight', transparent=True)
plt.close()

print(f"‚úÖ Gr√°fico generado: {ruta_salida}")



---
# grafico_Abasto_vs_Entregas_morelos.py

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
from datetime import datetime

# üìå Configuraci√≥n
estado = "MORELOS"  # Cambia el estado aqu√≠ (debe ir con acento si lo lleva en datos)
nombre_archivo = estado.lower().replace("√°", "a").replace("√©", "e").replace("√≠", "i").replace("√≥", "o").replace("√∫", "u").replace("√±", "n")

ruta_abasto = f"/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/abasto_{nombre_archivo}.csv"
ruta_entregas = f"/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/entregas_{nombre_archivo}.csv"
ruta_salida = f"/Users/Arturo/AGRICULTURA/INFORMES/Img_Temporales/grafico_abasto_{nombre_archivo}.png"

# üìÖ Diccionarios para etiquetas
DIAS = {0: "lun", 1: "mar", 2: "mi√©", 3: "jue", 4: "vier", 5: "s√°b", 6: "dom"}
MESES = {1: "ene", 2: "feb", 3: "mar", 4: "abr", 5: "may", 6: "jun", 7: "jul", 8: "ago", 9: "sep", 10: "oct", 11: "nov", 12: "dic"}

# üß™ Cargar datos
df_abasto = pd.read_csv(ruta_abasto)
df_entregas = pd.read_csv(ruta_entregas)

df_abasto['fecha'] = pd.to_datetime(df_abasto['fecha'])
df_entregas['fecha'] = pd.to_datetime(df_entregas['fecha'])

df = pd.merge(
    df_abasto[['fecha', 'total_recibido_2025_ton']],
    df_entregas[['fecha', 'total_ton_entregada']],
    on='fecha', how='outer'
).fillna(0).sort_values('fecha')

# üìÖ Agregar info de semana
df['dia_semana'] = df['fecha'].dt.dayofweek
df['semana'] = df['fecha'].dt.isocalendar().week

# üìå Etiquetas X
etiquetas_fecha = df[df['dia_semana'] == 4]['fecha'].tolist()
etiquetas_formato = [f"{DIAS[f.weekday()]}\n{f.day:02d}-{MESES[f.month]}" for f in etiquetas_fecha]

# üî∫ D√≠as para anotar
lunes_jueves = df[df['dia_semana'].isin([0, 1, 2, 3])]
idx_max = (lunes_jueves.assign(suma=lunes_jueves['total_recibido_2025_ton'] + lunes_jueves['total_ton_entregada'])
            .groupby('semana')['suma'].idxmax())
fechas_anotar = sorted(set(etiquetas_fecha + df.loc[idx_max, 'fecha'].tolist()))

# üé® Crear gr√°fico
fig, ax = plt.subplots(figsize=(12.6, 6.5))
ax.set_facecolor('white')
plt.gcf().patch.set_facecolor('none')

# √Åreas y l√≠neas
plt.fill_between(df['fecha'], df['total_recibido_2025_ton'], color='#1e5b4f', alpha=0.25)
plt.plot(df['fecha'], df['total_recibido_2025_ton'], color='#1e5b4f', linewidth=2.5, marker='o', label='Recibido (ton)')

plt.fill_between(df['fecha'], df['total_ton_entregada'], color='#a57f2c', alpha=0.25)
plt.plot(df['fecha'], df['total_ton_entregada'], color='#a57f2c', linewidth=2.5, marker='o', label='Entregado (ton)')

# L√≠neas viernes
for f in etiquetas_fecha:
    plt.axvline(x=f, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)

# Anotar valores clave
for _, fila in df[df['fecha'].isin(fechas_anotar)].iterrows():
    y = max(fila['total_recibido_2025_ton'], fila['total_ton_entregada'])
    if y <= 10000:
        plt.text(fila['fecha'], y + 300, f"{int(y):,}", fontsize=8, ha='center', va='bottom', rotation=90)

# KPIs
recibido = int(df['total_recibido_2025_ton'].sum())
entregado = int(df['total_ton_entregada'].sum())
inventario = recibido - entregado

plt.text(0.50, 1.13, f"{recibido:,}", transform=ax.transAxes, ha='left', fontsize=13, color='#1e5b4f', weight='bold')
plt.text(0.50, 1.08, "Recibido Total (ton)", transform=ax.transAxes, ha='left', fontsize=10.5)

plt.text(0.70, 1.13, f"{entregado:,}", transform=ax.transAxes, ha='left', fontsize=13, color='#a57f2c', weight='bold')
plt.text(0.70, 1.08, "Entregado Total (ton)", transform=ax.transAxes, ha='left', fontsize=10.5)

plt.text(0.90, 1.13, f"{inventario:,}", transform=ax.transAxes, ha='left', fontsize=13, color='#c60052', weight='bold')
plt.text(0.90, 1.08, "Inventarios (ton)", transform=ax.transAxes, ha='left', fontsize=10.5)

# Estilo y ejes
plt.xlabel("Fecha")
plt.ylabel("Toneladas")
plt.xlim(df['fecha'].min(), df['fecha'].max())
ax.get_yaxis().set_major_formatter(FuncFormatter(lambda x, _: f"{int(x):,}"))
ax.set_xticks(etiquetas_fecha)
ax.set_xticklabels(etiquetas_formato, fontsize=8, ha='center')

plt.legend(loc='upper left', bbox_to_anchor=(0, 1.13), fontsize=9)
plt.tight_layout()
plt.savefig(ruta_salida, dpi=300, bbox_inches='tight', transparent=True)
plt.close()

print(f"‚úÖ Gr√°fico generado: {ruta_salida}")



---
# grafico_Abasto_vs_Entregas_tlaxcala.py

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
from datetime import datetime

# üìå Configuraci√≥n
estado = "TLAXCALA"  # Cambia el estado aqu√≠ (debe ir con acento si lo lleva en datos)
nombre_archivo = estado.lower().replace("√°", "a").replace("√©", "e").replace("√≠", "i").replace("√≥", "o").replace("√∫", "u").replace("√±", "n")

ruta_abasto = f"/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/abasto_{nombre_archivo}.csv"
ruta_entregas = f"/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/entregas_{nombre_archivo}.csv"
ruta_salida = f"/Users/Arturo/AGRICULTURA/INFORMES/Img_Temporales/grafico_abasto_{nombre_archivo}.png"

# üìÖ Diccionarios para etiquetas
DIAS = {0: "lun", 1: "mar", 2: "mi√©", 3: "jue", 4: "vier", 5: "s√°b", 6: "dom"}
MESES = {1: "ene", 2: "feb", 3: "mar", 4: "abr", 5: "may", 6: "jun", 7: "jul", 8: "ago", 9: "sep", 10: "oct", 11: "nov", 12: "dic"}

# üß™ Cargar datos
df_abasto = pd.read_csv(ruta_abasto)
df_entregas = pd.read_csv(ruta_entregas)

df_abasto['fecha'] = pd.to_datetime(df_abasto['fecha'])
df_entregas['fecha'] = pd.to_datetime(df_entregas['fecha'])

df = pd.merge(
    df_abasto[['fecha', 'total_recibido_2025_ton']],
    df_entregas[['fecha', 'total_ton_entregada']],
    on='fecha', how='outer'
).fillna(0).sort_values('fecha')

# üìÖ Agregar info de semana
df['dia_semana'] = df['fecha'].dt.dayofweek
df['semana'] = df['fecha'].dt.isocalendar().week

# üìå Etiquetas X
etiquetas_fecha = df[df['dia_semana'] == 4]['fecha'].tolist()
etiquetas_formato = [f"{DIAS[f.weekday()]}\n{f.day:02d}-{MESES[f.month]}" for f in etiquetas_fecha]

# üî∫ D√≠as para anotar
lunes_jueves = df[df['dia_semana'].isin([0, 1, 2, 3])]
idx_max = (lunes_jueves.assign(suma=lunes_jueves['total_recibido_2025_ton'] + lunes_jueves['total_ton_entregada'])
            .groupby('semana')['suma'].idxmax())
fechas_anotar = sorted(set(etiquetas_fecha + df.loc[idx_max, 'fecha'].tolist()))

# üé® Crear gr√°fico
fig, ax = plt.subplots(figsize=(12.6, 6.5))
ax.set_facecolor('white')
plt.gcf().patch.set_facecolor('none')

# √Åreas y l√≠neas
plt.fill_between(df['fecha'], df['total_recibido_2025_ton'], color='#1e5b4f', alpha=0.25)
plt.plot(df['fecha'], df['total_recibido_2025_ton'], color='#1e5b4f', linewidth=2.5, marker='o', label='Recibido (ton)')

plt.fill_between(df['fecha'], df['total_ton_entregada'], color='#a57f2c', alpha=0.25)
plt.plot(df['fecha'], df['total_ton_entregada'], color='#a57f2c', linewidth=2.5, marker='o', label='Entregado (ton)')

# L√≠neas viernes
for f in etiquetas_fecha:
    plt.axvline(x=f, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)

# Anotar valores clave
for _, fila in df[df['fecha'].isin(fechas_anotar)].iterrows():
    y = max(fila['total_recibido_2025_ton'], fila['total_ton_entregada'])
    if y <= 10000:
        plt.text(fila['fecha'], y + 300, f"{int(y):,}", fontsize=8, ha='center', va='bottom', rotation=90)

# KPIs
recibido = int(df['total_recibido_2025_ton'].sum())
entregado = int(df['total_ton_entregada'].sum())
inventario = recibido - entregado

plt.text(0.50, 1.13, f"{recibido:,}", transform=ax.transAxes, ha='left', fontsize=13, color='#1e5b4f', weight='bold')
plt.text(0.50, 1.08, "Recibido Total (ton)", transform=ax.transAxes, ha='left', fontsize=10.5)

plt.text(0.70, 1.13, f"{entregado:,}", transform=ax.transAxes, ha='left', fontsize=13, color='#a57f2c', weight='bold')
plt.text(0.70, 1.08, "Entregado Total (ton)", transform=ax.transAxes, ha='left', fontsize=10.5)

plt.text(0.90, 1.13, f"{inventario:,}", transform=ax.transAxes, ha='left', fontsize=13, color='#c60052', weight='bold')
plt.text(0.90, 1.08, "Inventarios (ton)", transform=ax.transAxes, ha='left', fontsize=10.5)

# Estilo y ejes
plt.xlabel("Fecha")
plt.ylabel("Toneladas")
plt.xlim(df['fecha'].min(), df['fecha'].max())
ax.get_yaxis().set_major_formatter(FuncFormatter(lambda x, _: f"{int(x):,}"))
ax.set_xticks(etiquetas_fecha)
ax.set_xticklabels(etiquetas_formato, fontsize=8, ha='center')

plt.legend(loc='upper left', bbox_to_anchor=(0, 1.13), fontsize=9)
plt.tight_layout()
plt.savefig(ruta_salida, dpi=300, bbox_inches='tight', transparent=True)
plt.close()

print(f"‚úÖ Gr√°fico generado: {ruta_salida}")



---
# grafico_entregas_diarias.py

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
from datetime import datetime

# Diccionarios
DIAS = {0: "lun", 1: "mar", 2: "mi√©", 3: "jue", 4: "vier", 5: "s√°b", 6: "dom"}
MESES = {1: "ene", 2: "feb", 3: "mar", 4: "abr", 5: "may", 6: "jun",
         7: "jul", 8: "ago", 9: "sep", 10: "oct", 11: "nov", 12: "dic"}

# Rutas
ruta_csv = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/entregas_diarias_2025.csv"
ruta_salida = "/Users/Arturo/AGRICULTURA/INFORMES/Img_Temporales/grafico_entregas_diarias.png"

# Leer datos
df = pd.read_csv(ruta_csv)
df['fecha'] = pd.to_datetime(df['fecha'])
df = df[df['fecha'].notnull()]
df = df[df['derechohabientes_apoyados'] > 0]

# Clasificaci√≥n por d√≠a
df['dia_semana'] = df['fecha'].dt.dayofweek
df['semana'] = df['fecha'].dt.isocalendar().week
df['a√±o'] = df['fecha'].dt.isocalendar().year

# ‚ûï Obtener viernes
df['es_viernes'] = df['dia_semana'] == 4
etiquetas_fecha = sorted(df[df['es_viernes']]['fecha'].tolist())

# ‚ûï Obtener mayor valor lunes a jueves por semana
df_lj = df[df['dia_semana'].isin([0, 1, 2, 3])]
max_por_semana = df_lj.loc[df_lj.groupby(['a√±o', 'semana'])['derechohabientes_apoyados'].idxmax()]

# üìå Fechas a etiquetar
fechas_valor = set(etiquetas_fecha) | set(max_por_semana['fecha'])
df_valores = df[df['fecha'].isin(fechas_valor)]

# Etiquetas X en dos l√≠neas
etiquetas_formato = [
    f"{DIAS[f.weekday()]}\n{f.day:02d}-{MESES[f.month]}" for f in etiquetas_fecha
]

# üéØ Indicadores generales
total_entregado = df['derechohabientes_apoyados'].sum()
maximo_entrega = df['derechohabientes_apoyados'].max()
promedio_entrega = df['derechohabientes_apoyados'].mean()
meta_total = 2062239
porcentaje_cumplido = (total_entregado / meta_total) * 100

# üé® Crear gr√°fico
fig, ax = plt.subplots(figsize=(12.6, 6.3))  # ligeramente m√°s alto para panel
plt.style.use('default')
ax.set_facecolor('white')
fig.patch.set_facecolor('none')

# L√≠nea principal
ax.plot(df['fecha'], df['derechohabientes_apoyados'],
        color='#9b2247', linewidth=2.5, marker='o', markersize=5)

# Mostrar valores seleccionados
for _, fila in df_valores.iterrows():
    ax.text(
        fila['fecha'],
        fila['derechohabientes_apoyados'] + 500,
        f"{int(fila['derechohabientes_apoyados']):,}".replace(",", ","),
        color='black',
        fontsize=8.5,
        ha='center',
        va='bottom',
        rotation=90
    )

# L√≠neas verticales para los viernes
for fecha in etiquetas_fecha:
    ax.axvline(x=fecha, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)

# Ejes y estilo
ax.set_xlabel('Fecha de Entrega', fontsize=12, color='black')
ax.set_ylabel('Derechohabientes Apoyados', fontsize=12, color='black')
ax.set_xlim([df['fecha'].min(), df['fecha'].max()])
ax.get_yaxis().set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'.replace(",", ",")))
ax.tick_params(axis='y', colors='black')
ax.spines['top'].set_visible(False)
ax.yaxis.grid(True, linestyle='--', linewidth=0.5, alpha=0.5)

# Etiquetas X solo viernes
ax.set_xticks(etiquetas_fecha)
ax.set_xticklabels(etiquetas_formato, fontsize=8, ha='center', color='black')

# üî∑ Panel superior con KPIs (colores y estilos ajustados)
color_dato = '#9b2247'
color_etiqueta = '#444444'

fig.text(0.13, 0.95,
         f"{int(total_entregado):,}".replace(",", ",") + f" ({porcentaje_cumplido:.2f}%)",
         fontsize=14, fontweight='bold', color=color_dato, ha='center')
fig.text(0.13, 0.91, "Total Atendidos", fontsize=10, fontweight='bold', color=color_etiqueta, ha='center')

fig.text(0.5, 0.95,
         f"{int(maximo_entrega):,}".replace(",", ","),
         fontsize=14, fontweight='bold', color=color_dato, ha='center')
fig.text(0.5, 0.91, "M√°ximo de Entrega", fontsize=10, fontweight='bold', color=color_etiqueta, ha='center')

fig.text(0.87, 0.95,
         f"{int(promedio_entrega):,}".replace(",", ","),
         fontsize=14, fontweight='bold', color=color_dato, ha='center')
fig.text(0.87, 0.91, "Promedio Entrega / D√≠a", fontsize=10, fontweight='bold', color=color_etiqueta, ha='center')

# Guardar gr√°fico
plt.tight_layout(rect=[0, 0, 1, 0.89])  # deja espacio arriba para KPIs
plt.savefig(ruta_salida, dpi=300, bbox_inches='tight', transparent=True)
plt.close()

print(f"‚úÖ Gr√°fico de entregas diarias exportado en: {ruta_salida}")



---
# grafico_entregas_semanales_2025.py

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter

# Rutas
ruta_csv = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/entregas_semanales_2025.csv"
ruta_salida = "/Users/Arturo/AGRICULTURA/INFORMES/Img_Temporales/grafico_entregas_semanales.png"

# Leer datos
df = pd.read_csv(ruta_csv)
df = df[df['derechohabientes_apoyados'] > 0]

# Etiquetas del eje X: "abr - S14"
df['etiqueta_x'] = df['nombre_mes'].str[:3] + ' - S - ' + df['semana'].astype(str)

# üéØ KPIs generales
total_entregado = df['derechohabientes_apoyados'].sum()
maximo_entrega = df['derechohabientes_apoyados'].max()
promedio_entrega = df['derechohabientes_apoyados'].mean()
meta_total = 2062239
porcentaje_cumplido = (total_entregado / meta_total) * 100

# üé® Crear gr√°fico
fig, ax = plt.subplots(figsize=(12.6, 6.3))
plt.style.use('default')
ax.set_facecolor('white')
fig.patch.set_facecolor('none')

# L√≠nea principal
ax.plot(df['etiqueta_x'], df['derechohabientes_apoyados'],
        color='#9b2247', linewidth=2.5, marker='o', markersize=5)

# Valores destacados
for idx, row in df.iterrows():
    ax.text(
        row['etiqueta_x'],
        row['derechohabientes_apoyados'] + 1500,
        f"{int(row['derechohabientes_apoyados']):,}",
        color='black',
        fontsize=8.5,
        ha='center',
        va='bottom',
        rotation=0
    )

# Ejes y estilo
ax.set_xlabel('Semana', fontsize=12, color='black')
ax.set_ylabel('Derechohabientes Apoyados', fontsize=12, color='black')
ax.get_yaxis().set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))
ax.tick_params(axis='y', colors='black')
ax.spines['top'].set_visible(False)
ax.yaxis.grid(True, linestyle='--', linewidth=0.5, alpha=0.5)
ax.set_xticks(range(len(df)))
ax.set_xticklabels(df['etiqueta_x'], fontsize=8, ha='center', color='black', rotation=45)

# üî∑ Panel superior con KPIs
color_dato = '#9b2247'
color_etiqueta = '#444444'

fig.text(0.13, 0.95,
         f"{int(total_entregado):,} ({porcentaje_cumplido:.2f}%)",
         fontsize=14, fontweight='bold', color=color_dato, ha='center')
fig.text(0.13, 0.91, "Total Atendidos", fontsize=10, fontweight='bold', color=color_etiqueta, ha='center')

fig.text(0.5, 0.95,
         f"{int(maximo_entrega):,}",
         fontsize=14, fontweight='bold', color=color_dato, ha='center')
fig.text(0.5, 0.91, "M√°ximo de Entrega", fontsize=10, fontweight='bold', color=color_etiqueta, ha='center')

fig.text(0.87, 0.95,
         f"{int(promedio_entrega):,}",
         fontsize=14, fontweight='bold', color=color_dato, ha='center')
fig.text(0.87, 0.91, "Promedio Entrega / Semana", fontsize=10, fontweight='bold', color=color_etiqueta, ha='center')

# Guardar gr√°fico
plt.tight_layout(rect=[0, 0, 1, 0.89])
plt.savefig(ruta_salida, dpi=300, bbox_inches='tight', transparent=True)
plt.close()

print(f"‚úÖ Gr√°fico de entregas semanales exportado en: {ruta_salida}")



---
# grafico_enviado_vs_entregado.py

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
from datetime import datetime

# Diccionarios espa√±ol
DIAS = {0: "lun", 1: "mar", 2: "mi√©", 3: "jue", 4: "vier", 5: "s√°b", 6: "dom"}
MESES = {
    1: "ene", 2: "feb", 3: "mar", 4: "abr", 5: "may", 6: "jun",
    7: "jul", 8: "ago", 9: "sep", 10: "oct", 11: "nov", 12: "dic"
}

# Rutas
ruta_envios = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/envios_diarios_2025.csv"
ruta_entregas = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/entregas_diarias_2025.csv"
ruta_salida = "/Users/Arturo/AGRICULTURA/INFORMES/Img_Temporales/grafico_comparativo_abasto_entregas.png"

# Cargar datos
df_envios = pd.read_csv(ruta_envios)
df_entregas = pd.read_csv(ruta_entregas)

# Fechas y limpieza
df_envios['fecha'] = pd.to_datetime(df_envios['fecha'])
df_entregas['fecha'] = pd.to_datetime(df_entregas['fecha'])
df_envios = df_envios[df_envios['total_ton_enviadas'] > 0]
df_entregas = df_entregas[df_entregas['total_ton_entregada'] > 0]

# Unir en base a fechas
df = pd.merge(
    df_envios[['fecha', 'total_ton_enviadas']],
    df_entregas[['fecha', 'total_ton_entregada']],
    on='fecha',
    how='outer'
)
df = df.sort_values('fecha').fillna(0)

# Pen√∫ltima fecha v√°lida
fecha_previa = df['fecha'].sort_values().iloc[-2]

# Fechas clave (viernes + fecha previa)
df['es_viernes'] = df['fecha'].dt.dayofweek == 4
etiquetas_fecha = df[df['es_viernes']]['fecha'].tolist()
if fecha_previa not in etiquetas_fecha:
    etiquetas_fecha.append(fecha_previa)

# Picos altos
umbral_envios = df['total_ton_enviadas'].quantile(0.75)
umbral_entregas = df['total_ton_entregada'].quantile(0.75)
picos = df[
    (df['fecha'].isin(etiquetas_fecha)) |
    (df['total_ton_enviadas'] >= umbral_envios) |
    (df['total_ton_entregada'] >= umbral_entregas)
]

# Etiquetas eje X
etiquetas_formato = [
    f"{DIAS[f.weekday()]}\n{f.day:02d}-{MESES[f.month]}" for f in etiquetas_fecha
]

# Crear gr√°fico
plt.figure(figsize=(12.6, 5.9))
plt.style.use('default')
ax = plt.gca()
ax.set_facecolor('white')
plt.gcf().patch.set_facecolor('none')

# √Årea + l√≠nea de env√≠os
plt.fill_between(df['fecha'], df['total_ton_enviadas'], color='#1e5b4f', alpha=0.25)
plt.plot(df['fecha'], df['total_ton_enviadas'], color='#1e5b4f', linewidth=2.5, marker='o', markersize=4, label='Fertilizante Recibido (ton)')

# √Årea + l√≠nea de entregas
plt.fill_between(df['fecha'], df['total_ton_entregada'], color='#a57f2c', alpha=0.25)
plt.plot(df['fecha'], df['total_ton_entregada'], color='#a57f2c', linewidth=2.5, marker='o', markersize=4, label='Fertilizante Entregado (ton)')

# L√≠neas verticales
for fecha in etiquetas_fecha:
    plt.axvline(x=fecha, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)

# Anotar valores
for _, fila in picos.iterrows():
    y_max = max(fila['total_ton_enviadas'], fila['total_ton_entregada'])
    plt.text(
        fila['fecha'],
        y_max + 300,
        f"{int(y_max):,}".replace(",", ","),
        color='black',
        fontsize=8.5,
        ha='center',
        va='bottom',
        rotation=90
    )

# Ejes
plt.xlabel("Fecha", fontsize=12, color='black')
plt.ylabel("Fertilizante Recibido (ton) y Fertilizante Entregado (ton)", fontsize=12, color='black')
plt.xlim([df['fecha'].min(), df['fecha'].max()])
ax.get_yaxis().set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'.replace(",", ",")))
plt.yticks(color='black')
ax.spines['top'].set_visible(False)
ax.yaxis.grid(True, linestyle='--', linewidth=0.5, alpha=0.5)

# Eje X
ax.set_xticks(etiquetas_fecha)
ax.set_xticklabels(etiquetas_formato, fontsize=8, ha='center', color='black')

# Leyenda
plt.legend(
    loc='upper left',
    bbox_to_anchor=(0, 1.25),
    facecolor='none',
    edgecolor='none',
    labelcolor='black',
    fontsize=9
)

# Guardar
plt.tight_layout()
plt.savefig(ruta_salida, dpi=300, bbox_inches='tight', transparent=True)
plt.close()

print(f"‚úÖ Gr√°fico comparativo abasto vs entregas exportado en: {ruta_salida}")



---
# grafico_envios_diarios.py

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
from datetime import datetime

# Diccionarios para etiquetas
DIAS = {0: "lun", 1: "mar", 2: "mi√©", 3: "jue", 4: "vier", 5: "s√°b", 6: "dom"}
MESES = {
    1: "ene", 2: "feb", 3: "mar", 4: "abr", 5: "may", 6: "jun",
    7: "jul", 8: "ago", 9: "sep", 10: "oct", 11: "nov", 12: "dic"
}

# Rutas
ruta_csv = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/envios_diarios_2025.csv"
ruta_salida = "/Users/Arturo/AGRICULTURA/INFORMES/Img_Temporales/grafico_envios_diarios_powerbi.png"

# Leer datos
df = pd.read_csv(ruta_csv)
df['fecha'] = pd.to_datetime(df['fecha'])
df = df[df['fecha'].notnull()]
df = df[df['total_ton_enviadas'] > 0]

# Fecha previa
fecha_previa = df['fecha'].sort_values().iloc[-2]

# Etiquetas de viernes + pen√∫ltima
df['es_viernes'] = df['fecha'].dt.dayofweek == 4
etiquetas_fecha = df[df['es_viernes']]['fecha'].tolist()
if fecha_previa not in etiquetas_fecha:
    etiquetas_fecha.append(fecha_previa)

# Etiquetas formateadas
etiquetas_formato = [
    f"{DIAS[f.weekday()]}\n{f.day:02d}-{MESES[f.month]}" for f in etiquetas_fecha
]

# Fechas para mostrar valores (picos y viernes)
umbral = df['total_ton_enviadas'].quantile(0.75)
fechas_valor = sorted(set(etiquetas_fecha + df[df['total_ton_enviadas'] >= umbral]['fecha'].tolist()))
df_valores = df[df['fecha'].isin(fechas_valor)]

# Crear gr√°fico
plt.figure(figsize=(12.6, 5.9))
plt.style.use('default')
ax = plt.gca()
ax.set_facecolor('white')
plt.gcf().patch.set_facecolor('none')

# √Åreas
plt.fill_between(df['fecha'], df['total_ton_enviadas'], color='#a57f2c', alpha=0.35)
plt.fill_between(df['fecha'], df['urea_ton_enviadas'], color='#6bbf59', alpha=0.35)
plt.fill_between(df['fecha'], df['dap_ton_enviadas'], color='#204e4a', alpha=0.35)

# L√≠neas con puntos
plt.plot(df['fecha'], df['total_ton_enviadas'], color='#a57f2c', linewidth=2.2, marker='o', markersize=4, label='Total Enviado (ton)')
plt.plot(df['fecha'], df['urea_ton_enviadas'], color='#6bbf59', linewidth=1.8, linestyle='--', marker='o', markersize=3, label='UREA Enviada (ton)')
plt.plot(df['fecha'], df['dap_ton_enviadas'], color='#204e4a', linewidth=1.8, linestyle='--', marker='o', markersize=3, label='DAP Enviado (ton)')

# L√≠neas verticales para viernes
for fecha in etiquetas_fecha:
    plt.axvline(x=fecha, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)

# Anotar valores
for _, fila in df_valores.iterrows():
    plt.text(
        fila['fecha'],
        fila['total_ton_enviadas'] + 300,
        f"{int(fila['total_ton_enviadas']):,}".replace(",", ","),
        color='black',
        fontsize=8.3,
        ha='center',
        va='bottom',
        rotation=90
    )

# Ejes
plt.xlabel("Fecha", fontsize=12, color='black')
plt.ylabel("Toneladas Enviadas", fontsize=12, color='black')
plt.xlim([df['fecha'].min(), df['fecha'].max()])
ax.get_yaxis().set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'.replace(",", ",")))
plt.yticks(color='black')
ax.spines['top'].set_visible(False)
ax.yaxis.grid(True, linestyle='--', linewidth=0.5, alpha=0.4)

# Etiquetas eje X
ax.set_xticks(etiquetas_fecha)
ax.set_xticklabels(etiquetas_formato, fontsize=8, ha='center', color='black')

# Leyenda
plt.legend(
    loc='upper left',
    bbox_to_anchor=(0, 1.25),
    facecolor='none',
    edgecolor='none',
    labelcolor='black',
    fontsize=9
)

# Guardar
plt.tight_layout()
plt.savefig(ruta_salida, dpi=300, bbox_inches='tight', transparent=True)
plt.close()

print(f"‚úÖ Gr√°fico estilo Power BI generado en: {ruta_salida}")



---
# grafico_envios_diarios_chiapas.py

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
from datetime import datetime

# üìå Estado a graficar
estado = "CHIAPAS"
nombre_archivo = estado.lower().replace("√°", "a").replace("√©", "e").replace("√≠", "i") \
                               .replace("√≥", "o").replace("√∫", "u").replace("√±", "n")

# üìÇ Rutas de entrada y salida
ruta_csv = f"/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/envios_{nombre_archivo}.csv"
ruta_salida = f"/Users/Arturo/AGRICULTURA/INFORMES/Img_Temporales/grafico_envios_{nombre_archivo}.png"

# üß† Diccionarios etiquetas
DIAS = {0: "lun", 1: "mar", 2: "mi√©", 3: "jue", 4: "vier", 5: "s√°b", 6: "dom"}
MESES = {1: "ene", 2: "feb", 3: "mar", 4: "abr", 5: "may", 6: "jun",
         7: "jul", 8: "ago", 9: "sep", 10: "oct", 11: "nov", 12: "dic"}

# üìä Leer datos
df = pd.read_csv(ruta_csv)
df['fecha'] = pd.to_datetime(df['fecha'])
df = df[df['fecha'].notnull()]
df = df[df['total_ton_enviadas'] > 0]

# üïí Fecha previa (pen√∫ltimo d√≠a)
if len(df) >= 2:
    fecha_previa = df['fecha'].sort_values().iloc[-2]
else:
    fecha_previa = df['fecha'].max()

# üìÜ Etiquetas viernes y picos
df['es_viernes'] = df['fecha'].dt.dayofweek == 4
etiquetas_fecha = df[df['es_viernes']]['fecha'].tolist()
if fecha_previa not in etiquetas_fecha:
    etiquetas_fecha.append(fecha_previa)

etiquetas_formato = [
    f"{DIAS[f.weekday()]}\n{f.day:02d}-{MESES[f.month]}" for f in etiquetas_fecha
]

umbral = df['total_ton_enviadas'].quantile(0.75)
fechas_valor = sorted(set(etiquetas_fecha + df[df['total_ton_enviadas'] >= umbral]['fecha'].tolist()))
df_valores = df[df['fecha'].isin(fechas_valor)]

# üé® Crear gr√°fico
plt.figure(figsize=(12.6, 5.9))
plt.style.use('default')
ax = plt.gca()
ax.set_facecolor('white')
plt.gcf().patch.set_facecolor('none')

# √Åreas sombreadas
plt.fill_between(df['fecha'], df['total_ton_enviadas'], color='#a57f2c', alpha=0.35)
plt.fill_between(df['fecha'], df['urea_ton_enviadas'], color='#6bbf59', alpha=0.35)
plt.fill_between(df['fecha'], df['dap_ton_enviadas'], color='#204e4a', alpha=0.35)

# L√≠neas con puntos
plt.plot(df['fecha'], df['total_ton_enviadas'], color='#a57f2c', linewidth=2.2, marker='o', markersize=4, label='Total Enviado (ton)')
plt.plot(df['fecha'], df['urea_ton_enviadas'], color='#6bbf59', linewidth=1.8, linestyle='--', marker='o', markersize=3, label='UREA Enviada (ton)')
plt.plot(df['fecha'], df['dap_ton_enviadas'], color='#204e4a', linewidth=1.8, linestyle='--', marker='o', markersize=3, label='DAP Enviado (ton)')

# L√≠neas viernes
for fecha in etiquetas_fecha:
    plt.axvline(x=fecha, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)

# Anotar valores
for _, fila in df_valores.iterrows():
    plt.text(
        fila['fecha'],
        fila['total_ton_enviadas'] + 300,
        f"{int(fila['total_ton_enviadas']):,}",
        color='black',
        fontsize=8.3,
        ha='center',
        va='bottom',
        rotation=90
    )

# Ejes y estilo
plt.xlabel("Fecha", fontsize=12, color='black')
plt.ylabel("Toneladas Enviadas", fontsize=12, color='black')
plt.xlim([df['fecha'].min(), df['fecha'].max()])
ax.get_yaxis().set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))
plt.yticks(color='black')
ax.spines['top'].set_visible(False)
ax.yaxis.grid(True, linestyle='--', linewidth=0.5, alpha=0.4)

# Eje X
ax.set_xticks(etiquetas_fecha)
ax.set_xticklabels(etiquetas_formato, fontsize=8, ha='center', color='black')

# Leyenda
plt.legend(
    loc='upper left',
    bbox_to_anchor=(0, 1.25),
    facecolor='none',
    edgecolor='none',
    labelcolor='black',
    fontsize=9
)

# Guardar gr√°fico
plt.tight_layout()
plt.savefig(ruta_salida, dpi=300, bbox_inches='tight', transparent=True)
plt.close()

print(f"‚úÖ Gr√°fico generado: {ruta_salida}")



---
# grafico_envios_diarios_new.py

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
from datetime import datetime

# Diccionarios para etiquetas
DIAS = {0: "lun", 1: "mar", 2: "mi√©", 3: "jue", 4: "vier", 5: "s√°b", 6: "dom"}
MESES = {
    1: "ene", 2: "feb", 3: "mar", 4: "abr", 5: "may", 6: "jun",
    7: "jul", 8: "ago", 9: "sep", 10: "oct", 11: "nov", 12: "dic"
}

# Rutas
ruta_csv = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/envios_diarios_2025.csv"
ruta_salida = "/Users/Arturo/AGRICULTURA/INFORMES/Img_Temporales/grafico_envios_diarios_powerbi.png"

# Leer datos
df = pd.read_csv(ruta_csv)
df['fecha'] = pd.to_datetime(df['fecha'])
df = df[df['fecha'].notnull()]
df = df[df['total_ton_enviadas'] > 0]

# Fecha previa
fecha_previa = df['fecha'].sort_values().iloc[-2]

# Etiquetas de viernes + pen√∫ltima
df['es_viernes'] = df['fecha'].dt.dayofweek == 4
etiquetas_fecha = df[df['es_viernes']]['fecha'].tolist()

# Quitar el √∫ltimo viernes si no es la √∫ltima fecha del dataset
ultimo_viernes = max(etiquetas_fecha) if etiquetas_fecha else None
ultima_fecha = df['fecha'].max()
if ultimo_viernes and ultimo_viernes != ultima_fecha:
    etiquetas_fecha = [f for f in etiquetas_fecha if f != ultimo_viernes]

# Asegurar que est√© la pen√∫ltima fecha (por si no es viernes)
if fecha_previa not in etiquetas_fecha:
    etiquetas_fecha.append(fecha_previa)

# Etiquetas formateadas
etiquetas_formato = [
    f"{DIAS[f.weekday()]}\n{f.day:02d}-{MESES[f.month]}" for f in etiquetas_fecha
]

# Fechas para mostrar valores (picos y viernes)
umbral = df['total_ton_enviadas'].quantile(0.75)
fechas_valor = sorted(set(etiquetas_fecha + df[df['total_ton_enviadas'] >= umbral]['fecha'].tolist()))
df_valores = df[df['fecha'].isin(fechas_valor)]

# Crear gr√°fico
plt.figure(figsize=(12.6, 5.9))
plt.style.use('default')
ax = plt.gca()
ax.set_facecolor('white')
plt.gcf().patch.set_facecolor('none')

# √Åreas
plt.fill_between(df['fecha'], df['total_ton_enviadas'], color='#a57f2c', alpha=0.35)
plt.fill_between(df['fecha'], df['urea_ton_enviadas'], color='#6bbf59', alpha=0.35)
plt.fill_between(df['fecha'], df['dap_ton_enviadas'], color='#204e4a', alpha=0.35)

# Calcular KPIs
total_dap = df['dap_ton_enviadas'].sum()
total_urea = df['urea_ton_enviadas'].sum()
total_envio = df['total_ton_enviadas'].sum()

# L√≠neas con puntos
plt.plot(df['fecha'], df['total_ton_enviadas'], color='#a57f2c', linewidth=2.2, marker='o', markersize=4)
plt.plot(df['fecha'], df['urea_ton_enviadas'], color='#6bbf59', linewidth=1.8, linestyle='--', marker='o', markersize=3)
plt.plot(df['fecha'], df['dap_ton_enviadas'], color='#204e4a', linewidth=1.8, linestyle='--', marker='o', markersize=3)

# L√≠neas invisibles para leyenda personalizada
linea_total, = plt.plot([], [], color='#a57f2c', linewidth=2.2, marker='o', markersize=4)
linea_urea, = plt.plot([], [], color='#6bbf59', linewidth=1.8, linestyle='--', marker='o', markersize=3)
linea_dap, = plt.plot([], [], color='#204e4a', linewidth=1.8, linestyle='--', marker='o', markersize=3)

# Texto de leyenda con separadores y negritas solo en Total Enviado
label_total = r'Total Enviado (ton): $\bf{' + f'{total_envio:,.0f}' + r'}$'
label_urea = f'UREA Enviada (ton): {total_urea:,.0f}'
label_dap = f'DAP Enviada (ton): {total_dap:,.0f}'

# Insertar literal el separador "|" entre columnas, usando padding
ax.legend(
    handles=[linea_total, linea_urea, linea_dap],
    labels=[
        label_total,
        '|  ' + label_urea,
        '|  ' + label_dap
    ],
    loc='upper center',
    bbox_to_anchor=(0.5, 1.22),
    ncol=3,
    frameon=False,
    fontsize=10,
    labelcolor='black',
    columnspacing=4.5,
    handletextpad=2.5,
    borderpad=0.8
)


# L√≠neas verticales para viernes
for fecha in etiquetas_fecha:
    plt.axvline(x=fecha, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)

# Anotar valores
for _, fila in df_valores.iterrows():
    plt.text(
        fila['fecha'],
        fila['total_ton_enviadas'] + 300,
        f"{int(fila['total_ton_enviadas']):,}".replace(",", ","),
        color='black',
        fontsize=8.3,
        ha='center',
        va='bottom',
        rotation=90
    )

# Ejes
plt.xlabel("Fecha", fontsize=12, color='black')
plt.ylabel("Toneladas Enviadas", fontsize=12, color='black')
plt.xlim([df['fecha'].min(), df['fecha'].max()])
ax.get_yaxis().set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'.replace(",", ",")))
plt.yticks(color='black')
ax.spines['top'].set_visible(False)
ax.yaxis.grid(True, linestyle='--', linewidth=0.5, alpha=0.4)

# Etiquetas eje X
ax.set_xticks(etiquetas_fecha)
ax.set_xticklabels(etiquetas_formato, fontsize=8, ha='center', color='black')

# Guardar
plt.tight_layout()
plt.savefig(ruta_salida, dpi=300, bbox_inches='tight', transparent=True)
plt.close()

print(f"‚úÖ Gr√°fico estilo Power BI generado en: {ruta_salida}")



---
# grafico_envios_diarios_sinaloa.py

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
from datetime import datetime

# üìå Estado a graficar
estado = "SINALOA"
nombre_archivo = estado.lower().replace("√°", "a").replace("√©", "e").replace("√≠", "i") \
                               .replace("√≥", "o").replace("√∫", "u").replace("√±", "n")

# üìÇ Rutas de entrada y salida
ruta_csv = f"/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/envios_{nombre_archivo}.csv"
ruta_salida = f"/Users/Arturo/AGRICULTURA/INFORMES/Img_Temporales/grafico_envios_{nombre_archivo}.png"

# üß† Diccionarios etiquetas
DIAS = {0: "lun", 1: "mar", 2: "mi√©", 3: "jue", 4: "vier", 5: "s√°b", 6: "dom"}
MESES = {1: "ene", 2: "feb", 3: "mar", 4: "abr", 5: "may", 6: "jun",
         7: "jul", 8: "ago", 9: "sep", 10: "oct", 11: "nov", 12: "dic"}

# üìä Leer datos
df = pd.read_csv(ruta_csv)
df['fecha'] = pd.to_datetime(df['fecha'])
df = df[df['fecha'].notnull()]
df = df[df['total_ton_enviadas'] > 0]

# üïí Fecha previa (pen√∫ltimo d√≠a)
if len(df) >= 2:
    fecha_previa = df['fecha'].sort_values().iloc[-2]
else:
    fecha_previa = df['fecha'].max()

# üìÜ Etiquetas viernes y picos
df['es_viernes'] = df['fecha'].dt.dayofweek == 4
etiquetas_fecha = df[df['es_viernes']]['fecha'].tolist()
if fecha_previa not in etiquetas_fecha:
    etiquetas_fecha.append(fecha_previa)

etiquetas_formato = [
    f"{DIAS[f.weekday()]}\n{f.day:02d}-{MESES[f.month]}" for f in etiquetas_fecha
]

umbral = df['total_ton_enviadas'].quantile(0.75)
fechas_valor = sorted(set(etiquetas_fecha + df[df['total_ton_enviadas'] >= umbral]['fecha'].tolist()))
df_valores = df[df['fecha'].isin(fechas_valor)]

# üé® Crear gr√°fico
plt.figure(figsize=(12.6, 5.9))
plt.style.use('default')
ax = plt.gca()
ax.set_facecolor('white')
plt.gcf().patch.set_facecolor('none')

# √Åreas sombreadas
plt.fill_between(df['fecha'], df['total_ton_enviadas'], color='#a57f2c', alpha=0.35)
plt.fill_between(df['fecha'], df['urea_ton_enviadas'], color='#6bbf59', alpha=0.35)
plt.fill_between(df['fecha'], df['dap_ton_enviadas'], color='#204e4a', alpha=0.35)

# L√≠neas con puntos
plt.plot(df['fecha'], df['total_ton_enviadas'], color='#a57f2c', linewidth=2.2, marker='o', markersize=4, label='Total Enviado (ton)')
plt.plot(df['fecha'], df['urea_ton_enviadas'], color='#6bbf59', linewidth=1.8, linestyle='--', marker='o', markersize=3, label='UREA Enviada (ton)')
plt.plot(df['fecha'], df['dap_ton_enviadas'], color='#204e4a', linewidth=1.8, linestyle='--', marker='o', markersize=3, label='DAP Enviado (ton)')

# L√≠neas viernes
for fecha in etiquetas_fecha:
    plt.axvline(x=fecha, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)

# Anotar valores
for _, fila in df_valores.iterrows():
    plt.text(
        fila['fecha'],
        fila['total_ton_enviadas'] + 300,
        f"{int(fila['total_ton_enviadas']):,}",
        color='black',
        fontsize=8.3,
        ha='center',
        va='bottom',
        rotation=90
    )

# Ejes y estilo
plt.xlabel("Fecha", fontsize=12, color='black')
plt.ylabel("Toneladas Enviadas", fontsize=12, color='black')
plt.xlim([df['fecha'].min(), df['fecha'].max()])
ax.get_yaxis().set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))
plt.yticks(color='black')
ax.spines['top'].set_visible(False)
ax.yaxis.grid(True, linestyle='--', linewidth=0.5, alpha=0.4)

# Eje X
ax.set_xticks(etiquetas_fecha)
ax.set_xticklabels(etiquetas_formato, fontsize=8, ha='center', color='black')

# Leyenda
plt.legend(
    loc='upper left',
    bbox_to_anchor=(0, 1.25),
    facecolor='none',
    edgecolor='none',
    labelcolor='black',
    fontsize=9
)

# Guardar gr√°fico
plt.tight_layout()
plt.savefig(ruta_salida, dpi=300, bbox_inches='tight', transparent=True)
plt.close()

print(f"‚úÖ Gr√°fico generado: {ruta_salida}")



---
# grafico_envios_diarios_tabasco.py

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
from datetime import datetime

# üìå Estado a graficar
estado = "TABASCO"
nombre_archivo = estado.lower().replace("√°", "a").replace("√©", "e").replace("√≠", "i") \
                               .replace("√≥", "o").replace("√∫", "u").replace("√±", "n")

# üìÇ Rutas de entrada y salida
ruta_csv = f"/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/envios_{nombre_archivo}.csv"
ruta_salida = f"/Users/Arturo/AGRICULTURA/INFORMES/Img_Temporales/grafico_envios_{nombre_archivo}.png"

# üß† Diccionarios etiquetas
DIAS = {0: "lun", 1: "mar", 2: "mi√©", 3: "jue", 4: "vier", 5: "s√°b", 6: "dom"}
MESES = {1: "ene", 2: "feb", 3: "mar", 4: "abr", 5: "may", 6: "jun",
         7: "jul", 8: "ago", 9: "sep", 10: "oct", 11: "nov", 12: "dic"}

# üìä Leer datos
df = pd.read_csv(ruta_csv)
df['fecha'] = pd.to_datetime(df['fecha'])
df = df[df['fecha'].notnull()]
df = df[df['total_ton_enviadas'] > 0]

# üïí Fecha previa (pen√∫ltimo d√≠a)
if len(df) >= 2:
    fecha_previa = df['fecha'].sort_values().iloc[-2]
else:
    fecha_previa = df['fecha'].max()

# üìÜ Etiquetas viernes y picos
df['es_viernes'] = df['fecha'].dt.dayofweek == 4
etiquetas_fecha = df[df['es_viernes']]['fecha'].tolist()
if fecha_previa not in etiquetas_fecha:
    etiquetas_fecha.append(fecha_previa)

etiquetas_formato = [
    f"{DIAS[f.weekday()]}\n{f.day:02d}-{MESES[f.month]}" for f in etiquetas_fecha
]

umbral = df['total_ton_enviadas'].quantile(0.75)
fechas_valor = sorted(set(etiquetas_fecha + df[df['total_ton_enviadas'] >= umbral]['fecha'].tolist()))
df_valores = df[df['fecha'].isin(fechas_valor)]

# üé® Crear gr√°fico
plt.figure(figsize=(12.6, 5.9))
plt.style.use('default')
ax = plt.gca()
ax.set_facecolor('white')
plt.gcf().patch.set_facecolor('none')

# √Åreas sombreadas
plt.fill_between(df['fecha'], df['total_ton_enviadas'], color='#a57f2c', alpha=0.35)
plt.fill_between(df['fecha'], df['urea_ton_enviadas'], color='#6bbf59', alpha=0.35)
plt.fill_between(df['fecha'], df['dap_ton_enviadas'], color='#204e4a', alpha=0.35)

# L√≠neas con puntos
plt.plot(df['fecha'], df['total_ton_enviadas'], color='#a57f2c', linewidth=2.2, marker='o', markersize=4, label='Total Enviado (ton)')
plt.plot(df['fecha'], df['urea_ton_enviadas'], color='#6bbf59', linewidth=1.8, linestyle='--', marker='o', markersize=3, label='UREA Enviada (ton)')
plt.plot(df['fecha'], df['dap_ton_enviadas'], color='#204e4a', linewidth=1.8, linestyle='--', marker='o', markersize=3, label='DAP Enviado (ton)')

# L√≠neas viernes
for fecha in etiquetas_fecha:
    plt.axvline(x=fecha, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)

# Anotar valores
for _, fila in df_valores.iterrows():
    plt.text(
        fila['fecha'],
        fila['total_ton_enviadas'] + 300,
        f"{int(fila['total_ton_enviadas']):,}",
        color='black',
        fontsize=8.3,
        ha='center',
        va='bottom',
        rotation=90
    )

# Ejes y estilo
plt.xlabel("Fecha", fontsize=12, color='black')
plt.ylabel("Toneladas Enviadas", fontsize=12, color='black')
plt.xlim([df['fecha'].min(), df['fecha'].max()])
ax.get_yaxis().set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))
plt.yticks(color='black')
ax.spines['top'].set_visible(False)
ax.yaxis.grid(True, linestyle='--', linewidth=0.5, alpha=0.4)

# Eje X
ax.set_xticks(etiquetas_fecha)
ax.set_xticklabels(etiquetas_formato, fontsize=8, ha='center', color='black')

# Leyenda
plt.legend(
    loc='upper left',
    bbox_to_anchor=(0, 1.25),
    facecolor='none',
    edgecolor='none',
    labelcolor='black',
    fontsize=9
)

# Guardar gr√°fico
plt.tight_layout()
plt.savefig(ruta_salida, dpi=300, bbox_inches='tight', transparent=True)
plt.close()

print(f"‚úÖ Gr√°fico generado: {ruta_salida}")



---
# grafico_envios_diarios_veracruz.py

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
from datetime import datetime

# üìå Estado a graficar
estado = "VERACRUZ"
nombre_archivo = estado.lower().replace("√°", "a").replace("√©", "e").replace("√≠", "i") \
                               .replace("√≥", "o").replace("√∫", "u").replace("√±", "n")

# üìÇ Rutas de entrada y salida
ruta_csv = f"/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/envios_{nombre_archivo}.csv"
ruta_salida = f"/Users/Arturo/AGRICULTURA/INFORMES/Img_Temporales/grafico_envios_{nombre_archivo}.png"

# üß† Diccionarios etiquetas
DIAS = {0: "lun", 1: "mar", 2: "mi√©", 3: "jue", 4: "vier", 5: "s√°b", 6: "dom"}
MESES = {1: "ene", 2: "feb", 3: "mar", 4: "abr", 5: "may", 6: "jun",
         7: "jul", 8: "ago", 9: "sep", 10: "oct", 11: "nov", 12: "dic"}

# üìä Leer datos
df = pd.read_csv(ruta_csv)
df['fecha'] = pd.to_datetime(df['fecha'])
df = df[df['fecha'].notnull()]
df = df[df['total_ton_enviadas'] > 0]

# üïí Fecha previa (pen√∫ltimo d√≠a)
if len(df) >= 2:
    fecha_previa = df['fecha'].sort_values().iloc[-2]
else:
    fecha_previa = df['fecha'].max()

# üìÜ Etiquetas viernes y picos
df['es_viernes'] = df['fecha'].dt.dayofweek == 4
etiquetas_fecha = df[df['es_viernes']]['fecha'].tolist()
if fecha_previa not in etiquetas_fecha:
    etiquetas_fecha.append(fecha_previa)

etiquetas_formato = [
    f"{DIAS[f.weekday()]}\n{f.day:02d}-{MESES[f.month]}" for f in etiquetas_fecha
]

umbral = df['total_ton_enviadas'].quantile(0.75)
fechas_valor = sorted(set(etiquetas_fecha + df[df['total_ton_enviadas'] >= umbral]['fecha'].tolist()))
df_valores = df[df['fecha'].isin(fechas_valor)]

# üé® Crear gr√°fico
plt.figure(figsize=(12.6, 5.9))
plt.style.use('default')
ax = plt.gca()
ax.set_facecolor('white')
plt.gcf().patch.set_facecolor('none')

# √Åreas sombreadas
plt.fill_between(df['fecha'], df['total_ton_enviadas'], color='#a57f2c', alpha=0.35)
plt.fill_between(df['fecha'], df['urea_ton_enviadas'], color='#6bbf59', alpha=0.35)
plt.fill_between(df['fecha'], df['dap_ton_enviadas'], color='#204e4a', alpha=0.35)

# L√≠neas con puntos
plt.plot(df['fecha'], df['total_ton_enviadas'], color='#a57f2c', linewidth=2.2, marker='o', markersize=4, label='Total Enviado (ton)')
plt.plot(df['fecha'], df['urea_ton_enviadas'], color='#6bbf59', linewidth=1.8, linestyle='--', marker='o', markersize=3, label='UREA Enviada (ton)')
plt.plot(df['fecha'], df['dap_ton_enviadas'], color='#204e4a', linewidth=1.8, linestyle='--', marker='o', markersize=3, label='DAP Enviado (ton)')

# L√≠neas viernes
for fecha in etiquetas_fecha:
    plt.axvline(x=fecha, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)

# Anotar valores
for _, fila in df_valores.iterrows():
    plt.text(
        fila['fecha'],
        fila['total_ton_enviadas'] + 300,
        f"{int(fila['total_ton_enviadas']):,}",
        color='black',
        fontsize=8.3,
        ha='center',
        va='bottom',
        rotation=90
    )

# Ejes y estilo
plt.xlabel("Fecha", fontsize=12, color='black')
plt.ylabel("Toneladas Enviadas", fontsize=12, color='black')
plt.xlim([df['fecha'].min(), df['fecha'].max()])
ax.get_yaxis().set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))
plt.yticks(color='black')
ax.spines['top'].set_visible(False)
ax.yaxis.grid(True, linestyle='--', linewidth=0.5, alpha=0.4)

# Eje X
ax.set_xticks(etiquetas_fecha)
ax.set_xticklabels(etiquetas_formato, fontsize=8, ha='center', color='black')

# Leyenda
plt.legend(
    loc='upper left',
    bbox_to_anchor=(0, 1.25),
    facecolor='none',
    edgecolor='none',
    labelcolor='black',
    fontsize=9
)

# Guardar gr√°fico
plt.tight_layout()
plt.savefig(ruta_salida, dpi=300, bbox_inches='tight', transparent=True)
plt.close()

print(f"‚úÖ Gr√°fico generado: {ruta_salida}")



---
# grafico_recibido_vs_entregado.py

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter

# Diccionarios espa√±ol
DIAS = {0: "lun", 1: "mar", 2: "mi√©", 3: "jue", 4: "vier", 5: "s√°b", 6: "dom"}
MESES = {
    1: "ene", 2: "feb", 3: "mar", 4: "abr", 5: "may", 6: "jun",
    7: "jul", 8: "ago", 9: "sep", 10: "oct", 11: "nov", 12: "dic"
}

# Rutas
ruta_envios = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/abasto_y_remanente_x_dia_sin_transito_2025.csv"
ruta_entregas = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/entregas_diarias_2025.csv"
ruta_salida = "/Users/Arturo/AGRICULTURA/INFORMES/Img_Temporales/grafico_comparativo_abasto_entregas.png"

# Cargar datos
df_envios = pd.read_csv(ruta_envios)
df_entregas = pd.read_csv(ruta_entregas)

# Fechas y limpieza
df_envios['fecha'] = pd.to_datetime(df_envios['fecha'])
df_entregas['fecha'] = pd.to_datetime(df_entregas['fecha'])
df_envios = df_envios[df_envios['total_recibido_2025_ton'] > 0]
df_entregas = df_entregas[df_entregas['total_ton_entregada'] > 0]

# Unir en base a fechas
df = pd.merge(
    df_envios[['fecha', 'total_recibido_2025_ton']],
    df_entregas[['fecha', 'total_ton_entregada']],
    on='fecha',
    how='outer'
)
df = df.sort_values('fecha').fillna(0)

# Agregar d√≠a de la semana y semana ISO
df['dia_semana'] = df['fecha'].dt.dayofweek
df['semana'] = df['fecha'].dt.isocalendar().week

# Etiquetas en eje X (solo viernes)
fechas_viernes = df[df['dia_semana'] == 4]['fecha'].tolist()
etiquetas_fecha = fechas_viernes
etiquetas_formato = [
    f"{DIAS[f.weekday()]}\n{f.day:02d}-{MESES[f.month]}" for f in etiquetas_fecha
]

# D√≠as con anotaciones
lunes_a_jueves = df[df['dia_semana'].isin([0, 1, 2, 3])]
idx_maximos = (lunes_a_jueves
               .assign(suma=lunes_a_jueves['total_recibido_2025_ton'] + lunes_a_jueves['total_ton_entregada'])
               .groupby('semana')['suma']
               .idxmax())
maximos_lj = df.loc[idx_maximos]
fechas_para_anotar = sorted(set(fechas_viernes + maximos_lj['fecha'].tolist()))

# Crear gr√°fico
fig, ax = plt.subplots(figsize=(12.6, 6.5))
plt.style.use('default')
ax.set_facecolor('white')
plt.gcf().patch.set_facecolor('none')

# √Årea + l√≠nea de env√≠os
plt.fill_between(df['fecha'], df['total_recibido_2025_ton'], color='#1e5b4f', alpha=0.25)
plt.plot(df['fecha'], df['total_recibido_2025_ton'], color='#1e5b4f', linewidth=2.5, marker='o', markersize=4, label='Fertilizante Recibido (ton)')

# √Årea + l√≠nea de entregas
plt.fill_between(df['fecha'], df['total_ton_entregada'], color='#a57f2c', alpha=0.25)
plt.plot(df['fecha'], df['total_ton_entregada'], color='#a57f2c', linewidth=2.5, marker='o', markersize=4, label='Fertilizante Entregado (ton)')

# L√≠neas verticales
for fecha in etiquetas_fecha:
    plt.axvline(x=fecha, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)

# Anotar d√≠as clave: solo el valor m√°s alto entre recibido y entregado
for _, fila in df[df['fecha'].isin(fechas_para_anotar)].iterrows():
    fecha = fila['fecha']
    recibido = fila['total_recibido_2025_ton']
    entregado = fila['total_ton_entregada']

    if recibido > entregado and recibido <= 15000:
        plt.text(fecha, recibido + 300, f"{int(recibido):,}".replace(",", ","),
                 color='#1e5b4f', fontsize=8.5, ha='center', va='bottom', rotation=90)
    elif entregado >= recibido and entregado <= 15000:
        plt.text(fecha, entregado + 300, f"{int(entregado):,}".replace(",", ","),
                 color='#a57f2c', fontsize=8.5, ha='center', va='bottom', rotation=90)

# Anotar valor m√°s alto
fila_max = df.loc[(df['total_recibido_2025_ton'] + df['total_ton_entregada']).idxmax()]
fecha_max = fila_max['fecha']
valor_max = max(fila_max['total_recibido_2025_ton'], fila_max['total_ton_entregada'])
plt.annotate(
    f"{int(valor_max):,}".replace(",", ","),
    xy=(fecha_max, 15000), xytext=(fecha_max, 15200), textcoords='data',
    arrowprops=dict(facecolor='black', arrowstyle='->', lw=0.8),
    ha='center', fontsize=9, color='black'
)

# KPIs (en l√≠nea horizontal derecha)
recibido_total = int(df['total_recibido_2025_ton'].sum())
entregado_total = int(df['total_ton_entregada'].sum())
inventario_total = recibido_total - entregado_total

# Posiciones en el mismo nivel de la leyenda pero a la derecha
kpi_y_valor = 1.13
kpi_y_etiqueta = 1.08

plt.text(0.50, kpi_y_valor, f"{recibido_total:,}".replace(",", ","), transform=ax.transAxes,
         ha='left', fontsize=13, color='#1e5b4f', weight='bold')
plt.text(0.50, kpi_y_etiqueta, "Recibido Total (ton)", transform=ax.transAxes,
         ha='left', fontsize=10.5, color='#444444')

plt.text(0.70, kpi_y_valor, f"{entregado_total:,}".replace(",", ","), transform=ax.transAxes,
         ha='left', fontsize=13, color='#a57f2c', weight='bold')
plt.text(0.70, kpi_y_etiqueta, "Entregado Total (ton)", transform=ax.transAxes,
         ha='left', fontsize=10.5, color='#444444')

plt.text(0.90, kpi_y_valor, f"{inventario_total:,}".replace(",", ","), transform=ax.transAxes,
         ha='left', fontsize=13, color='#c60052', weight='bold')
plt.text(0.90, kpi_y_etiqueta, "Inventarios (ton)", transform=ax.transAxes,
         ha='left', fontsize=10.5, color='#444444')

# Ejes
plt.xlabel("Fecha", fontsize=12, color='black')
plt.ylabel("Fertilizante Recibido (ton) y Fertilizante Entregado (ton)", fontsize=12, color='black')
plt.xlim([df['fecha'].min(), df['fecha'].max()])
plt.ylim([0, 15000])
ax.get_yaxis().set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'.replace(",", ",")))
plt.yticks(color='black')
ax.spines['top'].set_visible(False)
ax.yaxis.grid(True, linestyle='--', linewidth=0.5, alpha=0.5)

# Eje X
ax.set_xticks(etiquetas_fecha)
ax.set_xticklabels(etiquetas_formato, fontsize=8, ha='center', color='black')

# Leyenda
plt.legend(
    loc='upper left',
    bbox_to_anchor=(0, 1.13),
    facecolor='none',
    edgecolor='none',
    labelcolor='black',
    fontsize=9
)

# Guardar
plt.tight_layout()
plt.savefig(ruta_salida, dpi=300, bbox_inches='tight', transparent=True)
plt.close()

print(f"‚úÖ Gr√°fico comparativo abasto vs entregas exportado en: {ruta_salida}")



---
# grafico_recibido_vs_entregado_x_semana.py

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
from datetime import timedelta

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 1. Diccionarios en espa√±ol (para etiquetas)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
DIAS = {0: "lun", 1: "mar", 2: "mi√©", 3: "jue", 4: "vie", 5: "s√°b", 6: "dom"}
MESES = {
    1: "ene", 2: "feb", 3: "mar", 4: "abr", 5: "may", 6: "jun",
    7: "jul", 8: "ago", 9: "sep", 10: "oct", 11: "nov", 12: "dic"
}

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 2. Rutas de entrada / salida
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
ruta_envios  = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/abasto_y_remanente_x_dia_sin_transito_2025.csv"
ruta_entregas = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/entregas_diarias_2025.csv"
ruta_salida  = "/Users/Arturo/AGRICULTURA/INFORMES/Img_Temporales/grafico_comparativo_abasto_entregas_semanal.png"

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 3. Cargar y depurar datos diarios
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
env = pd.read_csv(ruta_envios,  parse_dates=['fecha'])
ent = pd.read_csv(ruta_entregas, parse_dates=['fecha'])

env = env[env['total_recibido_2025_ton']   > 0]
ent = ent[ent['total_ton_entregada']       > 0]

df = (
    env[['fecha', 'total_recibido_2025_ton']]
    .merge(ent[['fecha', 'total_ton_entregada']], how='outer', on='fecha')
    .fillna(0)
)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 4. Agrupar por semana (semana que FINALIZA en s√°bado) 
#    ‚Üí 'W-SAT' genera un registro por cada s√°bado; as√≠
#      la semana operativa (lun‚Äìs√°b) queda completa.
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
df_sem = (
    df.set_index('fecha')
      .resample('W-SAT')                # corte al s√°bado
      .sum(min_count=1)                 # evita semanas vac√≠as
      .reset_index()
)

# A√±ade n√∫mero de semana ISO y una etiqueta amigable
df_sem['semana'] = df_sem['fecha'].dt.isocalendar().week
df_sem['etiqueta'] = df_sem['fecha'].apply(
    lambda f: f"{DIAS[f.weekday()]}\n{f.day:02d}-{MESES[f.month]}"
)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 5. KPI totales
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
tot_recibido  = int(df_sem['total_recibido_2025_ton'].sum())
tot_entregado = int(df_sem['total_ton_entregada'].sum())
tot_inv       = tot_recibido - tot_entregado

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 6. Gr√°fico comparativo semanal
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
plt.style.use('default')
fig, ax = plt.subplots(figsize=(12.6, 6.5))
ax.set_facecolor('white')
fig.patch.set_facecolor('none')

# √Årea + l√≠nea de env√≠os
ax.fill_between(df_sem['fecha'], df_sem['total_recibido_2025_ton'],
                color='#1e5b4f', alpha=0.25)
ax.plot(df_sem['fecha'], df_sem['total_recibido_2025_ton'],
        color='#1e5b4f', linewidth=2.5, marker='o', markersize=4,
        label='Fertilizante Recibido (ton)')

# √Årea + l√≠nea de entregas
ax.fill_between(df_sem['fecha'], df_sem['total_ton_entregada'],
                color='#a57f2c', alpha=0.25)
ax.plot(df_sem['fecha'], df_sem['total_ton_entregada'],
        color='#a57f2c', linewidth=2.5, marker='o', markersize=4,
        label='Fertilizante Entregado (ton)')

# L√≠neas verticales en cada s√°bado (fin de semana operativa)
for f in df_sem['fecha']:
    ax.axvline(x=f, color='gray', linestyle='--', linewidth=0.5, alpha=0.4)

# Etiquetas en eje X (formato sab dd-mes)
ax.set_xticks(df_sem['fecha'])
ax.set_xticklabels(df_sem['etiqueta'], fontsize=8, ha='center', color='black')

# Anotar el valor semanal m√°s alto entre recibido/entregado
fila_max = df_sem.loc[
    (df_sem['total_recibido_2025_ton'] + df_sem['total_ton_entregada']).idxmax()
]
valor_max = max(fila_max['total_recibido_2025_ton'],
                fila_max['total_ton_entregada'])
ax.annotate(f"{valor_max:,}".replace(",", ","),
            xy=(fila_max['fecha'], valor_max),
            xytext=(fila_max['fecha'], valor_max + 1000),
            textcoords='data',
            arrowprops=dict(facecolor='black', arrowstyle='->', lw=0.8),
            ha='center', fontsize=9, color='black')

# KPIs
kpi_y_valor   = 1.13
kpi_y_etiqueta = 1.08
ax.text(0.50, kpi_y_valor,   f"{tot_recibido:,}".replace(",", ","), transform=ax.transAxes,
        ha='left', fontsize=13, color='#1e5b4f',  weight='bold')
ax.text(0.50, kpi_y_etiqueta, "Recibido Total (ton)", transform=ax.transAxes,
        ha='left', fontsize=10.5, color='#444444')

ax.text(0.70, kpi_y_valor,   f"{tot_entregado:,}".replace(",", ","), transform=ax.transAxes,
        ha='left', fontsize=13, color='#a57f2c',  weight='bold')
ax.text(0.70, kpi_y_etiqueta, "Entregado Total (ton)", transform=ax.transAxes,
        ha='left', fontsize=10.5, color='#444444')

ax.text(0.90, kpi_y_valor,   f"{tot_inv:,}".replace(",", ","), transform=ax.transAxes,
        ha='left', fontsize=13, color='#c60052', weight='bold')
ax.text(0.90, kpi_y_etiqueta, "Inventarios (ton)", transform=ax.transAxes,
        ha='left', fontsize=10.5, color='#444444')

# Ejes y estilo
ax.set_xlabel("Semana (fin de semana operativa)", fontsize=12, color='black')
ax.set_ylabel("Toneladas", fontsize=12, color='black')
ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'.replace(",", ",")))
ax.spines['top'].set_visible(False)
ax.yaxis.grid(True, linestyle='--', linewidth=0.5, alpha=0.5)
ax.set_ylim(0, df_sem[['total_recibido_2025_ton','total_ton_entregada']].max().max() * 1.2)

# Leyenda
ax.legend(loc='upper left', bbox_to_anchor=(0, 1.13),
          facecolor='none', edgecolor='none',
          labelcolor='black', fontsize=9)

# Guardar
plt.tight_layout()
plt.savefig(ruta_salida, dpi=300, bbox_inches='tight', transparent=True)
plt.close()
print(f"‚úÖ Gr√°fico comparativo semanal exportado en: {ruta_salida}")



---
# identificar_nombre_columnas.py

import pandas as pd
import os

# Ajusta la ruta a tu CSV real
ruta_csv = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/1051-FERTILIZANTES-TRANSFERENCIAS-NACIONAL-ANUAL_2025-03-08 14_32_58_2025_TR.csv"

def ver_nombres_columnas(ruta):
    # Lee el CSV con la codificaci√≥n que uses normalmente
    df = pd.read_csv(ruta, dtype=str, encoding="utf-8")

    print("\nNombres de columnas le√≠das (tal cual las ve pandas):")
    for col in df.columns:
        print(repr(col))

if __name__ == "__main__":
    if os.path.exists(ruta_csv):
        ver_nombres_columnas(ruta_csv)
    else:
        print(f"El archivo '{ruta_csv}' no existe.")



---
# importar_derechohabientes.py

import os
import pandas as pd
import glob
import time
from sqlalchemy import text
from conexion import engine, psycopg_conn, DB_NAME

# Iniciar medici√≥n de tiempo
inicio = time.time()

# 1Ô∏è‚É£ Leer y combinar todos los archivos CSV en derechohabientes
carpeta_derechohabientes = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/derechohabientes/"
archivos_csv = glob.glob(os.path.join(carpeta_derechohabientes, "*.csv"))

# üß† Diccionario de renombrado: nombres nuevos ‚Üí nombres esperados
RENOMBRAR_COLUMNAS = {
    "urea_25_kg_anio_actual": "urea_anio_actual",
    "dap_25_kg_anio_actual": "dap_anio_actual",
    "dap_remanente_25_kg": "dap_remanente",
    "urea_remanente_25_kg": "urea_remanente"
}

# 1Ô∏è‚É£ Leer y combinar todos los archivos CSV en derechohabientes
carpeta_derechohabientes = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/derechohabientes/"
archivos_csv = glob.glob(os.path.join(carpeta_derechohabientes, "*.CSV"))

df_derechohabientes = pd.concat(
    (
        pd.read_csv(archivo, encoding="utf-8", delimiter=",", dtype={12: str})
        .rename(columns=lambda col: col.lower().strip())  # normalizar nombres
        .rename(columns=RENOMBRAR_COLUMNAS)               # corregir nombres nuevos
        for archivo in archivos_csv
    ),
    ignore_index=True
)

df_derechohabientes.columns = df_derechohabientes.columns.str.lower().str.strip()
print(f"üìÇ Se han combinado {len(archivos_csv)} archivos de la carpeta derechohabientes.")

# 2Ô∏è‚É£ Eliminar registros marcados con errores
archivo_eliminar = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/derechohabientes_eliminar_2025.xlsx"
df_eliminar = pd.read_excel(archivo_eliminar, dtype=str)
df_eliminar.columns = df_eliminar.columns.str.lower().str.strip()

if "acuse_estatal" not in df_eliminar.columns:
    print("‚ùå ERROR: Falta columna 'acuse_estatal' en archivo de eliminaci√≥n.")
    exit()

antes_eliminar = len(df_derechohabientes)
df_derechohabientes = df_derechohabientes[~df_derechohabientes["acuse_estatal"].isin(df_eliminar["acuse_estatal"])]
print(f"‚úÖ Se eliminaron {antes_eliminar - len(df_derechohabientes)} registros.")

# 3Ô∏è‚É£ Agregar registros corregidos (ajustando solo la fecha_entrega de este lote)
archivo_corregidos = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/derechohabientes_corregidos_2025.csv"
if os.path.exists(archivo_corregidos):
    df_corregidos = pd.read_csv(archivo_corregidos, encoding="utf-8", delimiter=",")
    df_corregidos.columns = df_corregidos.columns.str.lower().str.strip()

    if "fecha_entrega" in df_corregidos.columns:
        df_corregidos["fecha_entrega"] = pd.to_datetime(
            df_corregidos["fecha_entrega"],
            format="%d/%m/%y",
            errors="coerce"
        ).dt.strftime("%Y-%m-%d")

    df_derechohabientes = pd.concat([df_derechohabientes, df_corregidos], ignore_index=True)
    print("‚úÖ Se combin√≥ el archivo derechohabientes_corregidos_2025.csv.")
else:
    print("‚ö†Ô∏è No se encontr√≥ archivo de registros corregidos, se omiti√≥.")

# 4Ô∏è‚É£ Conversi√≥n de tipos
columnas_integer = ["clave_estado_predio_capturada", "clave_municipio_predio_capturada", "clave_localidad_predio_capturada",
                    "id_nu_solicitud", "id_cdf_entrega", "folio_persona", "clave_ddr", "clave_cader_ventanilla",
                    "superficie_apoyada"]
columnas_integer = [col for col in columnas_integer if col in df_derechohabientes.columns]
for col in columnas_integer:
    df_derechohabientes[col] = pd.to_numeric(df_derechohabientes[col], errors='coerce').astype("Int64")

# Columnas enteras que llegan como decimales
columnas_int_con_decimales = ["dap_anio_actual", "urea_anio_actual", "dap_remanente", "urea_remanente"]
columnas_int_con_decimales = [col for col in columnas_int_con_decimales if col in df_derechohabientes.columns]
for col in columnas_int_con_decimales:
    df_derechohabientes[col] = pd.to_numeric(df_derechohabientes[col], errors='coerce').fillna(0).astype("Int64")

# Columnas decimales reales
columnas_numeric = ["ton_dap_entregada", "ton_urea_entregada"]
df_derechohabientes[columnas_numeric] = df_derechohabientes[columnas_numeric].apply(pd.to_numeric, errors='coerce').fillna(0)

# Fechas generales (ya corregidas las del archivo corregido)
if "fecha_entrega" in df_derechohabientes.columns:
    df_derechohabientes["fecha_entrega"] = pd.to_datetime(
        df_derechohabientes["fecha_entrega"],
        errors="coerce"
    ).dt.strftime("%Y-%m-%d")

# 5Ô∏è‚É£ Reordenar columnas seg√∫n estructura real de PostgreSQL
with engine.connect() as conn:
    result = conn.execute(text(
        "SELECT column_name FROM information_schema.columns WHERE table_name = 'derechohabientes';"
    ))
    columnas_postgres = [row[0] for row in result.fetchall()]
df_derechohabientes = df_derechohabientes[columnas_postgres]


# üîÑ Correcci√≥n de toneladas y remanentes seg√∫n padr√≥n (antes del COPY)
try:
    print("üîß Corrigiendo datos de DAP y UREA entregadas comparando con el padr√≥n...")

    query_padron = """
        SELECT acuse_estatal, dap_ton, urea_ton
        FROM derechohabientes_padrones_2025
    """
    df_padron = pd.read_sql(query_padron, engine)

    df_derechohabientes = df_derechohabientes.merge(
        df_padron,
        on='acuse_estatal',
        how='left',
        suffixes=('', '_padron')
    )

    mask_dif = (
        (df_derechohabientes['ton_dap_entregada'] != df_derechohabientes['dap_ton']) |
        (df_derechohabientes['ton_urea_entregada'] != df_derechohabientes['urea_ton'])
    )

    df_derechohabientes.loc[mask_dif, 'ton_dap_entregada'] = df_derechohabientes.loc[mask_dif, 'dap_ton']
    df_derechohabientes.loc[mask_dif, 'ton_urea_entregada'] = df_derechohabientes.loc[mask_dif, 'urea_ton']
    df_derechohabientes.loc[mask_dif, 'dap_anio_actual'] = (df_derechohabientes.loc[mask_dif, 'dap_ton'] / 0.025).round().astype("Int64")
    df_derechohabientes.loc[mask_dif, 'urea_anio_actual'] = (df_derechohabientes.loc[mask_dif, 'urea_ton'] / 0.025).round().astype("Int64")
    df_derechohabientes.loc[mask_dif, 'dap_remanente'] = 0
    df_derechohabientes.loc[mask_dif, 'urea_remanente'] = 0

    df_derechohabientes.drop(columns=['dap_ton', 'urea_ton'], inplace=True)

    print(f"‚úÖ Se corrigieron {mask_dif.sum()} registros antes de cargar a PostgreSQL.")
except Exception as e:
    print(f"‚ùå Error al aplicar correcci√≥n previa al COPY: {e}")
    import sys
    sys.exit(1)

# 6Ô∏è‚É£ Exportar CSV temporal para COPY
csv_temp_path = "/tmp/derechohabientes_temp.csv"
df_derechohabientes.to_csv(csv_temp_path, index=False, header=False, encoding="utf-8", na_rep='')

# 7Ô∏è‚É£ Cargar datos con COPY para eficiencia
try:
    with psycopg_conn, psycopg_conn.cursor() as cur:
        print("üóëÔ∏è Truncando tabla derechohabientes...")
        cur.execute("TRUNCATE TABLE derechohabientes;")
        print("üì• Importando datos con COPY...")
        with open(csv_temp_path, "r", encoding="utf-8") as f:
            cur.copy_expert(f"COPY derechohabientes ({', '.join(columnas_postgres)}) FROM STDIN WITH CSV", f)
    print(f"‚úÖ Se han sustituido los datos antiguos por los nuevos en la tabla 'derechohabientes' de la base '{DB_NAME}'.")
    print(f"üìä Registros importados: {len(df_derechohabientes)}")
except Exception as e:
    print(f"‚ùå Error durante la importaci√≥n con COPY: {e}")

# Tiempo total
fin = time.time()
print(f"‚è±Ô∏è Tiempo total: {round(fin - inicio, 2)} segundos.")


---
# importar_derechohabientes_old.py

import os
import pandas as pd
import glob
import time
from sqlalchemy import text
from conexion import engine, psycopg_conn, DB_NAME

# Iniciar medici√≥n de tiempo
inicio = time.time()

# 1Ô∏è‚É£ Leer y combinar todos los archivos CSV en derechohabientes
carpeta_derechohabientes = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/derechohabientes/"
archivos_csv = glob.glob(os.path.join(carpeta_derechohabientes, "*.csv"))

df_derechohabientes = pd.concat(
    (pd.read_csv(archivo, encoding="utf-8", delimiter=",") for archivo in archivos_csv),
    ignore_index=True
)

df_derechohabientes.columns = df_derechohabientes.columns.str.lower().str.strip()
print(f"üìÇ Se han combinado {len(archivos_csv)} archivos de la carpeta derechohabientes.")

# 2Ô∏è‚É£ Eliminar registros marcados con errores
archivo_eliminar = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/derechohabientes_eliminar_2025.xlsx"
df_eliminar = pd.read_excel(archivo_eliminar, dtype=str)
df_eliminar.columns = df_eliminar.columns.str.lower().str.strip()

if "acuse_estatal" not in df_eliminar.columns:
    print("‚ùå ERROR: Falta columna 'acuse_estatal' en archivo de eliminaci√≥n.")
    exit()

antes_eliminar = len(df_derechohabientes)
df_derechohabientes = df_derechohabientes[~df_derechohabientes["acuse_estatal"].isin(df_eliminar["acuse_estatal"])]
print(f"‚úÖ Se eliminaron {antes_eliminar - len(df_derechohabientes)} registros.")

# 3Ô∏è‚É£ Agregar registros corregidos (ajustando solo la fecha_entrega de este lote)
archivo_corregidos = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/derechohabientes_corregidos_2025.csv"
if os.path.exists(archivo_corregidos):
    df_corregidos = pd.read_csv(archivo_corregidos, encoding="utf-8", delimiter=",")
    df_corregidos.columns = df_corregidos.columns.str.lower().str.strip()

    if "fecha_entrega" in df_corregidos.columns:
        df_corregidos["fecha_entrega"] = pd.to_datetime(
            df_corregidos["fecha_entrega"],
            format="%d/%m/%y",
            errors="coerce"
        ).dt.strftime("%Y-%m-%d")

    df_derechohabientes = pd.concat([df_derechohabientes, df_corregidos], ignore_index=True)
    print("‚úÖ Se combin√≥ el archivo derechohabientes_corregidos_2025.csv.")
else:
    print("‚ö†Ô∏è No se encontr√≥ archivo de registros corregidos, se omiti√≥.")

# 4Ô∏è‚É£ Conversi√≥n de tipos
columnas_integer = ["clave_estado_predio_capturada", "clave_municipio_predio_capturada", "clave_localidad_predio_capturada",
                    "id_nu_solicitud", "id_cdf_entrega", "folio_persona", "clave_ddr", "clave_cader_ventanilla",
                    "superficie_apoyada"]
columnas_integer = [col for col in columnas_integer if col in df_derechohabientes.columns]
for col in columnas_integer:
    df_derechohabientes[col] = pd.to_numeric(df_derechohabientes[col], errors='coerce').astype("Int64")

# Columnas enteras que llegan como decimales
columnas_int_con_decimales = ["dap_anio_actual", "urea_anio_actual", "dap_remanente", "urea_remanente"]
columnas_int_con_decimales = [col for col in columnas_int_con_decimales if col in df_derechohabientes.columns]
for col in columnas_int_con_decimales:
    df_derechohabientes[col] = pd.to_numeric(df_derechohabientes[col], errors='coerce').fillna(0).astype("Int64")

# Columnas decimales reales
columnas_numeric = ["ton_dap_entregada", "ton_urea_entregada"]
df_derechohabientes[columnas_numeric] = df_derechohabientes[columnas_numeric].apply(pd.to_numeric, errors='coerce').fillna(0)

# Fechas generales (ya corregidas las del archivo corregido)
if "fecha_entrega" in df_derechohabientes.columns:
    df_derechohabientes["fecha_entrega"] = pd.to_datetime(
        df_derechohabientes["fecha_entrega"],
        errors="coerce"
    ).dt.strftime("%Y-%m-%d")

# 5Ô∏è‚É£ Reordenar columnas seg√∫n estructura real de PostgreSQL
with engine.connect() as conn:
    result = conn.execute(text(
        "SELECT column_name FROM information_schema.columns WHERE table_name = 'derechohabientes';"
    ))
    columnas_postgres = [row[0] for row in result.fetchall()]
df_derechohabientes = df_derechohabientes[columnas_postgres]

# 6Ô∏è‚É£ Exportar CSV temporal para COPY
csv_temp_path = "/tmp/derechohabientes_temp.csv"
df_derechohabientes.to_csv(csv_temp_path, index=False, header=False, encoding="utf-8", na_rep='')

# 7Ô∏è‚É£ Cargar datos con COPY para eficiencia
try:
    with psycopg_conn, psycopg_conn.cursor() as cur:
        print("üóëÔ∏è Truncando tabla derechohabientes...")
        cur.execute("TRUNCATE TABLE derechohabientes;")
        print("üì• Importando datos con COPY...")
        with open(csv_temp_path, "r", encoding="utf-8") as f:
            cur.copy_expert(f"COPY derechohabientes ({', '.join(columnas_postgres)}) FROM STDIN WITH CSV", f)
    print(f"‚úÖ Se han sustituido los datos antiguos por los nuevos en la tabla 'derechohabientes' de la base '{DB_NAME}'.")
    print(f"üìä Registros importados: {len(df_derechohabientes)}")
except Exception as e:
    print(f"‚ùå Error durante la importaci√≥n con COPY: {e}")

# Tiempo total
fin = time.time()
print(f"‚è±Ô∏è Tiempo total: {round(fin - inicio, 2)} segundos.")
    


---
# importar_derechohabientes_old2.py

import os
import pandas as pd
import glob
import time
from sqlalchemy import text
from conexion import engine, psycopg_conn, DB_NAME

# Iniciar medici√≥n de tiempo
inicio = time.time()

# 1Ô∏è‚É£ Leer y combinar todos los archivos CSV en derechohabientes
carpeta_derechohabientes = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/derechohabientes/"
archivos_csv = glob.glob(os.path.join(carpeta_derechohabientes, "*.csv"))

df_derechohabientes = pd.concat(
    (pd.read_csv(archivo, encoding="utf-8", delimiter=",", dtype={12: str}) for archivo in archivos_csv),
    ignore_index=True
)

df_derechohabientes.columns = df_derechohabientes.columns.str.lower().str.strip()
print(f"üìÇ Se han combinado {len(archivos_csv)} archivos de la carpeta derechohabientes.")

# 2Ô∏è‚É£ Eliminar registros marcados con errores
archivo_eliminar = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/derechohabientes_eliminar_2025.xlsx"
df_eliminar = pd.read_excel(archivo_eliminar, dtype=str)
df_eliminar.columns = df_eliminar.columns.str.lower().str.strip()

if "acuse_estatal" not in df_eliminar.columns:
    print("‚ùå ERROR: Falta columna 'acuse_estatal' en archivo de eliminaci√≥n.")
    exit()

antes_eliminar = len(df_derechohabientes)
df_derechohabientes = df_derechohabientes[~df_derechohabientes["acuse_estatal"].isin(df_eliminar["acuse_estatal"])]
print(f"‚úÖ Se eliminaron {antes_eliminar - len(df_derechohabientes)} registros.")

# 3Ô∏è‚É£ Agregar registros corregidos (ajustando solo la fecha_entrega de este lote)
archivo_corregidos = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/derechohabientes_corregidos_2025.csv"
if os.path.exists(archivo_corregidos):
    df_corregidos = pd.read_csv(archivo_corregidos, encoding="utf-8", delimiter=",")
    df_corregidos.columns = df_corregidos.columns.str.lower().str.strip()

    if "fecha_entrega" in df_corregidos.columns:
        df_corregidos["fecha_entrega"] = pd.to_datetime(
            df_corregidos["fecha_entrega"],
            format="%d/%m/%y",
            errors="coerce"
        ).dt.strftime("%Y-%m-%d")

    df_derechohabientes = pd.concat([df_derechohabientes, df_corregidos], ignore_index=True)
    print("‚úÖ Se combin√≥ el archivo derechohabientes_corregidos_2025.csv.")
else:
    print("‚ö†Ô∏è No se encontr√≥ archivo de registros corregidos, se omiti√≥.")

# 4Ô∏è‚É£ Conversi√≥n de tipos
columnas_integer = ["clave_estado_predio_capturada", "clave_municipio_predio_capturada", "clave_localidad_predio_capturada",
                    "id_nu_solicitud", "id_cdf_entrega", "folio_persona", "clave_ddr", "clave_cader_ventanilla",
                    "superficie_apoyada"]
columnas_integer = [col for col in columnas_integer if col in df_derechohabientes.columns]
for col in columnas_integer:
    df_derechohabientes[col] = pd.to_numeric(df_derechohabientes[col], errors='coerce').astype("Int64")

# Columnas enteras que llegan como decimales
columnas_int_con_decimales = ["dap_anio_actual", "urea_anio_actual", "dap_remanente", "urea_remanente"]
columnas_int_con_decimales = [col for col in columnas_int_con_decimales if col in df_derechohabientes.columns]
for col in columnas_int_con_decimales:
    df_derechohabientes[col] = pd.to_numeric(df_derechohabientes[col], errors='coerce').fillna(0).astype("Int64")

# Columnas decimales reales
columnas_numeric = ["ton_dap_entregada", "ton_urea_entregada"]
df_derechohabientes[columnas_numeric] = df_derechohabientes[columnas_numeric].apply(pd.to_numeric, errors='coerce').fillna(0)

# Fechas generales (ya corregidas las del archivo corregido)
if "fecha_entrega" in df_derechohabientes.columns:
    df_derechohabientes["fecha_entrega"] = pd.to_datetime(
        df_derechohabientes["fecha_entrega"],
        errors="coerce"
    ).dt.strftime("%Y-%m-%d")

# 5Ô∏è‚É£ Reordenar columnas seg√∫n estructura real de PostgreSQL
with engine.connect() as conn:
    result = conn.execute(text(
        "SELECT column_name FROM information_schema.columns WHERE table_name = 'derechohabientes';"
    ))
    columnas_postgres = [row[0] for row in result.fetchall()]
df_derechohabientes = df_derechohabientes[columnas_postgres]

# 6Ô∏è‚É£ Exportar CSV temporal para COPY
csv_temp_path = "/tmp/derechohabientes_temp.csv"
df_derechohabientes.to_csv(csv_temp_path, index=False, header=False, encoding="utf-8", na_rep='')

# 7Ô∏è‚É£ Cargar datos con COPY para eficiencia
try:
    with psycopg_conn, psycopg_conn.cursor() as cur:
        print("üóëÔ∏è Truncando tabla derechohabientes...")
        cur.execute("TRUNCATE TABLE derechohabientes;")
        print("üì• Importando datos con COPY...")
        with open(csv_temp_path, "r", encoding="utf-8") as f:
            cur.copy_expert(f"COPY derechohabientes ({', '.join(columnas_postgres)}) FROM STDIN WITH CSV", f)
    print(f"‚úÖ Se han sustituido los datos antiguos por los nuevos en la tabla 'derechohabientes' de la base '{DB_NAME}'.")
    print(f"üìä Registros importados: {len(df_derechohabientes)}")
except Exception as e:
    print(f"‚ùå Error durante la importaci√≥n con COPY: {e}")

# Tiempo total
fin = time.time()
print(f"‚è±Ô∏è Tiempo total: {round(fin - inicio, 2)} segundos.")



---
# importar_directorio_csv.py

#!/usr/bin/env python3
"""
importar_directorio_csv.py
Carga inicial del directorio desde CSV:
  - Trunca la tabla
  - Lee todo el CSV (UTF-8-SIG)
  - Inserta sin validaciones de unicidad
Uso:
  $ python importar_directorio_csv.py
Requiere: conexion.py (exporta `engine`)
"""
import pandas as pd
import unicodedata
from sqlalchemy import text
from conexion import engine

# Ruta al CSV exportado de Excel, con codificaci√≥n UTF-8-SIG
CSV_PATH = (
    "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/"
    "DIRECTORIO DE COORDINACIONES ESTATALES 2025_09062025.csv"
)

def quitar_acentos(s):
    return (unicodedata.normalize("NFKD", s)
            .encode("ascii", "ignore")
            .decode()) if isinstance(s, str) else s

def load_csv():
    # 1) Leer CSV
    df = pd.read_csv(CSV_PATH, encoding="utf-8-sig", dtype=str)

    # 2) Normalizar encabezados a snake_case sin acentos
    df.columns = (
        df.columns
          .str.strip()
          .map(quitar_acentos)
          .str.lower()
          .str.replace(" ", "_")
          .str.replace(r"[^0-9a-z_]", "", regex=True)
    )

    # 3) Asegurar columnas m√≠nimas
    req = {"estado","unidad_operativa","cargo","nombre_completo"}
    faltan = req - set(df.columns)
    if faltan:
        raise RuntimeError(f"Faltan columnas en CSV: {faltan}")

    # 4) Rellenar opcionales
    df["zona_operativa"]      = df.get("zona_operativa", "N/A").fillna("N/A")
    df["id_ceda_agricultura"] = df.get("id_ceda_agricultura", "N/A").fillna("N/A")
    df["curp"]                = df.get("curp", None)
    df["correo_electronico"]  = df.get("correo_electronico", None)
    df["telefono"]            = df.get("telefono", None)
    df["comentarios"]         = df.get("comentarios", None)
    df["fecha_actualizacion"] = pd.Timestamp.now()

    # 5) Estandarizar texto
    for c in ["estado","unidad_operativa","zona_operativa","cargo"]:
        df[c] = (df[c]
                    .astype(str)
                    .str.strip()
                    .str.upper()
                    .replace({"": "N/A"}))
    return df

def main():
    df = load_csv()
    with engine.begin() as conn:
        # A) Truncar tabla
        conn.execute(text("TRUNCATE TABLE directorio;"))
        # B) Insertar todo
        df.to_sql(
            "directorio",
            conn,
            if_exists="append",
            index=False,
            method="multi",
            chunksize=5_000
        )
    print(f"‚úÖ Directorio inicial cargado: {len(df):,} filas.")

if __name__ == "__main__":
    main()



---
# importar_fletes.py


import os
import glob
import unicodedata
import pandas as pd
from datetime import datetime
from sqlalchemy import text
from sqlalchemy.types import Text, Integer, Numeric, TIMESTAMP, Date
from conexion import engine, DB_NAME

# -----------------------------------------------------------------------------
# Funci√≥n para remover acentos y diacr√≠ticos de una cadena
# -----------------------------------------------------------------------------
def remover_acentos(cadena):
    if not isinstance(cadena, str):
        return cadena
    nfkd_form = unicodedata.normalize('NFD', cadena)
    return nfkd_form.encode('ASCII', 'ignore').decode('utf-8', 'ignore')


# -----------------------------------------------------------------------------
# 2. Rutas de archivos
# -----------------------------------------------------------------------------
ruta_base = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/"
archivo_fletes = glob.glob(os.path.join(ruta_base, "*FLETES-NACIONAL-ANUAL*.CSV"))[0]
archivo_eliminar = os.path.join(ruta_base, "eliminar_fletes.xlsx")
archivo_corregidos = os.path.join(ruta_base, "fletes_corregidos.csv")

# -----------------------------------------------------------------------------
# 3. Lectura y normalizaci√≥n del archivo principal de fletes
# -----------------------------------------------------------------------------
fletes_df = pd.read_csv(archivo_fletes, dtype=str, encoding="utf-8")

# Normalizar nombres de columnas (quitar acentos, min√∫sculas, y "_" en lugar de espacios)
nuevos_nombres = []
for col in fletes_df.columns:
    col_sin_acentos = remover_acentos(col).lower().strip()
    col_sin_acentos = col_sin_acentos.replace(" ", "_")
    nuevos_nombres.append(col_sin_acentos)
fletes_df.columns = nuevos_nombres

# -----------------------------------------------------------------------------
# 4. Eliminar registros seg√∫n "eliminar_fletes.xlsx" (si existe)
# -----------------------------------------------------------------------------
if os.path.exists(archivo_eliminar):
    eliminar_df = pd.read_excel(archivo_eliminar, dtype=str)
    cols_eliminar = [remover_acentos(col).lower().strip().replace(" ", "_") for col in eliminar_df.columns]
    eliminar_df.columns = cols_eliminar

    if "folio_del_flete" in eliminar_df.columns and "folio_del_flete" in fletes_df.columns:
        antes_eliminar = len(fletes_df)
        fletes_df = fletes_df[~fletes_df["folio_del_flete"].isin(eliminar_df["folio_del_flete"])]
        despues_eliminar = len(fletes_df)
        print(f"‚úÖ Se eliminaron {antes_eliminar - despues_eliminar} registros basados en 'eliminar_fletes.xlsx'.")
    else:
        print("‚ö†Ô∏è No se encontr√≥ la columna 'folio_del_flete' en alguno de los archivos para eliminar.")
else:
    print("‚ö†Ô∏è No se encontr√≥ el archivo 'eliminar_fletes.xlsx'. No se eliminaron registros.")

# -----------------------------------------------------------------------------
# 5. A√±adir registros corregidos (si existe) con la misma limpieza
# -----------------------------------------------------------------------------
if os.path.exists(archivo_corregidos):
    corregidos_df = pd.read_csv(archivo_corregidos, dtype=str, encoding="utf-8")
    nombres_corregidos = [remover_acentos(col).lower().strip().replace(" ", "_") for col in corregidos_df.columns]
    corregidos_df.columns = nombres_corregidos

    # üõ†Ô∏è Correcci√≥n de fechas DD/MM/YY HH:MM en fletes_corregidos.csv
    def convertir_fecha_custom(fecha_str):
        try:
            if pd.isna(fecha_str) or "0000" in fecha_str:
                return None
            return pd.to_datetime(fecha_str, format="%d/%m/%y %H:%M", errors="coerce")
        except:
            return None

    for col in ["fecha_de_salida", "fecha_de_llegada", "fecha_de_entrega"]:
        if col in corregidos_df.columns:
            corregidos_df[col] = corregidos_df[col].apply(convertir_fecha_custom)

    if "fecha_de_salida" in corregidos_df.columns:
        corregidos_df["fecha_de_salida"] = corregidos_df["fecha_de_salida"].dt.date

    antes_corregidos = len(fletes_df)
    fletes_df = pd.concat([fletes_df, corregidos_df], ignore_index=True)
    despues_corregidos = len(fletes_df)
    print(f"‚úÖ Se a√±adieron {despues_corregidos - antes_corregidos} registros corregidos.")
else:
    print("‚ö†Ô∏è No se encontr√≥ el archivo 'fletes_corregidos.csv'. Se omiti√≥ su carga.")

# -----------------------------------------------------------------------------
# 6. Transformaciones de fechas y tipos (para el DF unificado)
# -----------------------------------------------------------------------------
if "fecha_de_salida" in fletes_df.columns:
    fletes_df["fecha_de_salida"] = pd.to_datetime(
        fletes_df["fecha_de_salida"],
        format="%Y-%m-%d %H:%M:%S",
        errors="coerce"
    ).dt.date

if "fecha_de_llegada" in fletes_df.columns:
    fletes_df["fecha_de_llegada"] = pd.to_datetime(
        fletes_df["fecha_de_llegada"],
        format="%Y-%m-%d %H:%M:%S",
        errors="coerce"
    )

if "estatus" in fletes_df.columns and "fecha_de_entrega" in fletes_df.columns:
    mask_autorizado = fletes_df["estatus"].str.lower() == "autorizado"
    fletes_df.loc[mask_autorizado, "fecha_de_entrega"] = None

if "fecha_de_entrega" in fletes_df.columns:
    fletes_df["fecha_de_entrega"] = pd.to_datetime(
        fletes_df["fecha_de_entrega"],
        format="%Y-%m-%d %H:%M:%S",
        errors="coerce"
    )

# -----------------------------------------------------------------------------
# 7. Conversi√≥n de columnas num√©ricas y texto
# -----------------------------------------------------------------------------
numeric_cols_3_decimals = ["toneladas_iniciales", "toneladas_en_el_destino", "toneladas_con_incidentes"]
for col in numeric_cols_3_decimals:
    if col in fletes_df.columns:
        fletes_df[col] = pd.to_numeric(fletes_df[col], errors="coerce").round(3)

if "ticket_bascula" in fletes_df.columns:
    fletes_df["ticket_bascula"] = fletes_df["ticket_bascula"].astype(str)

if "telefono_operador" in fletes_df.columns:
    fletes_df["telefono_operador"] = fletes_df["telefono_operador"].astype(str)

# -----------------------------------------------------------------------------
# 8. Crear/Asignar 'id_ceda_agricultura' seg√∫n estatus
# -----------------------------------------------------------------------------
if "id_ceda_agricultura" not in fletes_df.columns:
    fletes_df["id_ceda_agricultura"] = None

if "estatus" in fletes_df.columns:
    if "cdf_destino_final" in fletes_df.columns:
        mask_entregado = fletes_df["estatus"].str.lower() == "entregado"
        fletes_df.loc[mask_entregado, "id_ceda_agricultura"] = fletes_df["cdf_destino_final"]
    if "cdf_destino_original" in fletes_df.columns:
        mask_autorizado = fletes_df["estatus"].str.lower() == "autorizado"
        fletes_df.loc[mask_autorizado, "id_ceda_agricultura"] = fletes_df["cdf_destino_original"]

# -----------------------------------------------------------------------------
# 9. Verificaci√≥n r√°pida
# -----------------------------------------------------------------------------
print("\nEjemplo de algunos registros post-limpieza:")
cols_mostrar = [
    "folio_del_flete","estatus","fecha_de_salida",
    "fecha_de_llegada","fecha_de_entrega","cdf_destino_original",
    "cdf_destino_final","id_ceda_agricultura"
]
existen = [c for c in cols_mostrar if c in fletes_df.columns]
print(fletes_df[existen].head(5))

print("\nConteo de valores nulos en columnas de fecha (si existen):")
for fecha_col in ["fecha_de_salida", "fecha_de_llegada", "fecha_de_entrega"]:
    if fecha_col in fletes_df.columns:
        print(f"  {fecha_col}: {fletes_df[fecha_col].isnull().sum()}")

archivo_limpio = os.path.join(ruta_base, "fletes_limpios_para_importar.csv")
fletes_df.to_csv(archivo_limpio, index=False, encoding="utf-8")
print(f"\nüöÄ CSV temporal guardado en: {archivo_limpio}")

# -----------------------------------------------------------------------------
# 10. Definir correspondencia de columnas a tipos SQLAlchemy
# -----------------------------------------------------------------------------
dtype_sqlalchemy = {
    "folio_del_flete": Text,
    "estatus": Text,
    "producto": Text,
    "abreviacion_producto": Text,
    "toneladas_iniciales": Numeric(10, 3),
    "bultos_iniciales": Integer,
    "bultos_en_destino": Integer,
    "ticket_bascula": Text,
    "estado_procedencia": Text,
    "toneladas_en_el_destino": Numeric(10, 3),
    "fecha_de_salida": Date,
    "cdf_destino_original": Text,
    "cdf_destino_final": Text,
    "fecha_de_llegada": TIMESTAMP,
    "fecha_de_entrega": TIMESTAMP,
    "toneladas_con_incidentes": Numeric(10, 3),
    "estatus_de_recepcion_incidente": Text,
    "descripcion": Text,
    "destino_final": Text,
    "nombre_operador": Text,
    "telefono_operador": Text,
    "placas_transporte": Text,
    "tipo_transporte": Text,
    "estado_llegada": Text,
    "bultos_por_anio": Integer,
    "id_ceda_agricultura": Text
}

# -----------------------------------------------------------------------------
# 11. Vaciar tabla 'fletes' y reinsertar (sustituir)
# -----------------------------------------------------------------------------
table_name = "fletes"

print(f"\nüóë Vaciar la tabla '{table_name}'...")
with engine.connect() as conn:
    conn.execute(text(f"TRUNCATE TABLE {table_name};"))
    conn.commit()

print(f"‚¨ÜÔ∏è Insertando {len(fletes_df)} registros en la tabla '{table_name}'...")
fletes_df.to_sql(
    name=table_name,
    con=engine,
    if_exists="append",
    index=False,
    dtype=dtype_sqlalchemy
)

print(f"\n‚úÖ Se han sustituido los datos antiguos por los nuevos en la tabla '{table_name}' de la base '{DB_NAME}'.")



---
# importar_fletes_old.py

import os
import glob
import unicodedata
import pandas as pd
from sqlalchemy import text
from sqlalchemy.types import Text, Integer, Numeric, TIMESTAMP, Date
from conexion import engine, DB_NAME

# -----------------------------------------------------------------------------
# Funci√≥n para remover acentos y diacr√≠ticos de una cadena
# -----------------------------------------------------------------------------
def remover_acentos(cadena):
    if not isinstance(cadena, str):
        return cadena
    nfkd_form = unicodedata.normalize('NFD', cadena)
    return nfkd_form.encode('ASCII', 'ignore').decode('utf-8', 'ignore')


# -----------------------------------------------------------------------------
# 2. Rutas de archivos
# -----------------------------------------------------------------------------
ruta_base = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/"
archivo_fletes = glob.glob(os.path.join(ruta_base, "*FLETES-NACIONAL-ANUAL*.csv"))[0]
archivo_eliminar = os.path.join(ruta_base, "eliminar_fletes.xlsx")
archivo_corregidos = os.path.join(ruta_base, "fletes_corregidos.csv")

# -----------------------------------------------------------------------------
# 3. Lectura y normalizaci√≥n del archivo principal de fletes
# -----------------------------------------------------------------------------
fletes_df = pd.read_csv(archivo_fletes, dtype=str, encoding="utf-8")

# Normalizar nombres de columnas (quitar acentos, min√∫sculas, y "_" en lugar de espacios)
nuevos_nombres = []
for col in fletes_df.columns:
    col_sin_acentos = remover_acentos(col).lower().strip()
    col_sin_acentos = col_sin_acentos.replace(" ", "_")
    nuevos_nombres.append(col_sin_acentos)
fletes_df.columns = nuevos_nombres

# -----------------------------------------------------------------------------
# 4. Eliminar registros seg√∫n "eliminar_fletes.xlsx" (si existe)
# -----------------------------------------------------------------------------
if os.path.exists(archivo_eliminar):
    eliminar_df = pd.read_excel(archivo_eliminar, dtype=str)
    # Normalizar columnas en 'eliminar_df'
    cols_eliminar = []
    for col in eliminar_df.columns:
        col_norm = remover_acentos(col).lower().strip().replace(" ", "_")
        cols_eliminar.append(col_norm)
    eliminar_df.columns = cols_eliminar
    
    if "folio_del_flete" in eliminar_df.columns and "folio_del_flete" in fletes_df.columns:
        antes_eliminar = len(fletes_df)
        fletes_df = fletes_df[~fletes_df["folio_del_flete"].isin(eliminar_df["folio_del_flete"])]
        despues_eliminar = len(fletes_df)
        print(f"‚úÖ Se eliminaron {antes_eliminar - despues_eliminar} registros basados en 'eliminar_fletes.xlsx'.")
    else:
        print("‚ö†Ô∏è No se encontr√≥ la columna 'folio_del_flete' en alguno de los archivos para eliminar.")
else:
    print("‚ö†Ô∏è No se encontr√≥ el archivo 'eliminar_fletes.xlsx'. No se eliminaron registros.")

# -----------------------------------------------------------------------------
# 5. A√±adir registros corregidos (si existe) con la misma limpieza
# -----------------------------------------------------------------------------
if os.path.exists(archivo_corregidos):
    corregidos_df = pd.read_csv(archivo_corregidos, dtype=str, encoding="utf-8")
    # Normalizar columnas en 'corregidos_df'
    nombres_corregidos = []
    for col in corregidos_df.columns:
        col_sin_acentos = remover_acentos(col).lower().strip()
        col_sin_acentos = col_sin_acentos.replace(" ", "_")
        nombres_corregidos.append(col_sin_acentos)
    corregidos_df.columns = nombres_corregidos
    
    antes_corregidos = len(fletes_df)
    # Concatenar ambos DF (principal + corregidos)
    fletes_df = pd.concat([fletes_df, corregidos_df], ignore_index=True)
    despues_corregidos = len(fletes_df)
    print(f"‚úÖ Se a√±adieron {despues_corregidos - antes_corregidos} registros corregidos.")
else:
    print("‚ö†Ô∏è No se encontr√≥ el archivo 'fletes_corregidos.csv'. Se omiti√≥ su carga.")

# -----------------------------------------------------------------------------
# 6. Transformaciones de fechas y tipos (para el DF unificado)
#    Seg√∫n tu verificaci√≥n, el CSV usa formato 'YYYY-MM-DD HH:MM:SS'
#    - fecha_de_salida => date
#    - fecha_de_llegada => timestamp
#    - fecha_de_entrega => timestamp (si 'estatus' = 'autorizado' => NaT)
# -----------------------------------------------------------------------------
if "fecha_de_salida" in fletes_df.columns:
    fletes_df["fecha_de_salida"] = pd.to_datetime(
        fletes_df["fecha_de_salida"],
        format="%Y-%m-%d %H:%M:%S",
        errors="coerce"
    ).dt.date

if "fecha_de_llegada" in fletes_df.columns:
    fletes_df["fecha_de_llegada"] = pd.to_datetime(
        fletes_df["fecha_de_llegada"],
        format="%Y-%m-%d %H:%M:%S",
        errors="coerce"
    )

if "estatus" in fletes_df.columns and "fecha_de_entrega" in fletes_df.columns:
    mask_autorizado = fletes_df["estatus"].str.lower() == "autorizado"
    fletes_df.loc[mask_autorizado, "fecha_de_entrega"] = None

if "fecha_de_entrega" in fletes_df.columns:
    fletes_df["fecha_de_entrega"] = pd.to_datetime(
        fletes_df["fecha_de_entrega"],
        format="%Y-%m-%d %H:%M:%S",
        errors="coerce"
    )

# -----------------------------------------------------------------------------
# 7. Conversi√≥n de columnas num√©ricas y texto
# -----------------------------------------------------------------------------
numeric_cols_3_decimals = ["toneladas_iniciales", "toneladas_en_el_destino", "toneladas_con_incidentes"]
for col in numeric_cols_3_decimals:
    if col in fletes_df.columns:
        fletes_df[col] = pd.to_numeric(fletes_df[col], errors="coerce").round(3)

if "ticket_bascula" in fletes_df.columns:
    fletes_df["ticket_bascula"] = fletes_df["ticket_bascula"].astype(str)

if "telefono_operador" in fletes_df.columns:
    fletes_df["telefono_operador"] = fletes_df["telefono_operador"].astype(str)

# -----------------------------------------------------------------------------
# 8. Crear/Asignar 'id_ceda_agricultura' seg√∫n estatus
# -----------------------------------------------------------------------------
if "id_ceda_agricultura" not in fletes_df.columns:
    fletes_df["id_ceda_agricultura"] = None

if "estatus" in fletes_df.columns:
    # ENTREGADO => cdf_destino_final
    if "cdf_destino_final" in fletes_df.columns:
        mask_entregado = fletes_df["estatus"].str.lower() == "entregado"
        fletes_df.loc[mask_entregado, "id_ceda_agricultura"] = fletes_df["cdf_destino_final"]
    # AUTORIZADO => cdf_destino_original
    if "cdf_destino_original" in fletes_df.columns:
        mask_autorizado = fletes_df["estatus"].str.lower() == "autorizado"
        fletes_df.loc[mask_autorizado, "id_ceda_agricultura"] = fletes_df["cdf_destino_original"]

# -----------------------------------------------------------------------------
# 9. Verificaci√≥n r√°pida
# -----------------------------------------------------------------------------
print("\nEjemplo de algunos registros post-limpieza:")
cols_mostrar = [
    "folio_del_flete","estatus","fecha_de_salida",
    "fecha_de_llegada","fecha_de_entrega","cdf_destino_original",
    "cdf_destino_final","id_ceda_agricultura"
]
existen = [c for c in cols_mostrar if c in fletes_df.columns]
print(fletes_df[existen].head(5))

print("\nConteo de valores nulos en columnas de fecha (si existen):")
for fecha_col in ["fecha_de_salida", "fecha_de_llegada", "fecha_de_entrega"]:
    if fecha_col in fletes_df.columns:
        print(f"  {fecha_col}: {fletes_df[fecha_col].isnull().sum()}")

# 9B. (Opcional) Guardar CSV temporal para revisi√≥n
archivo_limpio = os.path.join(ruta_base, "fletes_limpios_para_importar.csv")
fletes_df.to_csv(archivo_limpio, index=False, encoding="utf-8")
print(f"\nüöÄ CSV temporal guardado en: {archivo_limpio}")

# -----------------------------------------------------------------------------
# 10. Definir correspondencia de columnas a tipos SQLAlchemy
# -----------------------------------------------------------------------------
dtype_sqlalchemy = {
    "folio_del_flete": Text,
    "estatus": Text,
    "producto": Text,
    "abreviacion_producto": Text,  # normalizado
    "toneladas_iniciales": Numeric(10, 3),
    "bultos_iniciales": Integer,
    "bultos_en_destino": Integer,
    "ticket_bascula": Text,
    "estado_procedencia": Text,
    "toneladas_en_el_destino": Numeric(10, 3),
    "fecha_de_salida": Date,
    "cdf_destino_original": Text,
    "cdf_destino_final": Text,
    "fecha_de_llegada": TIMESTAMP,
    "fecha_de_entrega": TIMESTAMP,
    "toneladas_con_incidentes": Numeric(10, 3),
    "estatus_de_recepcion_incidente": Text,
    "descripcion": Text,
    "destino_final": Text,
    "nombre_operador": Text,
    "telefono_operador": Text,
    "placas_transporte": Text,
    "tipo_transporte": Text,
    "estado_llegada": Text,
    "bultos_por_anio": Integer,
    "id_ceda_agricultura": Text
}

# -----------------------------------------------------------------------------
# 11. Vaciar tabla 'fletes' y reinsertar (sustituir)
# -----------------------------------------------------------------------------
table_name = "fletes"

print(f"\nüóë Vaciar la tabla '{table_name}'...")
with engine.connect() as conn:
    conn.execute(text(f"TRUNCATE TABLE {table_name};"))
    conn.commit()

print(f"‚¨ÜÔ∏è Insertando {len(fletes_df)} registros en la tabla '{table_name}'...")
fletes_df.to_sql(
    name=table_name,
    con=engine,
    if_exists="append",  # Se inserta en la tabla vac√≠a
    index=False,
    dtype=dtype_sqlalchemy
)

print(f"\n‚úÖ Se han sustituido los datos antiguos por los nuevos en la tabla '{table_name}' de la base '{DB_NAME}'.")


---
# importar_full_derechohabientes_2025.py

import os
import pandas as pd
import glob
import time
from sqlalchemy import text
from conexion import engine, psycopg_conn, DB_NAME

# ‚è±Ô∏è Medir tiempo de ejecuci√≥n
inicio = time.time()

# üìÇ Ruta con archivos CSV
carpeta_csv = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/full_derechohabientes/"
archivos_csv = glob.glob(os.path.join(carpeta_csv, "*.CSV"))

# üìä Combinar todos los archivos en un solo DataFrame
df_full = pd.concat(
    (pd.read_csv(archivo, encoding="utf-8", dtype=str) for archivo in archivos_csv),
    ignore_index=True
)

# üõ†Ô∏è Paso 1: Renombrar columnas espec√≠ficas ANTES de normalizar

# üîÑ Solo renombrar si la columna existe exactamente como "Estatus Solicitud"
if "Estatus Solicitud" in df_full.columns:
    df_full = df_full.rename(columns={"Estatus Solicitud": "estatus_solicitud_pago"})

# üîÑ Tambi√©n aplicar renombramientos seguros para columnas con may√∫sculas iniciales
renombramientos_especiales = {
    "dap_toneladas": "dap_entregada",
    "urea_toneladas": "urea_entregada"
}

# Buscar nombres que coincidan ignorando may√∫sculas y espacios
nuevos_nombres = {}
for col in df_full.columns:
    col_limpio = col.strip().lower().replace(" ", "_")
    if col_limpio in renombramientos_especiales:
        nuevos_nombres[col] = renombramientos_especiales[col_limpio]

df_full = df_full.rename(columns=nuevos_nombres)

# üßº Paso 2: Normalizar todos los nombres
df_full.columns = (
    pd.Series(df_full.columns)
    .str.strip()
    .str.lower()
    .str.replace(" ", "_")
    .str.replace("√°", "a")
    .str.replace("√©", "e")
    .str.replace("√≠", "i")
    .str.replace("√≥", "o")
    .str.replace("√∫", "u")
    .str.replace("√±", "n")
    .str.replace(r"[^a-z0-9_]", "", regex=True)
)

# üß™ Paso 2.5: Duplicados por acuse_estatal
if "acuse_estatal" in df_full.columns:
    df_full["__orden_original__"] = df_full.index
    duplicados = df_full[df_full.duplicated("acuse_estatal", keep="first")].copy()

    ruta_duplicados = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/U_TEMPORAL/duplicados_full_derechohabientes.csv"
    duplicados.to_csv(ruta_duplicados, index=False, encoding="utf-8")
    print(f"‚ö†Ô∏è Se encontraron {len(duplicados)} registros duplicados por 'acuse_estatal'. Se guardaron en:")
    print(f"   {ruta_duplicados}")

    df_full = df_full.sort_values("__orden_original__").drop(columns=["__orden_original__"])
    df_full = df_full.drop_duplicates("acuse_estatal", keep="first")
else:
    print("‚ùå ERROR: No se encontr√≥ la columna 'acuse_estatal'. No se puede detectar duplicados.")

# üßÆ Paso 3: Corregir columnas con enteros declarados como decimales
columnas_a_convertir = [
    "dap_25_kg_anio_actual",
    "urea_25_kg_anio_actual",
    "dap_25_kg_remanente",
    "urea_25_kg_remanente",
    "superficie_apoyada"
]

for col in columnas_a_convertir:
    if col in df_full.columns:
        df_full[col] = pd.to_numeric(df_full[col], errors="coerce").fillna(0).astype("Int64")

# üóÉÔ∏è Obtener columnas de PostgreSQL
with engine.connect() as conn:
    result = conn.execute(text("""
        SELECT column_name
        FROM information_schema.columns
        WHERE table_name = 'full_derechohabientes_2025'
        ORDER BY ordinal_position
    """))
    columnas_pg = [row[0] for row in result.fetchall()]

# üßΩ Reordenar columnas seg√∫n la base
faltantes = [col for col in columnas_pg if col not in df_full.columns]
if faltantes:
    print(f"‚ùå Las siguientes columnas no est√°n en el DataFrame: {faltantes}")
    raise Exception("üö´ No se puede continuar. Faltan columnas requeridas.")
df_full = df_full[columnas_pg]

# üßæ Convertir fecha_entrega si existe
if "fecha_entrega" in df_full.columns:
    df_full["fecha_entrega"] = pd.to_datetime(df_full["fecha_entrega"], errors="coerce").dt.strftime("%Y-%m-%d")

# üíæ Exportar archivo temporal
ruta_temp = "/tmp/full_derechohabientes_2025_temp.csv"
df_full.to_csv(ruta_temp, index=False, header=False, encoding="utf-8", na_rep='')

# üöÄ Ejecutar COPY
try:
    with psycopg_conn, psycopg_conn.cursor() as cur:
        print("üóëÔ∏è Truncando tabla full_derechohabientes_2025...")
        cur.execute("TRUNCATE TABLE full_derechohabientes_2025;")
        print("üì• Importando datos con COPY...")
        with open(ruta_temp, "r", encoding="utf-8") as f:
            cur.copy_expert(
                f"COPY full_derechohabientes_2025 ({', '.join(columnas_pg)}) FROM STDIN WITH CSV",
                f
            )
    print(f"‚úÖ Importaci√≥n completada en la tabla 'full_derechohabientes_2025' ({len(df_full)} registros).")
except Exception as e:
    print(f"‚ùå Error durante la importaci√≥n con COPY: {e}")

# ‚è±Ô∏è Fin del proceso
fin = time.time()
print(f"‚è±Ô∏è Tiempo total de ejecuci√≥n: {round(fin - inicio, 2)} segundos.")


---
# importar_incidencias.py

import pandas as pd
from sqlalchemy import text
import os
import glob
import unicodedata
import re
from dateutil import parser
from conexion import engine, DB_NAME

# Ruta base de los archivos CSV
ruta_base = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/"

# Buscar archivo con la m√°scara "*-INCIDENTES-NACIONAL-ANUAL_*"
file_list = glob.glob(os.path.join(ruta_base, "*INCIDENTES-NACIONAL-ANUAL_*"))
if not file_list:
    print("‚ùå No se encontr√≥ ning√∫n archivo con la m√°scara '*INCIDENTES-NACIONAL-ANUAL*'")
    exit()

file_path = file_list[0]
print(f"‚ÑπÔ∏è Se utilizar√° el archivo: {file_path}")

# -----------------------------------------------------------------------------
# Funci√≥n para normalizar nombres de columnas
# -----------------------------------------------------------------------------
def normalizar_columna(col):
    col = col.strip().lower()
    col = ''.join(
        c for c in unicodedata.normalize('NFD', col)
        if unicodedata.category(c) != 'Mn'
    )
    col = re.sub(r'[^a-z0-9]+', '_', col)
    col = re.sub(r'[_]+', '_', col).strip('_')
    return col

# 1Ô∏è‚É£ Leer CSV como cadenas
df_incidencias = pd.read_csv(file_path, dtype=str, encoding='utf-8', delimiter=',')

# 2Ô∏è‚É£ Normalizar nombres de columnas
df_incidencias.columns = [normalizar_columna(c) for c in df_incidencias.columns]

# 3Ô∏è‚É£ Reemplazar valores inv√°lidos con None
df_incidencias.replace(["N/A", "NA", "n/a", "-"], None, inplace=True)

# 4Ô∏è‚É£ Convertir a num√©rico la columna 'ton_incidencia' (si existe)
if "ton_incidencia" in df_incidencias.columns:
    df_incidencias["ton_incidencia"] = pd.to_numeric(
        df_incidencias["ton_incidencia"], 
        errors="coerce"
    ).round(3)

# 5Ô∏è‚É£ Convertir 'fecha_incidente' a formato fecha
if "fecha_incidente" in df_incidencias.columns:
    def parse_fecha(valor):
        if pd.notna(valor):
            return parser.parse(valor, dayfirst=True).date()
        return None
    df_incidencias["fecha_incidente"] = df_incidencias["fecha_incidente"].apply(parse_fecha)

# 6Ô∏è‚É£ Vista previa
print("\nüéØ Vista previa del DataFrame:")
print(df_incidencias.head(10))

# 7Ô∏è‚É£ Importar datos
try:
    with engine.begin() as conn:
        print("üßπ Eliminando registros anteriores de la tabla 'incidentes'...")
        conn.execute(text("DELETE FROM incidentes;"))

        print("‚¨ÜÔ∏è Insertando nuevos registros...")
        df_incidencias.to_sql("incidentes", conn, if_exists="append", index=False)

    print(f"‚úÖ Se han sustituido los datos antiguos por los nuevos en la tabla 'incidentes' de la base '{DB_NAME}'.")
except Exception as e:
    print(f"‚ùå Error al importar los datos: {e}")


---
# importar_inventarios_campo_2025.py

#!/usr/bin/env python3
import pandas as pd
import hashlib
import datetime
import re
import unicodedata
import socket
import traceback
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
from sqlalchemy import text
from conexion import engine  # tu conexi√≥n centralizada


# --- Configuraci√≥n ------------------------------------------------------
CREDS_FILE     = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/SCRIPTS/inventarios-2025-pf-69b1417ea0df.json"
SCOPES         = ["https://www.googleapis.com/auth/spreadsheets.readonly"]
SPREADSHEET_ID = "1YLmnolQ4TPab8LcVcAHTTqgYECddMfNAJGqohvBPZLM"
RANGE          = "'Respuestas de formulario 1'!A1:AS"
TABLE_NAME     = "inventarios_diarios_ceda_2025_campo"
LOG_FILE       = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/error_inventarios.txt"


def normalize_columns(cols):
    new_cols = []
    for col in cols:
        c = col.lower().strip()
        c = unicodedata.normalize('NFD', c).encode('ascii', 'ignore').decode('utf-8')
        c = re.sub(r"[^\w\s]", '', c)
        c = re.sub(r"\s+", '_', c)
        c = re.sub(r"_+", '_', c)
        new_cols.append(c)
    return new_cols

def main():
    socket.setdefaulttimeout(120)
    creds = Credentials.from_service_account_file(CREDS_FILE, scopes=SCOPES)
    service = build("sheets", "v4", credentials=creds, cache_discovery=False)

    # 2) Obtener datos brutos
    resp = service.spreadsheets().values().get(
        spreadsheetId=SPREADSHEET_ID,
        range=RANGE
    ).execute()
    values = resp.get("values", [])
    if not values:
        print("‚ö†Ô∏è No hay datos para importar.")
        return

    # 3) Encabezados y filas
    headers, rows = values[0], values[1:]
    n_cols = len(headers)
    padded_rows = [row + [''] * (n_cols - len(row)) for row in rows]
    df = pd.DataFrame(padded_rows, columns=headers)

    # 4) Normalizar nombres de columnas
    df.columns = normalize_columns(df.columns)

    # 5) Consolidar columnas de selecci√≥n de CEDA
    centro_cols = [col for col in df.columns if col.startswith('selecciona_tu_centro_de_distribucion')]
    df['id_ceda_agricultura'] = df[centro_cols].replace('', pd.NA).bfill(axis=1).iloc[:, 0]
    df.drop(columns=centro_cols, inplace=True)

    # 6) Renombrar campos fijos y nuevos
    rename_map = {
        'marca_temporal': 'fecha_y_hora',
        'direccion_de_correo_electronico': 'correo_electronico',
        'selecciona_la_fecha_del_dia_a_registrar': 'fecha_registro',
        'selecciona_tu_estado': 'estado',
        'quien_o_quienes_fueron_los_responsables_de_realizar_el_levantamiento_del_inventario': 'quien_reporta',
        'nombre_del_coz': 'nombre_de_coz',
        'nombre_del_rc': 'nombre_del_rc',
        'nombre_del_ce': 'nombre_del_ce',
        'numero_de_bultos_de_dap_en_el_centro_de_distribucion': 'bultos_dap',
        'numero_de_bultos_de_urea_en_el_centro_de_distribucion': 'bultos_urea',
        'observaciones': 'observaciones',
        'nombre_de_la_persona_que_realizo_el_levantamiento': 'otra_persona'
    }
    df.rename(columns=rename_map, inplace=True)

    # 7) Convertir tipos de bultos
    if 'bultos_dap' in df.columns:
        df['bultos_dap'] = pd.to_numeric(df['bultos_dap'], errors='coerce').fillna(0).astype(int)
    if 'bultos_urea' in df.columns:
        df['bultos_urea'] = pd.to_numeric(df['bultos_urea'], errors='coerce').fillna(0).astype(int)

    # 8) Convertir fechas (con dayfirst para evitar error con formato dd/mm/yyyy)
    if 'fecha_y_hora' in df.columns:
        df['fecha_y_hora'] = pd.to_datetime(df['fecha_y_hora'], errors='coerce', dayfirst=True)
    if 'fecha_registro' in df.columns:
        df['fecha_registro'] = pd.to_datetime(df['fecha_registro'], errors='coerce', dayfirst=True).dt.date

    # 9) Seleccionar columnas finales
    cols_to_keep = [
        'fecha_y_hora', 'correo_electronico', 'fecha_registro', 'estado',
        'id_ceda_agricultura', 'bultos_dap', 'bultos_urea',
        'quien_reporta', 'nombre_de_coz', 'nombre_del_rc', 'nombre_del_ce',
        'observaciones', 'otra_persona'
    ]
    missing = [c for c in cols_to_keep if c not in df.columns]
    if missing:
        print(f"‚ö†Ô∏è Columnas faltantes tras procesamiento: {missing}")
    df_final = df[cols_to_keep]

    # ‚ùó Eliminar registros sin fecha obligatoria para evitar error NOT NULL
    df_final = df_final[df_final['fecha_y_hora'].notnull()]
    if df_final.empty:
        print("‚ö†Ô∏è Todos los registros tienen fecha_y_hora vac√≠a. No se insertar√° nada.")
        return

    # üîÑ Truncar la tabla e insertar todo el DataFrame completo
    with engine.begin() as conn:
        conn.execute(text(f"TRUNCATE TABLE {TABLE_NAME};"))
        df_final.to_sql(
            name=TABLE_NAME,
            con=conn,
            if_exists='append',
            index=False,
            method='multi',
            chunksize=5000
        )

    # 13) Log de ejecuci√≥n
    ahora = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"‚úÖ Tabla '{TABLE_NAME}' truncada e insertadas {len(df_final)} filas nuevas ‚Äî {ahora}")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        with open(LOG_FILE, "a", encoding="utf-8") as f:
            f.write(f"\n--- Error ejecutando importar_inventarios_campo_2025.py ---\n")
            f.write(f"{datetime.datetime.now()}\n")
            traceback.print_exc(file=f)
        print(f"‚ùå Se produjo un error. Revisa el archivo de log: {LOG_FILE}")


---
# importar_inventarios_campo_2025_new.py

#!/usr/bin/env python3
import pandas as pd
import hashlib
import datetime
import re
import unicodedata
from io import StringIO
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
from sqlalchemy import text
from conexion import engine, psycopg_conn

import traceback
import sys
import os

# --- Configuraci√≥n ------------------------------------------------------
CREDS_FILE     = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/SCRIPTS/inventarios-2025-pf-69b1417ea0df.json"
SCOPES         = ["https://www.googleapis.com/auth/spreadsheets.readonly"]
SPREADSHEET_ID = "1YLmnolQ4TPab8LcVcAHTTqgYECddMfNAJGqohvBPZLM"
RANGE          = "'Respuestas de formulario 1'!A1:AT"
TABLE_NAME     = "inventarios_diarios_ceda_2025_campo"
LOG_FILE       = "error_inventarios.txt"

# -----------------------------------------------------------------------
def normalize_columns(cols):
    new_cols = []
    for col in cols:
        c = col.lower().strip()
        c = unicodedata.normalize("NFD", c).encode("ascii", "ignore").decode("utf-8")
        c = re.sub(r"[^\w\s]", "", c)
        c = re.sub(r"\s+", "_", c)
        c = re.sub(r"_+", "_", c)
        new_cols.append(c)
    return new_cols

# -----------------------------------------------------------------------
def main():
    # 1) autenticaci√≥n Google Sheets
    creds   = Credentials.from_service_account_file(CREDS_FILE, scopes=SCOPES)
    service = build("sheets", "v4", credentials=creds)

    resp = service.spreadsheets().values().get(
        spreadsheetId=SPREADSHEET_ID,
        range=RANGE
    ).execute()
    values = resp.get("values", [])
    if not values:
        print("‚ö†Ô∏è  No hay datos para importar.")
        return

    headers, rows = values[0], values[1:]
    n_cols = len(headers)
    padded = [row + [""] * (n_cols - len(row)) for row in rows]
    df = pd.DataFrame(padded, columns=headers)

    # 2) normalizar headers
    df.columns = normalize_columns(df.columns)

    # 3) consolidar centro de distribuci√≥n
    centro_cols = [c for c in df.columns if c.startswith("selecciona_tu_centro_de_distribucion")]
    df["id_ceda_agricultura"] = (
        df[centro_cols].replace("", pd.NA).bfill(axis=1).iloc[:, 0]
    )
    df.drop(columns=centro_cols, inplace=True)

    # 4) renombrar columnas
    rename_map = {
        "marca_temporal": "fecha_y_hora",
        "direccion_de_correo_electronico": "correo_electronico",
        "selecciona_la_fecha_del_dia_a_registrar": "fecha_registro",
        "selecciona_tu_estado": "estado",
        "quien_o_quienes_fueron_los_responsables_de_realizar_el_levantamiento_del_inventario": "quien_reporta",
        "nombre_del_coz": "nombre_de_coz",
        "nombre_del_rc": "nombre_del_rc",
        "nombre_del_ce": "nombre_del_ce",
        "numero_de_bultos_de_dap_en_el_centro_de_distribucion": "bultos_dap",
        "numero_de_bultos_de_urea_en_el_centro_de_distribucion": "bultos_urea",
        "observaciones": "observaciones",
        "nombre_de_la_persona_que_realizo_el_levantamiento": "otra_persona",
        "nombre_de_responsable_en_direccion_general": "personal_direccion",
    }
    df.rename(columns=rename_map, inplace=True)

    # 5) convertir tipos
    if "bultos_dap" in df.columns:
        df["bultos_dap"] = pd.to_numeric(df["bultos_dap"], errors="coerce").fillna(0).astype(int)
    if "bultos_urea" in df.columns:
        df["bultos_urea"] = pd.to_numeric(df["bultos_urea"], errors="coerce").fillna(0).astype(int)

    if "fecha_y_hora" in df.columns:
        df["fecha_y_hora"] = pd.to_datetime(df["fecha_y_hora"], errors="coerce", dayfirst=True)
    if "fecha_registro" in df.columns:
        df["fecha_registro"] = pd.to_datetime(df["fecha_registro"], errors="coerce", dayfirst=True).dt.date

    # 6) columnas finales
    keep = [
        "fecha_y_hora", "correo_electronico", "fecha_registro", "estado",
        "id_ceda_agricultura", "bultos_dap", "bultos_urea",
        "quien_reporta", "nombre_de_coz", "nombre_del_rc", "nombre_del_ce",
        "observaciones", "otra_persona", "personal_direccion",
    ]
    df_final = df[keep]

    # 7) eliminar registros sin fecha
    df_final = df_final[df_final["fecha_y_hora"].notnull()]
    if df_final.empty:
        print("‚ö†Ô∏è  Todos los registros tienen fecha_y_hora vac√≠a. No se insertar√° nada.")
        return

    # 8) clave CEDA en MAY√öSCULAS
    df_final["id_ceda_agricultura"] = df_final["id_ceda_agricultura"].astype(str).str.strip().str.upper()

    # 9) normalizaci√≥n ligera
    for col in df_final.columns:
        if col != "id_ceda_agricultura" and df_final[col].dtype == "object":
            df_final[col] = df_final[col].astype(str).str.strip().str.lower()

    # 10) hash de fila
    df_final["hash_respuesta"] = (
        df_final.astype(str).agg("|".join, axis=1)
        .apply(lambda x: hashlib.md5(x.encode()).hexdigest())
    )

    # 11) filtrar duplicados contra la BD
    with engine.connect() as conn:
        existentes = pd.read_sql(f"SELECT hash_respuesta FROM {TABLE_NAME}", conn)["hash_respuesta"]
    df_nuevos = df_final[~df_final["hash_respuesta"].isin(existentes)]

    # üí° Eliminar duplicados internos (mismo hash), conservar el √∫ltimo
    df_nuevos = df_nuevos.drop_duplicates(subset="hash_respuesta", keep="last")

    if df_nuevos.empty:
        print("‚ÑπÔ∏è  No hay registros nuevos para insertar.")
        return

    # 12) COPY FROM STDIN
    buf = StringIO()
    df_nuevos.to_csv(buf, sep="|", index=False, header=False, na_rep="")
    buf.seek(0)

    cols_pg = ", ".join(df_nuevos.columns)
    with psycopg_conn.cursor() as cur:
        cur.copy_expert(
            f"COPY {TABLE_NAME} ({cols_pg}) FROM STDIN WITH (FORMAT CSV, DELIMITER '|')",
            buf
        )
        psycopg_conn.commit()

    print(f"‚úÖ Insertadas {len(df_nuevos)} filas nuevas en '{TABLE_NAME}' ‚Äî {datetime.datetime.now():%Y-%m-%d %H:%M:%S}")

# -----------------------------------------------------------------------
if __name__ == "__main__":
    try:
        main()
    except Exception:
        with open(LOG_FILE, "w", encoding="utf-8") as f:
            f.write("‚ùå Se produjo un error en la ejecuci√≥n del script:\n\n")
            traceback.print_exc(file=f)
        print(f"‚ö†Ô∏è  Ocurri√≥ un error. Revisa el archivo: {LOG_FILE}")


---
# importar_padrones_2025.py

import os
import pandas as pd
import glob
import time
import unicodedata
import re
from sqlalchemy import text
from conexion import engine, psycopg_conn, DB_NAME

# ‚è±Ô∏è Inicio
inicio = time.time()

# üìÅ Rutas
base_path = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/derechohabientes_padrones"
csv_temp_path = os.path.join(base_path, "CSV_TEMPORAL")
os.makedirs(csv_temp_path, exist_ok=True)

archivo_duplicados = os.path.join(base_path, "duplicados.csv")
archivo_acuses_vacios = os.path.join(base_path, "acuses_estatal_vacios.csv")
csv_temp_carga = "/tmp/derechohabientes_padrones_2025_temp.csv"

# üìã Columnas esperadas (en orden exacto)
columnas_esperadas = [
    "acuse_estatal",
    "primer_apellido",
    "segundo_apellido",
    "nombre",
    "cultivo",
    "sexo",
    "estado_predio_inegi",
    "municipio_predio_inegi",
    "localidad_predio",
    "id_ceda",
    "dap_ton",
    "urea_ton",
    "superficie_apoyada",
    "publicacion__fecha"
]

# üßº Funci√≥n para limpiar nombres de columnas
def limpiar_columna(col):
    col = unicodedata.normalize('NFKD', col)
    col = ''.join(c for c in col if not unicodedata.combining(c))
    col = col.strip().lower().replace(" ", "_")
    col = re.sub(r"[^a-z0-9_]", "", col)
    return col

# üîÑ Paso 1: Convertir archivos .xlsx a CSV
excel_files = []
for root, dirs, files in os.walk(base_path):
    for file in files:
        if file.endswith(".xlsx") and not file.startswith("~$"):
            excel_files.append(os.path.join(root, file))

csv_generados = []
for archivo in excel_files:
    try:
        df = pd.read_excel(archivo, dtype=str)
        df.columns = [limpiar_columna(c) for c in df.columns]
        nombre_csv = os.path.splitext(os.path.basename(archivo))[0] + ".csv"
        ruta_csv = os.path.join(csv_temp_path, nombre_csv)
        df.to_csv(ruta_csv, index=False, encoding="utf-8")
        csv_generados.append(ruta_csv)
        print(f"‚úÖ Convertido: {archivo}")
    except Exception as e:
        print(f"‚ùå Error en {archivo}: {e}")

# üì• Paso 2: Leer todos los .csv generados
df_total = pd.concat(
    (pd.read_csv(archivo, dtype=str, encoding="utf-8") for archivo in csv_generados),
    ignore_index=True
)

# üßΩ Asegurar columnas esperadas y normalizadas
df_total.columns = df_total.columns.str.strip().str.lower()
df_total = df_total[[col for col in columnas_esperadas if col in df_total.columns]]

# üö® Identificar registros sin acuse_estatal
df_vacios = df_total[df_total["acuse_estatal"].isna() | (df_total["acuse_estatal"].str.strip() == "")]
if not df_vacios.empty:
    df_vacios.to_csv(archivo_acuses_vacios, index=False, encoding="utf-8")
    print(f"‚ö†Ô∏è {len(df_vacios)} registros eliminados por tener 'acuse_estatal' vac√≠o. Guardados en:\n{archivo_acuses_vacios}")
    df_total = df_total[~df_total.index.isin(df_vacios.index)]

# üîÅ Identificar y eliminar duplicados
df_total["__orden__"] = df_total.index
duplicados = df_total[df_total.duplicated("acuse_estatal", keep=False)].copy()
duplicados.drop(columns="__orden__").to_csv(archivo_duplicados, index=False, encoding="utf-8")
print(f"‚ö†Ô∏è {len(duplicados)} duplicados guardados en {archivo_duplicados}")

df_total = df_total.sort_values("__orden__").drop(columns="__orden__")
df_total = df_total.drop_duplicates("acuse_estatal", keep="last")

# üßæ Conversi√≥n de tipos
df_total["superficie_apoyada"] = pd.to_numeric(df_total["superficie_apoyada"], errors="coerce").fillna(0).astype("Int64")
df_total["publicacion__fecha"] = pd.to_datetime(df_total["publicacion__fecha"], errors="coerce").dt.strftime("%Y-%m-%d")

# üóÉÔ∏è Obtener columnas reales de la tabla
with engine.connect() as conn:
    result = conn.execute(text("""
        SELECT column_name
        FROM information_schema.columns
        WHERE table_name = 'derechohabientes_padrones_2025'
        ORDER BY ordinal_position
    """))
    columnas_pg = [row[0] for row in result.fetchall()]

# üßΩ Reordenar y asegurar estructura final
df_total = df_total[columnas_pg]

# üíæ Exportar CSV temporal
df_total.to_csv(csv_temp_carga, index=False, header=False, encoding="utf-8", na_rep='')

# üöÄ COPY a PostgreSQL
try:
    with psycopg_conn, psycopg_conn.cursor() as cur:
        print("üóëÔ∏è Truncando tabla derechohabientes_padrones_2025...")
        cur.execute("TRUNCATE TABLE derechohabientes_padrones_2025;")
        print("üì• Importando datos con COPY...")
        with open(csv_temp_carga, "r", encoding="utf-8") as f:
            cur.copy_expert(
                f"""
                COPY derechohabientes_padrones_2025 ({', '.join(columnas_pg)})
                FROM STDIN WITH CSV
                """, f
            )
    print(f"‚úÖ Se importaron {len(df_total)} registros a 'derechohabientes_padrones_2025'.")
except Exception as e:
    print(f"‚ùå Error durante la importaci√≥n con COPY: {e}")

# ‚è±Ô∏è Fin
fin = time.time()
print(f"‚è±Ô∏è Tiempo total de ejecuci√≥n: {round(fin - inicio, 2)} segundos.")



---
# importar_pedidos_desglosado.py

import pandas as pd
from sqlalchemy import text
import os
import glob
import unicodedata
import re
from conexion import engine, DB_NAME

# Ruta base
ruta_base = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/"
archivo_eliminar = os.path.join(ruta_base, "eliminar_pedidos_desglosado.csv")

# Buscar archivo con la m√°scara "*-PEDIDOS DESGLOSE-NACIONAL-ANUAL_*"
file_list = glob.glob(os.path.join(ruta_base, "*-DESGLOSE PEDIDOS MULTIFERTILIZANTES-NACIONAL-ANUAL*"))
if not file_list:
    print("‚ùå Error: No se encontr√≥ ning√∫n archivo que coincida con la m√°scara.")
    exit()
file_path = file_list[0]

# Cargar CSV principal
df_pedidos = pd.read_csv(file_path, encoding="utf-8", delimiter=",", header=0)

# Normalizar nombres de columnas
def normalizar_columna(col):
    col = col.strip().lower()
    col = ''.join(c for c in unicodedata.normalize('NFD', col) if unicodedata.category(c) != 'Mn')
    col = re.sub(r'[^a-z0-9]+', '_', col)
    col = re.sub(r'[_]+', '_', col).strip('_')
    return col

df_pedidos.columns = [normalizar_columna(c) for c in df_pedidos.columns]

# Reemplazar valores no v√°lidos
df_pedidos.replace(["N/A", "NA", "n/a", "-"], None, inplace=True)

# Convertir columnas num√©ricas
for col in ["dap", "urea"]:
    if col in df_pedidos.columns:
        df_pedidos[col] = pd.to_numeric(df_pedidos[col], errors="coerce").round(3)

#  ‚úÖ Convertir columna de fecha (auto-detecci√≥n) y normalizar a YYYY-MM-DD
if "fecha" in df_pedidos.columns:
    # 1) eliminar espacios invisibles o finales
    fechas_limpias = df_pedidos["fecha"].astype(str).str.strip()

    # 2) intentar parseo autom√°tico; mantiene cualquier formato ISO ya correcto
    df_pedidos["fecha"] = (
        pd.to_datetime(fechas_limpias, errors="coerce", dayfirst=False)
          .dt.strftime("%Y-%m-%d")      # salida uniforme para la base
    )

# Insertar ID consecutivo
df_pedidos.insert(0, "id_pedido", range(1, len(df_pedidos) + 1))

# Filtrar por 'AUTORIZADO'
if "estatus_pedido_detalle" in df_pedidos.columns:
    df_pedidos = df_pedidos[df_pedidos["estatus_pedido_detalle"] == "Autorizado"]
else:
    print("‚ö†Ô∏è No se encontr√≥ la columna 'estatus_pedido_detalle'. No se aplicar√° el filtro.")

# Eliminar registros seg√∫n archivo externo
if os.path.exists(archivo_eliminar):
    df_eliminar = pd.read_csv(archivo_eliminar, dtype=str, encoding="utf-8")
    df_eliminar.columns = [normalizar_columna(c) for c in df_eliminar.columns]

    if "id_ceda_agricultura" in df_eliminar.columns and "folio_cdf" in df_pedidos.columns:
        antes = len(df_pedidos)
        df_pedidos = df_pedidos[~df_pedidos["folio_cdf"].isin(df_eliminar["id_ceda_agricultura"])]
        print(f"‚úÖ Se eliminaron {antes - len(df_pedidos)} registros basados en 'eliminar_pedidos_desglosado.csv'.")
    else:
        print("‚ö†Ô∏è No se encontr√≥ la columna requerida en alguno de los archivos.")
else:
    print("‚ö†Ô∏è No se encontr√≥ el archivo 'eliminar_pedidos_desglosado.csv'. No se eliminaron registros.")

# Verificaci√≥n r√°pida
print("‚úÖ Columnas tras limpieza:", df_pedidos.columns.tolist())
print("üîç Vista previa:")
print(df_pedidos[["id_pedido", "folio_cdf", "dap", "urea"]].head(10))

# Importaci√≥n a PostgreSQL
try:
    with engine.begin() as conn:
        print("üßπ Eliminando registros anteriores de 'pedidos_desglosado'...")
        conn.execute(text("DELETE FROM pedidos_desglosado;"))

        # ‚úÖ Filtrar columnas v√°lidas antes de insertar
        result = conn.execute(text(
            "SELECT column_name FROM information_schema.columns WHERE table_name = 'pedidos_desglosado';"
        ))
        columnas_validas = [row[0] for row in result.fetchall()]
        df_pedidos = df_pedidos[[col for col in columnas_validas if col in df_pedidos.columns]]

        print("‚¨ÜÔ∏è Insertando nuevos registros...")
        df_pedidos.to_sql("pedidos_desglosado", conn, if_exists="append", index=False)

    print(f"‚úÖ Se han sustituido los datos antiguos por los nuevos en la tabla 'pedidos_desglosado' de la base '{DB_NAME}'.")
except Exception as e:
    print(f"‚ùå Error al importar los datos: {e}")


---
# importar_pedidos_desglosado_old.py

import pandas as pd
from sqlalchemy import text
import os
import glob
import unicodedata
import re
from conexion import engine, DB_NAME

# Ruta base de los archivos CSV
ruta_base = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/"

# Buscar archivo con la m√°scara "*-PEDIDOS DESGLOSE-NACIONAL-ANUAL_*"
file_list = glob.glob(os.path.join(ruta_base, "*PEDIDOS DESGLOSE-NACIONAL-ANUAL*"))
if not file_list:
    print("‚ùå Error: No se encontr√≥ ning√∫n archivo que coincida con la m√°scara.")
    exit()
file_path = file_list[0]

if not os.path.exists(file_path):
    print(f"‚ùå Error: No se encontr√≥ el archivo {file_path}")
    exit()

# Cargar el CSV en un DataFrame
df_pedidos_desglosados = pd.read_csv(file_path, encoding="utf-8", delimiter=",", header=0)

# --------------------------------------------------------------------------
# Funci√≥n para normalizar nombres de columnas: min√∫sculas, sin acentos, sin espacios
def normalizar_columna(col):
    col = col.strip()
    col = col.lower()
    col = ''.join(
        c for c in unicodedata.normalize('NFD', col)
        if unicodedata.category(c) != 'Mn'
    )
    col = re.sub(r'[^a-z0-9]+', '_', col)
    col = re.sub(r'[_]+', '_', col).strip('_')
    return col

# Normalizar las columnas del DataFrame
df_pedidos_desglosados.columns = [normalizar_columna(c) for c in df_pedidos_desglosados.columns]

# Reemplazar valores no v√°lidos ("N/A", "NA", "-") con None
df_pedidos_desglosados.replace(["N/A", "NA", "n/a", "-"], None, inplace=True)

# Columnas num√©ricas que deseas convertir
columnas_numericas = ["dap", "urea"]
for col in columnas_numericas:
    if col in df_pedidos_desglosados.columns:
        df_pedidos_desglosados[col] = pd.to_numeric(df_pedidos_desglosados[col], errors="coerce").round(3)

# 1Ô∏è‚É£ Agregar columna 'id_pedido' con numeraci√≥n consecutiva
df_pedidos_desglosados.insert(
    0,
    "id_pedido",
    range(1, len(df_pedidos_desglosados) + 1)
)

# 2Ô∏è‚É£ Filtrar solo 'AUTORIZADO'
if "estatus_pedido_detalle" in df_pedidos_desglosados.columns:
    df_pedidos_desglosados = df_pedidos_desglosados.loc[
        df_pedidos_desglosados["estatus_pedido_detalle"] == "AUTORIZADO"
    ]
else:
    print("‚ö†Ô∏è Advertencia: No se encontr√≥ la columna 'estatus_pedido_detalle'. No se aplicar√° el filtro.")

print("‚úÖ Columnas del DataFrame despu√©s de limpiar:", df_pedidos_desglosados.columns.tolist())
print("Cantidad de filas con estatus 'AUTORIZADO':", len(df_pedidos_desglosados))

cols_vista = ["id_pedido"] + [c for c in columnas_numericas if c in df_pedidos_desglosados.columns]
print(df_pedidos_desglosados[cols_vista].head(10))

# Importar a PostgreSQL
try:
    with engine.begin() as conn:
        print("üßπ Eliminando registros anteriores de 'pedidos_desglosado'...")
        conn.execute(text("DELETE FROM pedidos_desglosado;"))

        print("‚¨ÜÔ∏è Insertando nuevos registros...")
        df_pedidos_desglosados.to_sql("pedidos_desglosado", conn, if_exists="append", index=False)

    print(f"‚úÖ Se han sustituido los datos antiguos por los nuevos en la tabla 'pedidos_desglosado' de la base '{DB_NAME}'.")
except Exception as e:
    print(f"‚ùå Error al importar los datos: {e}")



---
# importar_pedidos_desglosado_old2.py

import pandas as pd
from sqlalchemy import text
import os
import glob
import unicodedata
import re
from conexion import engine, DB_NAME

# Ruta base
ruta_base = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/"
archivo_eliminar = os.path.join(ruta_base, "eliminar_pedidos_desglosado.csv")

# Buscar archivo con la m√°scara "*-PEDIDOS DESGLOSE-NACIONAL-ANUAL_*"
file_list = glob.glob(os.path.join(ruta_base, "*PEDIDOS DESGLOSE-NACIONAL-ANUAL*"))
if not file_list:
    print("‚ùå Error: No se encontr√≥ ning√∫n archivo que coincida con la m√°scara.")
    exit()
file_path = file_list[0]

# Cargar CSV principal
df_pedidos = pd.read_csv(file_path, encoding="utf-8", delimiter=",", header=0)

# Normalizar nombres de columnas
def normalizar_columna(col):
    col = col.strip().lower()
    col = ''.join(c for c in unicodedata.normalize('NFD', col) if unicodedata.category(c) != 'Mn')
    col = re.sub(r'[^a-z0-9]+', '_', col)
    col = re.sub(r'[_]+', '_', col).strip('_')
    return col

df_pedidos.columns = [normalizar_columna(c) for c in df_pedidos.columns]

# Reemplazar valores no v√°lidos
df_pedidos.replace(["N/A", "NA", "n/a", "-"], None, inplace=True)

# Convertir columnas num√©ricas
for col in ["dap", "urea"]:
    if col in df_pedidos.columns:
        df_pedidos[col] = pd.to_numeric(df_pedidos[col], errors="coerce").round(3)

# Insertar ID consecutivo
df_pedidos.insert(0, "id_pedido", range(1, len(df_pedidos) + 1))

# Filtrar por 'AUTORIZADO'
if "estatus_pedido_detalle" in df_pedidos.columns:
    df_pedidos = df_pedidos[df_pedidos["estatus_pedido_detalle"] == "AUTORIZADO"]
else:
    print("‚ö†Ô∏è No se encontr√≥ la columna 'estatus_pedido_detalle'. No se aplicar√° el filtro.")

# Eliminar registros seg√∫n archivo externo
if os.path.exists(archivo_eliminar):
    df_eliminar = pd.read_csv(archivo_eliminar, dtype=str, encoding="utf-8")
    df_eliminar.columns = [normalizar_columna(c) for c in df_eliminar.columns]

    if "id_ceda_agricultura" in df_eliminar.columns and "folio_cdf" in df_pedidos.columns:
        antes = len(df_pedidos)
        df_pedidos = df_pedidos[~df_pedidos["folio_cdf"].isin(df_eliminar["id_ceda_agricultura"])]
        print(f"‚úÖ Se eliminaron {antes - len(df_pedidos)} registros basados en 'eliminar_pedidos_desglosado.csv'.")
    else:
        print("‚ö†Ô∏è No se encontr√≥ la columna requerida en alguno de los archivos.")
else:
    print("‚ö†Ô∏è No se encontr√≥ el archivo 'eliminar_pedidos_desglosado.csv'. No se eliminaron registros.")

# Verificaci√≥n r√°pida
print("‚úÖ Columnas tras limpieza:", df_pedidos.columns.tolist())
print("üîç Vista previa:")
print(df_pedidos[["id_pedido", "folio_cdf", "dap", "urea"]].head(10))

# Importaci√≥n a PostgreSQL
try:
    with engine.begin() as conn:
        print("üßπ Eliminando registros anteriores de 'pedidos_desglosado'...")
        conn.execute(text("DELETE FROM pedidos_desglosado;"))
        print("‚¨ÜÔ∏è Insertando nuevos registros...")
        df_pedidos.to_sql("pedidos_desglosado", conn, if_exists="append", index=False)

    print(f"‚úÖ Se han sustituido los datos antiguos por los nuevos en la tabla 'pedidos_desglosado' de la base '{DB_NAME}'.")
except Exception as e:
    print(f"‚ùå Error al importar los datos: {e}")




---
# importar_pedidos_desglosado_old3.py

import pandas as pd
from sqlalchemy import text
import os
import glob
import unicodedata
import re
from conexion import engine, DB_NAME

# Ruta base
ruta_base = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/"
archivo_eliminar = os.path.join(ruta_base, "eliminar_pedidos_desglosado.csv")

# Buscar archivo con la m√°scara "*-PEDIDOS DESGLOSE-NACIONAL-ANUAL_*"
file_list = glob.glob(os.path.join(ruta_base, "*PEDIDOS DESGLOSE-NACIONAL-ANUAL*"))
if not file_list:
    print("‚ùå Error: No se encontr√≥ ning√∫n archivo que coincida con la m√°scara.")
    exit()
file_path = file_list[0]

# Cargar CSV principal
df_pedidos = pd.read_csv(file_path, encoding="utf-8", delimiter=",", header=0)

# Normalizar nombres de columnas
def normalizar_columna(col):
    col = col.strip().lower()
    col = ''.join(c for c in unicodedata.normalize('NFD', col) if unicodedata.category(c) != 'Mn')
    col = re.sub(r'[^a-z0-9]+', '_', col)
    col = re.sub(r'[_]+', '_', col).strip('_')
    return col

df_pedidos.columns = [normalizar_columna(c) for c in df_pedidos.columns]

# Reemplazar valores no v√°lidos
df_pedidos.replace(["N/A", "NA", "n/a", "-"], None, inplace=True)

# Convertir columnas num√©ricas
for col in ["dap", "urea"]:
    if col in df_pedidos.columns:
        df_pedidos[col] = pd.to_numeric(df_pedidos[col], errors="coerce").round(3)

# Insertar ID consecutivo
df_pedidos.insert(0, "id_pedido", range(1, len(df_pedidos) + 1))

# Filtrar por 'AUTORIZADO'
if "estatus_pedido_detalle" in df_pedidos.columns:
    df_pedidos = df_pedidos[df_pedidos["estatus_pedido_detalle"] == "AUTORIZADO"]
else:
    print("‚ö†Ô∏è No se encontr√≥ la columna 'estatus_pedido_detalle'. No se aplicar√° el filtro.")

# Eliminar registros seg√∫n archivo externo
if os.path.exists(archivo_eliminar):
    df_eliminar = pd.read_csv(archivo_eliminar, dtype=str, encoding="utf-8")
    df_eliminar.columns = [normalizar_columna(c) for c in df_eliminar.columns]

    if "id_ceda_agricultura" in df_eliminar.columns and "folio_cdf" in df_pedidos.columns:
        antes = len(df_pedidos)
        df_pedidos = df_pedidos[~df_pedidos["folio_cdf"].isin(df_eliminar["id_ceda_agricultura"])]
        print(f"‚úÖ Se eliminaron {antes - len(df_pedidos)} registros basados en 'eliminar_pedidos_desglosado.csv'.")
    else:
        print("‚ö†Ô∏è No se encontr√≥ la columna requerida en alguno de los archivos.")
else:
    print("‚ö†Ô∏è No se encontr√≥ el archivo 'eliminar_pedidos_desglosado.csv'. No se eliminaron registros.")

# Verificaci√≥n r√°pida
print("‚úÖ Columnas tras limpieza:", df_pedidos.columns.tolist())
print("üîç Vista previa:")
print(df_pedidos[["id_pedido", "folio_cdf", "dap", "urea"]].head(10))

# Importaci√≥n a PostgreSQL
try:
    with engine.begin() as conn:
        print("üßπ Eliminando registros anteriores de 'pedidos_desglosado'...")
        conn.execute(text("DELETE FROM pedidos_desglosado;"))

        # ‚úÖ Filtrar columnas v√°lidas antes de insertar
        result = conn.execute(text(
            "SELECT column_name FROM information_schema.columns WHERE table_name = 'pedidos_desglosado';"
        ))
        columnas_validas = [row[0] for row in result.fetchall()]
        df_pedidos = df_pedidos[[col for col in columnas_validas if col in df_pedidos.columns]]

        print("‚¨ÜÔ∏è Insertando nuevos registros...")
        df_pedidos.to_sql("pedidos_desglosado", conn, if_exists="append", index=False)

    print(f"‚úÖ Se han sustituido los datos antiguos por los nuevos en la tabla 'pedidos_desglosado' de la base '{DB_NAME}'.")
except Exception as e:
    print(f"‚ùå Error al importar los datos: {e}")


---
# importar_pedidos_sigap.py

import pandas as pd
from sqlalchemy import text
import os
import glob
import unicodedata
import re
from conexion import engine, DB_NAME

# Ruta base de los archivos CSV
ruta_base = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/"

# Buscar archivo con la m√°scara "*SEGUIMIENTO CANTIDADES-NACIONAL-ANUAL_*.csv"
file_list = glob.glob(os.path.join(ruta_base, "*-FERTILIZANTES-PEDIDOS SEGUIMIENTO CANTIDADES MULTIFERTILIZANTES-NACIONAL-ANUAL*.CSV"))
if not file_list:
    print("‚ùå Error: No se encontr√≥ ning√∫n archivo que coincida con la m√°scara '*-FERTILIZANTES-PEDIDOS SEGUIMIENTO CANTIDADES MULTIFERTILIZANTES-NACIONAL-ANUAL*.CSV'.")
    exit()

file_path = file_list[0]
if not os.path.exists(file_path):
    print(f"‚ùå Error: No se encontr√≥ el archivo {file_path}")
    exit()

print(f"‚ÑπÔ∏è Se utilizar√° el archivo: {file_path}")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def normalizar_columna(col):
    col = col.strip().lower()
    col = ''.join(
        c for c in unicodedata.normalize('NFD', col)
        if unicodedata.category(c) != 'Mn'
    )
    col = re.sub(r'[^a-z0-9]+', '_', col)
    col = re.sub(r'[_]+', '_', col).strip('_')
    return col

# 1Ô∏è‚É£ Leer el archivo CSV como texto
df_pedidos_sigap = pd.read_csv(file_path, encoding="utf-8", delimiter=",", header=0, dtype=str)

# 2Ô∏è‚É£ Normalizar nombres de columnas
df_pedidos_sigap.columns = [normalizar_columna(c) for c in df_pedidos_sigap.columns]

# 3Ô∏è‚É£ Reemplazar valores no v√°lidos por None
df_pedidos_sigap.replace(["N/A", "NA", "n/a", "-"], None, inplace=True)

# 4Ô∏è‚É£ Convertir a num√©rico ciertas columnas
columnas_numericas = [
    "dap_solicitado", "urea_solicitada",
    "dap_suministrado", "urea_suministrada",
    "dap_por_suministrar", "urea_por_suministrar",
    "catr_dap", "itr_dap",
    "catr_urea", "itr_urea"
]
for col in columnas_numericas:
    if col in df_pedidos_sigap.columns:
        df_pedidos_sigap[col] = pd.to_numeric(df_pedidos_sigap[col], errors="coerce").round(3)

# üîç Verificaci√≥n r√°pida
print("‚úÖ Columnas del DataFrame despu√©s de limpiar:", df_pedidos_sigap.columns.tolist())
print("üîç Vista previa de filas:")
print(df_pedidos_sigap.head(5))

# ‚úÖ Filtrar solo columnas que existen en la tabla pedidos_sigap
with engine.connect() as conn:
    result = conn.execute(text(
        "SELECT column_name FROM information_schema.columns WHERE table_name = 'pedidos_sigap';"
    ))
    columnas_validas = [row[0] for row in result.fetchall()]
    df_pedidos_sigap = df_pedidos_sigap[[col for col in columnas_validas if col in df_pedidos_sigap.columns]]

# 5Ô∏è‚É£ Insertar en PostgreSQL
try:
    with engine.begin() as conn:
        print("üßπ Eliminando registros anteriores de 'pedidos_sigap'...")
        conn.execute(text("DELETE FROM pedidos_sigap;"))

        print("‚¨ÜÔ∏è Insertando nuevos registros...")
        df_pedidos_sigap.to_sql("pedidos_sigap", conn, if_exists="append", index=False)

    print(f"‚úÖ Se han sustituido los datos antiguos por los nuevos en la tabla 'pedidos_sigap' de la base '{DB_NAME}'.")
except Exception as e:
    print(f"‚ùå Error al importar los datos: {e}")


---
# importar_pedidos_sigap_old.py

import pandas as pd
from sqlalchemy import text
import os
import glob
import unicodedata
import re
from conexion import engine, DB_NAME

# Ruta base de los archivos CSV
ruta_base = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/"

# Buscar archivo con la m√°scara "*SEGUIMIENTO CANTIDADES-NACIONAL-ANUAL_*.csv"
file_list = glob.glob(os.path.join(ruta_base, "*SEGUIMIENTO CANTIDADES-NACIONAL-ANUAL*.csv"))
if not file_list:
    print("‚ùå Error: No se encontr√≥ ning√∫n archivo que coincida con la m√°scara '*SEGUIMIENTO CANTIDADES-NACIONAL-ANUAL_*.csv'.")
    exit()

file_path = file_list[0]
if not os.path.exists(file_path):
    print(f"‚ùå Error: No se encontr√≥ el archivo {file_path}")
    exit()

print(f"‚ÑπÔ∏è Se utilizar√° el archivo: {file_path}")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def normalizar_columna(col):
    col = col.strip().lower()
    col = ''.join(
        c for c in unicodedata.normalize('NFD', col)
        if unicodedata.category(c) != 'Mn'
    )
    col = re.sub(r'[^a-z0-9]+', '_', col)
    col = re.sub(r'[_]+', '_', col).strip('_')
    return col

# 1Ô∏è‚É£ Leer el archivo CSV como texto
df_pedidos_sigap = pd.read_csv(file_path, encoding="utf-8", delimiter=",", header=0, dtype=str)

# 2Ô∏è‚É£ Normalizar nombres de columnas
df_pedidos_sigap.columns = [normalizar_columna(c) for c in df_pedidos_sigap.columns]

# 3Ô∏è‚É£ Reemplazar valores no v√°lidos por None
df_pedidos_sigap.replace(["N/A", "NA", "n/a", "-"], None, inplace=True)

# 4Ô∏è‚É£ Convertir a num√©rico ciertas columnas
columnas_numericas = [
    "dap_solicitado", "urea_solicitada",
    "dap_suministrado", "urea_suministrada",
    "dap_por_suministrar", "urea_por_suministrar",
    "catr_dap", "itr_dap",
    "catr_urea", "itr_urea"
]
for col in columnas_numericas:
    if col in df_pedidos_sigap.columns:
        df_pedidos_sigap[col] = pd.to_numeric(df_pedidos_sigap[col], errors="coerce").round(3)

# üîç Verificaci√≥n r√°pida
print("‚úÖ Columnas del DataFrame despu√©s de limpiar:", df_pedidos_sigap.columns.tolist())
print("üîç Vista previa de filas:")
print(df_pedidos_sigap.head(5))

# 5Ô∏è‚É£ Insertar en PostgreSQL
try:
    with engine.begin() as conn:
        print("üßπ Eliminando registros anteriores de 'pedidos_sigap'...")
        conn.execute(text("DELETE FROM pedidos_sigap;"))

        print("‚¨ÜÔ∏è Insertando nuevos registros...")
        df_pedidos_sigap.to_sql("pedidos_sigap", conn, if_exists="append", index=False)

    print(f"‚úÖ Se han sustituido los datos antiguos por los nuevos en la tabla 'pedidos_sigap' de la base '{DB_NAME}'.")
except Exception as e:
    print(f"‚ùå Error al importar los datos: {e}")


---
# importar_proyeccion_abasto_2025.py

import pandas as pd
from sqlalchemy import text
from conexion import engine, DB_NAME

# Ruta al CSV
ruta_csv = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/Abasto_Programado_Fertilizante_2025.csv"

# Leer datos
df = pd.read_csv(ruta_csv)
df.columns = df.columns.str.lower().str.replace(" ", "_")

# ‚úÖ Corregido: fechas con a√±o de 2 d√≠gitos
df['fecha'] = pd.to_datetime(df['fecha'], format="%d/%m/%y")

# Importar a PostgreSQL
try:
    with engine.begin() as conn:
        conn.execute(text("TRUNCATE TABLE proyeccion_abasto_2025;"))
        df.to_sql("proyeccion_abasto_2025", con=conn, if_exists="append", index=False)
    print(f"‚úÖ Datos importados correctamente a la tabla 'proyeccion_abasto_2025' en la base '{DB_NAME}'")
except Exception as e:
    print(f"‚ùå Error al importar los datos: {e}")



---
# importar_red.py

import pandas as pd
import csv
from sqlalchemy import text
import os

# ‚úÖ Importar la conexi√≥n centralizada
from conexion import engine

# Ruta del archivo CSV
file_path = os.path.expanduser("~/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/red_distribucion_2025.csv")

# Verifica que exista
if not os.path.exists(file_path):
    print(f"‚ùå Archivo no encontrado: {file_path}")
    exit()

# Cargar CSV usando pandas y soporte para campos de texto largos
df = pd.read_csv(
    file_path,
    encoding="utf-8",
    delimiter=",",
    quoting=csv.QUOTE_MINIMAL,
    quotechar='"',
    skip_blank_lines=True
)

# Reemplazo de valores no v√°lidos
df.replace(["N/A", "NA", "n/a", "-"], None, inplace=True)

# Columnas num√©ricas que deben limpiarse
columnas_numericas = [
    "meta_derechohabientes", "meta_superficie_ha", "meta_dap_ton", "meta_urea_ton", "meta_total_ton",
    "largo_m", "ancho_m", "alto_m", "altura_estiba_m",
    "tonelaje_alm_dap", "tonelaje_alm_urea", "tonelaje_alm_total",
    "latitud", "longitud", "traspaleo_latitud", "traspaleo_longitud",
    "capacidad_descarga", "prioridad"
]

for columna in columnas_numericas:
    if columna in df.columns:
        df[columna] = df[columna].astype(str).str.replace(",", "", regex=True).str.strip()
        df[columna] = pd.to_numeric(df[columna], errors="coerce")

# Verificaci√≥n inicial
print("‚úÖ Columnas cargadas:", df.columns.tolist())
print(df[columnas_numericas].head(5))

try:
    with engine.begin() as conn:
        # Insertar datos directamente
        df.to_sql("red_distribucion", conn, if_exists="append", index=False)
        print("‚úÖ Datos insertados correctamente en 'red_distribucion'.")

except Exception as e:
    print(f"‚ùå Error durante la carga: {e}")



---
# importar_red_distribucion_2024.py

import pandas as pd
from sqlalchemy import create_engine

# Configuraci√≥n de la conexi√≥n a PostgreSQL
DB_USER = "postgres"
DB_PASSWORD = "Art4125r0"
DB_HOST = "localhost"
DB_PORT = "5432"
DB_NAME = "fertilizantes"

# Crear la conexi√≥n
engine = create_engine(f'postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}')

# Ruta del archivo CSV
ruta_csv = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/red_distribucion_2024.csv"

# Leer el archivo CSV
df = pd.read_csv(ruta_csv, encoding='utf-8-sig')

# Asegurar que los campos tengan el tipo correcto antes de insertar
df['meta_derechohabientes'] = df['meta_derechohabientes'].astype(int)
df['meta_superficie_ha'] = df['meta_superficie_ha'].astype(int)
df['meta_dap_ton'] = df['meta_dap_ton'].astype(float)
df['meta_urea_ton'] = df['meta_urea_ton'].astype(float)

# Insertar sin modificar la estructura existente
df.to_sql('red_distribucion_2024', engine, if_exists='append', index=False)

print("‚úÖ Datos importados correctamente en red_distribucion_2024 con estructura respetada.")



---
# importar_remanentes.py

import os
import glob
import unicodedata
import pandas as pd
import re
from sqlalchemy import text
from sqlalchemy.types import Text, Integer, Numeric, TIMESTAMP, Date
from conexion import engine, DB_NAME

# =============================================================================
# FUNCION PARA REMOVER ACENTOS
# =============================================================================
def remover_acentos(cadena):
    if not isinstance(cadena, str):
        return cadena
    nfkd_form = unicodedata.normalize('NFD', cadena)
    return nfkd_form.encode('ASCII', 'ignore').decode('utf-8', 'ignore')

# =============================================================================
# 1. RUTAS DE ARCHIVOS
# =============================================================================
ruta_base = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/"
archivo_remanentes = glob.glob(os.path.join(ruta_base, "*TRANSFERENCIAS-NACIONAL-ANUAL_2025_rem.csv"))[0]
archivo_eliminar = os.path.join(ruta_base, "eliminar_remanentes.xlsx")
archivo_corregidos = os.path.join(ruta_base, "remanentes_corregido.csv")

# =============================================================================
# FASE A: LECTURA DEL CSV E IMPRESION DE COLUMNAS INICIALES (ARCHIVO ORIGINAL)
# =============================================================================
remanentes_df = pd.read_csv(archivo_remanentes, dtype=str, encoding="utf-8")
print("FASE A) Columnas inmediatamente tras la lectura (ORIGINAL):\n", list(remanentes_df.columns), "\n")

# =============================================================================
# FASE B: RENOMBRADO PUNTUAL PREVIO (SI EXISTE "estatus_de_recepci√É¬≥n")
# =============================================================================
if "estatus_de_recepci√É¬≥n" in remanentes_df.columns:
    remanentes_df.rename(columns={"estatus_de_recepci√É¬≥n": "estatus_de_recepci√≥n"}, inplace=True)

print("FASE B) Columnas tras renombrar 'estatus_de_recepci√É¬≥n' -> 'estatus_de_recepci√≥n' (ORIGINAL):\n",
      list(remanentes_df.columns), "\n")

# =============================================================================
# FASE C: NORMALIZAR (REMOVER ACENTOS, minusculas, etc.) E IMPRIMIR
# =============================================================================
remanentes_df.columns = [
    remover_acentos(col).lower().strip().replace(" ", "_") for col in remanentes_df.columns
]
print("FASE C) Columnas tras normalizar (ORIGINAL):\n", list(remanentes_df.columns), "\n")

# =============================================================================
# FASE D: VERIFICACION FINAL (Si aparece "estatus_de_recepcian")
# =============================================================================
if "estatus_de_recepcian" in remanentes_df.columns:
    print("Detectamos la columna conflictiva 'estatus_de_recepcian' en el archivo original.")
    remanentes_df["estatus_de_recepcion"] = remanentes_df["estatus_de_recepcian"]
    remanentes_df.drop(columns=["estatus_de_recepcian"], inplace=True)
    print("Renombramos 'estatus_de_recepcian' -> 'estatus_de_recepcion'.")

print("FASE D) Columnas tras verificaci√≥n final (ORIGINAL):\n", list(remanentes_df.columns), "\n")

# =============================================================================
# (Opcional) Eliminar registros seg√∫n "eliminar_remanentes.xlsx"
# =============================================================================
if os.path.exists(archivo_eliminar):
    eliminar_df = pd.read_excel(archivo_eliminar, dtype=str)
    eliminar_df.columns = [
        remover_acentos(col).lower().strip().replace(" ", "_") for col in eliminar_df.columns
    ]

    if "folio_transferencia" in eliminar_df.columns and "folio_transferencia" in remanentes_df.columns:
        antes_eliminar = len(remanentes_df)
        remanentes_df = remanentes_df[~remanentes_df["folio_transferencia"].isin(eliminar_df["folio_transferencia"])]
        despues_eliminar = len(remanentes_df)
        print(f"‚úÖ Se eliminaron {antes_eliminar - despues_eliminar} registros basados en 'eliminar_remanentes.xlsx'.")
    else:
        print("‚ö†Ô∏è No se encontr√≥ la columna 'folio_transferencia' en el archivo de eliminaci√≥n.")
else:
    print("‚ö†Ô∏è No se encontr√≥ 'eliminar_remanentes.xlsx'. Se omite parte de eliminaci√≥n.")

# =============================================================================
# LEER Y NORMALIZAR ARCHIVO CORREGIDO (SI EXISTE)
# =============================================================================
if os.path.exists(archivo_corregidos):
    corregidos_df = pd.read_csv(archivo_corregidos, dtype=str, encoding="utf-8")
    print("\nArchivo 'remanentes_corregido.csv' con columnas:\n", list(corregidos_df.columns))

    if "estatus_de_recepci√É¬≥n" in corregidos_df.columns:
        corregidos_df.rename(columns={"estatus_de_recepci√É¬≥n": "estatus_de_recepci√≥n"}, inplace=True)

    corregidos_df.columns = [
        remover_acentos(col).lower().strip().replace(" ", "_") for col in corregidos_df.columns
    ]

    if "estatus_de_recepcian" in corregidos_df.columns:
        print("Detectamos 'estatus_de_recepcian' en el archivo corregido.")
        corregidos_df["estatus_de_recepcion"] = corregidos_df["estatus_de_recepcian"]
        corregidos_df.drop(columns=["estatus_de_recepcian"], inplace=True)

    antes_corr = len(remanentes_df)
    remanentes_df = pd.concat([remanentes_df, corregidos_df], ignore_index=True)
    despues_corr = len(remanentes_df)
    print(f"‚úÖ Se a√±adieron {despues_corr - antes_corr} registros de 'remanentes_corregido.csv'.")
else:
    print("\n‚ö†Ô∏è No se encontr√≥ 'remanentes_corregido.csv'. Se omite.")

# =============================================================================
# FECHAS: INTENTA %d/%m/%Y Y LUEGO %d/%m/%y
# =============================================================================
def convertir_fecha_dual(col_name):
    parse_col = f"{col_name}_parse"
    remanentes_df[parse_col] = pd.to_datetime(remanentes_df[col_name], format="%d/%m/%Y", errors="coerce")
    mask = remanentes_df[parse_col].isna() & remanentes_df[col_name].notna()
    remanentes_df.loc[mask, parse_col] = pd.to_datetime(remanentes_df.loc[mask, col_name], format="%d/%m/%y", errors="coerce")
    remanentes_df[col_name] = remanentes_df[parse_col].dt.date
    remanentes_df.drop(columns=[parse_col], inplace=True)

if "fecha_de_salida" in remanentes_df.columns:
    convertir_fecha_dual("fecha_de_salida")

if "fecha_de_llegada" in remanentes_df.columns:
    convertir_fecha_dual("fecha_de_llegada")

# =============================================================================
# ASIGNAR ID CEDA SEGUN ESTATUS
# =============================================================================
if "id_ceda_agricultura" not in remanentes_df.columns:
    remanentes_df["id_ceda_agricultura"] = None

if "estatus" in remanentes_df.columns:
    if "cdf_destino_final" in remanentes_df.columns:
        mask_entregada = remanentes_df["estatus"].str.lower() == "entregada"
        remanentes_df.loc[mask_entregada, "id_ceda_agricultura"] = remanentes_df["cdf_destino_final"]
    if "cdf_destino_original" in remanentes_df.columns:
        mask_no_entregada = remanentes_df["estatus"].str.lower() != "entregada"
        remanentes_df.loc[mask_no_entregada, "id_ceda_agricultura"] = remanentes_df["cdf_destino_original"]

print("\nAntes de Insertar, columnas definitivas:\n", list(remanentes_df.columns), "\n")

# =============================================================================
# DTYPE PARA LA INSERCION
# =============================================================================
dtype_sqlalchemy = {
    "folio_transferencia": Text,
    "cdf_origen": Text,
    "estatus": Text,
    "producto": Text,
    "abreviacion_producto": Text,
    "toneladas_iniciales": Numeric(10, 3),
    "toneladas_en_el_destino": Numeric(10, 3),
    "fecha_de_salida": Date,
    "cdf_destino_original": Text,
    "cdf_destino_final": Text,
    "fecha_de_llegada": Date,
    "estatus_de_recepcion": Text,
    "descripcion": Text,
    "destino_final": Text,
    "nombre_operador": Text,
    "telefono_operador": Text,
    "placas_transporte": Text,
    "tipo_transporte": Text,
    "bultos_iniciales": Integer,
    "numero_ticket_bascula": Text,
    "bultos_en_el_destino": Integer,
    "id_ceda_agricultura": Text
}

table_name = "remanentes"

print("‚ö†Ô∏è Revisar las columnas EXACTAS en 'remanentes_df' final y si coincide con 'dtype_sqlalchemy':")
for col in remanentes_df.columns:
    if col not in dtype_sqlalchemy:
        print(f" -> La columna '{col}' no est√° en dtype_sqlalchemy (posible causa de error).")

print(f"\nSe van a insertar {len(remanentes_df)} registros en '{table_name}'.")

# TRUNCATE la tabla
with engine.connect() as conn:
    conn.execute(text(f"TRUNCATE TABLE {table_name};"))
    conn.commit()

print("\n‚¨ÜÔ∏è Insertando en la tabla:", table_name)
remanentes_df.to_sql(
    name=table_name,
    con=engine,
    if_exists="append",
    index=False,
    dtype=dtype_sqlalchemy
)

print(f"‚úÖ Se han sustituido los datos antiguos por los nuevos en la tabla '{table_name}' de la base '{DB_NAME}'.")



---
# importar_transferencias.py

import os
import glob
import unicodedata
import pandas as pd
from sqlalchemy import text
from sqlalchemy.types import Text, Integer, Numeric, Date
from conexion import engine  # Usamos la conexi√≥n centralizada

# =============================================================================
# FUNCION PARA REMOVER ACENTOS
# =============================================================================
def remover_acentos(cadena):
    if not isinstance(cadena, str):
        return cadena
    nfkd_form = unicodedata.normalize('NFD', cadena)
    return nfkd_form.encode('ASCII', 'ignore').decode('utf-8', 'ignore')


# =============================================================================
# 1. RUTAS DE ARCHIVOS
# =============================================================================
ruta_base = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/"
archivo_transferencias = glob.glob(os.path.join(ruta_base, "*-FERTILIZANTES-TRANSFERENCIAS-NACIONAL-ANUAL_2025*.CSV"))[0]
archivo_eliminar = os.path.join(ruta_base, "eliminar_transferencias.xlsx")
archivo_corregidos = os.path.join(ruta_base, "transferencias_corregido.csv")

# =============================================================================
# LECTURA DEL CSV
# =============================================================================
transferencias_df = pd.read_csv(archivo_transferencias, dtype=str, encoding="utf-8")

# Renombrado especial
if "estatus_de_recepci√É¬≥n" in transferencias_df.columns:
    transferencias_df.rename(columns={"estatus_de_recepci√É¬≥n": "estatus_de_recepci√≥n"}, inplace=True)

# Normalizar nombres de columnas
transferencias_df.columns = [
    remover_acentos(col).lower().strip().replace(" ", "_") for col in transferencias_df.columns
]

# Verificaci√≥n de errores comunes
if "estatus_de_recepcian" in transferencias_df.columns:
    transferencias_df["estatus_de_recepcion"] = transferencias_df["estatus_de_recepcian"]
    transferencias_df.drop(columns=["estatus_de_recepcian"], inplace=True)

# =============================================================================
# ELIMINAR REGISTROS
# =============================================================================
if os.path.exists(archivo_eliminar):
    eliminar_df = pd.read_excel(archivo_eliminar, dtype=str)
    eliminar_df.columns = [
        remover_acentos(col).lower().strip().replace(" ", "_") for col in eliminar_df.columns
    ]
    if "folio_transferencia" in eliminar_df.columns:
        antes = len(transferencias_df)
        transferencias_df = transferencias_df[~transferencias_df["folio_transferencia"].isin(eliminar_df["folio_transferencia"])]
        print(f"‚úÖ Se eliminaron {antes - len(transferencias_df)} registros con 'eliminar_transferencias.xlsx'.")

# =============================================================================
# AGREGAR REGISTROS CORREGIDOS
# =============================================================================
if os.path.exists(archivo_corregidos):
    corregidos_df = pd.read_csv(archivo_corregidos, dtype=str, encoding="utf-8")
    corregidos_df.columns = [
        remover_acentos(col).lower().strip().replace(" ", "_") for col in corregidos_df.columns
    ]
    antes = len(transferencias_df)
    transferencias_df = pd.concat([transferencias_df, corregidos_df], ignore_index=True)
    print(f"‚úÖ Se a√±adieron {len(transferencias_df) - antes} registros corregidos.")

# =============================================================================
# PARSEO DE FECHAS
# =============================================================================
for col in ["fecha_de_salida", "fecha_de_llegada"]:
    if col in transferencias_df.columns:
        transferencias_df[col] = pd.to_datetime(transferencias_df[col], errors="coerce").dt.date

# =============================================================================
# ASIGNAR 'id_ceda_agricultura'
# =============================================================================
if "id_ceda_agricultura" not in transferencias_df.columns:
    transferencias_df["id_ceda_agricultura"] = None

if "estatus" in transferencias_df.columns:
    if "cdf_destino_final" in transferencias_df.columns:
        mask_entregada = transferencias_df["estatus"].str.lower() == "entregada"
        transferencias_df.loc[mask_entregada, "id_ceda_agricultura"] = transferencias_df["cdf_destino_final"]

    if "cdf_destino_original" in transferencias_df.columns:
        mask_no_entregada = transferencias_df["estatus"].str.lower() != "entregada"
        transferencias_df.loc[mask_no_entregada, "id_ceda_agricultura"] = transferencias_df["cdf_destino_original"]

# =============================================================================
# DTYPE PARA LA INSERCI√ìN
# =============================================================================
dtype_sqlalchemy = {
    "folio_transferencia": Text,
    "cdf_origen": Text,
    "estatus": Text,
    "producto": Text,
    "abreviacion_producto": Text,
    "toneladas_iniciales": Numeric(10, 3),
    "toneladas_en_el_destino": Numeric(10, 3),
    "fecha_de_salida": Date,
    "cdf_destino_original": Text,
    "cdf_destino_final": Text,
    "fecha_de_llegada": Date,
    "estatus_de_recepcion": Text,
    "descripcion": Text,
    "destino_final": Text,
    "nombre_operador": Text,
    "telefono_operador": Text,
    "placas_transporte": Text,
    "tipo_transporte": Text,
    "bultos_iniciales": Integer,
    "numero_ticket_bascula": Text,
    "bultos_en_destino": Integer,
    "id_ceda_agricultura": Text
}

# =============================================================================
# INSERTAR A BASE DE DATOS
# =============================================================================
table_name = "transferencias"
print(f"\nüóë Truncando tabla '{table_name}'...")
with engine.begin() as conn:
    conn.execute(text(f"TRUNCATE TABLE {table_name};"))

print(f"‚¨ÜÔ∏è Insertando {len(transferencias_df)} registros en '{table_name}'...")
transferencias_df.to_sql(
    name=table_name,
    con=engine,
    if_exists="append",
    index=False,
    dtype=dtype_sqlalchemy
)
print("‚úÖ Proceso finalizado sin errores.")



---
# informe_2025py.py

import xlwings as xw
from pptx import Presentation
from pptx.util import Inches
import os
import shutil

# üìÅ Archivos
ruta_excel = "/Users/Arturo/AGRICULTURA/fertilizantes_principal_2025.xlsx"
hoja = "Avances Generales"
rango = "B4:Q17"
imagen_temp = "/Users/Arturo/AGRICULTURA/imagen_temp_avance.png"
ruta_presentacion = "/Users/Arturo/AGRICULTURA/INFORMES/Avances Fertilizantes 2025_28032025.pptx"

# üß© Abrir Excel y copiar el rango como imagen
app = xw.App(visible=False)
wb = xw.Book(ruta_excel)
sht = wb.sheets[hoja]
sht.range(rango).api.CopyPicture(Appearance=1, Format=2)

# üñºÔ∏è Pegar en hoja temporal
wb_temp = xw.Book()
ws_temp = wb_temp.sheets[1]
ws_temp.api.Paste()
shape = ws_temp.api.Shapes(1)
shape.Export(imagen_temp, 2)
wb_temp.close()
wb.close()
app.quit()
print(f"‚úÖ Imagen creada: {imagen_temp}")

# üñºÔ∏è Abrir presentaci√≥n y duplicar slide base
ppt = Presentation(ruta_presentacion)
slide_base = ppt.slides[0]  # toma el primero como plantilla
slide_layout = slide_base.slide_layout
slide_nuevo = ppt.slides.add_slide(slide_layout)

# üîÅ Copiar shapes del slide base
for shape in slide_base.shapes:
    el = shape.element
    new_el = el.clone()
    slide_nuevo.shapes._spTree.insert_element_before(new_el, 'p:extLst')

# üñºÔ∏è Agregar la imagen en posici√≥n fija
slide_nuevo.shapes.add_picture(imagen_temp, Inches(1), Inches(1.6), height=Inches(4.8))

# üíæ Guardar presentaci√≥n
ppt.save(ruta_presentacion)
print(f"‚úÖ Slide duplicado con imagen en: {ruta_presentacion}")



---
# integrar_queries.py

import os
from pathlib import Path

# Ruta base donde est√°n los archivos
carpeta = Path("/Users/Arturo/AGRICULTURA/FERTILIZANTES/QUERIES")
archivo_salida = carpeta / "acumulado_queries.txt"

# Obtener todos los archivos .txt y .sql en la carpeta
archivos = sorted([f for f in carpeta.iterdir() if f.suffix in [".txt", ".sql"]])

with open(archivo_salida, "w", encoding="utf-8") as salida:
    for archivo in archivos:
        salida.write(f"-- CONTENIDO DE: {archivo.name}\n\n")
        with open(archivo, "r", encoding="utf-8") as f:
            salida.write(f.read())
        salida.write("\n\n" + "-"*80 + "\n\n")

print(f"Archivo generado: {archivo_salida}")



---
# integrar_scripts.py

import os
from pathlib import Path

# Ruta donde est√°n los scripts
carpeta_scripts = Path("/Users/Arturo/AGRICULTURA/FERTILIZANTES/SCRIPTS")
archivo_salida = carpeta_scripts / "acumulado_scripts.txt"

# Obtener todos los archivos .py ordenados alfab√©ticamente
archivos_py = sorted([f for f in carpeta_scripts.iterdir() if f.suffix == ".py"])

with open(archivo_salida, "w", encoding="utf-8") as salida:
    for archivo in archivos_py:
        salida.write(f"# ===== ARCHIVO: {archivo.name} =====\n\n")
        with open(archivo, "r", encoding="utf-8") as f:
            salida.write(f.read())
        salida.write("\n\n" + "#" * 80 + "\n\n")

print(f"Archivo generado: {archivo_salida}")



---
# leer_encabezados_beneficiarios.py

import pandas as pd

# Ruta del archivo CSV de derechohabientes
ruta_csv = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/derechohabientes/1051-FERTILIZANTES-BENEFICIARIOS-NACIONAL-PRIMER CORTE_2025-03-09 14_45_47_.csv"

# Cargar el archivo solo con los encabezados
df = pd.read_csv(ruta_csv, nrows=0)

# Mostrar los nombres de las columnas
print("üìå Encabezados en el archivo CSV:")
print(df.columns.tolist())


---
# limpiar_bases_fertilizantes_old.py

from sqlalchemy import create_engine, text

# Configuraci√≥n de conexi√≥n
DB_USER = "postgres"
DB_PASSWORD = "Art4125r0"
DB_HOST = "localhost"
DB_PORT = "5432"
DB_NAME = "fertilizantes"

# Crear engine de conexi√≥n
engine = create_engine(f"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}")

# Listado de tablas en orden para limpiar
tablas_ordenadas = [
    "derechohabientes",
    "fletes",
    "transferencias",
    "remanentes",
    "pedidos_desglosado",
    "pedidos_sigap",
    "incidentes",
    "red_distribucion"  # Esta debe ir al final
]

try:
    with engine.begin() as conn:
        print("üö® Iniciando limpieza de tablas...")
        for tabla in tablas_ordenadas:
            print(f"üßπ Limpiando: {tabla}...")
            conn.execute(text(f"TRUNCATE {tabla} CASCADE;"))
        print("‚úÖ Todas las tablas fueron limpiadas correctamente.")

except Exception as e:
    print(f"‚ùå Error durante la limpieza: {e}")


---
# mapa_cedas_2025.py

import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt
from sqlalchemy import text
from conexion import engine

# === 1. Cargar divisiones estatales ===
path_geojson = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/MAPAS/georef-mexico-state@public.geojson"
estados = gpd.read_file(path_geojson)

# === 2. Cargar CEDAS desde PostgreSQL ===
query = """
SELECT id_ceda_agricultura, nombre_cedas, estado, latitud, longitud
FROM red_distribucion
WHERE latitud IS NOT NULL AND longitud IS NOT NULL;
"""
df_cedas = pd.read_sql(query, engine)

# === 3. Convertir CEDAS a GeoDataFrame ===
gdf_cedas = gpd.GeoDataFrame(
    df_cedas,
    geometry=gpd.points_from_xy(df_cedas["longitud"], df_cedas["latitud"]),
    crs="EPSG:4326"
)

# === 4. Crear figura ===
fig, ax = plt.subplots(figsize=(10, 10))

# Rellenar los estados con color #1e5b4f y bordes blancos
estados.plot(ax=ax, facecolor="#e6d194", edgecolor="white", linewidth=0.7)

# Dibujar los CEDAS encima
gdf_cedas.plot(ax=ax, color="#9b2247", markersize=10, alpha=0.9)

# Limpiar gr√°fico
ax.set_axis_off()
ax.set_xlim(-118, -86)
ax.set_ylim(14.5, 33.5)

# === 5. Guardar como imagen final ===
ruta_salida = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/MAPAS/cedas_mapa_estatal_relleno.png"
plt.savefig(ruta_salida, dpi=300, bbox_inches='tight')
plt.close()

print(f"‚úÖ Mapa generado con fondo interno en: {ruta_salida}")



---
# no_sincronizaados_p1.py

from sqlalchemy import text
import pandas as pd
from conexion import engine

query = """
SELECT DISTINCT acuse_estatal
FROM full_derechohabientes_2025
WHERE acuse_estatal IS NOT NULL
LIMIT 20
"""

df = pd.read_sql_query(text(query), engine)
print("üóÇÔ∏è Acuses reales en la base:")
print(df.head(20))


---
# pasar_a_excel_reporte_derechohabientes_bienestar_td.py

import os
import pandas as pd
from openpyxl import load_workbook, Workbook
from openpyxl.utils import get_column_letter
from copy import copy

# üìÅ Rutas
carpeta_csv = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/"
ruta_encabezado = "/Users/Arturo/AGRICULTURA/INFORMES/INFORMES BIENESTAR/formato_Fertilizantes - seguimiento beneficiarios.xlsx"

# üß† Copiar valores y formato de celdas
def copiar_celda_origen_a_destino(celda_origen, celda_destino):
    celda_destino.value = celda_origen.value
    if celda_origen.has_style:
        celda_destino.font = copy(celda_origen.font)
        celda_destino.border = copy(celda_origen.border)
        celda_destino.fill = copy(celda_origen.fill)
        celda_destino.number_format = copy(celda_origen.number_format)
        celda_destino.protection = copy(celda_origen.protection)
        celda_destino.alignment = copy(celda_origen.alignment)

# üîÅ Procesar cada archivo CSV
for archivo in os.listdir(carpeta_csv):
    if archivo.endswith(".csv") and "Fertilizantes - seguimiento beneficiarios -" in archivo:
        ruta_csv = os.path.join(carpeta_csv, archivo)
        df = pd.read_csv(ruta_csv, dtype=str)

        # üìò Crear nuevo archivo Excel
        wb_nuevo = Workbook()
        ws_nuevo = wb_nuevo.active

        # üìñ Cargar encabezado con formato
        wb_formato = load_workbook(ruta_encabezado)
        ws_formato = wb_formato.active

        # Copiar celdas A1:L2 con estilos
        for fila in range(1, 3):  # filas 1 y 2
            for col in range(1, 13):  # columnas A a L
                celda_origen = ws_formato.cell(row=fila, column=col)
                celda_destino = ws_nuevo.cell(row=fila, column=col)
                copiar_celda_origen_a_destino(celda_origen, celda_destino)

        # Copiar celdas combinadas (merge)
        for rango in ws_formato.merged_cells.ranges:
            if str(rango).startswith("A1") or str(rango).startswith("B1") or str(rango).startswith("C1") or "L2" in str(rango):
                ws_nuevo.merge_cells(str(rango))

        # Copiar contenido CSV desde la fila 3
        for i, row in df.iterrows():
            for j, value in enumerate(row.tolist(), start=1):
                # Si la columna es 'F' o 'G', convertir a n√∫mero decimal si es posible
                col_letra = get_column_letter(j)
                if col_letra in ['F', 'G']:
                    try:
                        ws_nuevo.cell(row=i + 3, column=j, value=float(value))
                    except:
                        ws_nuevo.cell(row=i + 3, column=j, value=value)  # deja el texto si no es n√∫mero
                else:
                    ws_nuevo.cell(row=i + 3, column=j, value=value)

        # Ajustar ancho de columnas
        for col in ws_nuevo.columns:
            max_length = 0
            col_letter = get_column_letter(col[0].column)
            for cell in col:
                try:
                    if cell.value:
                        max_length = max(max_length, len(str(cell.value)))
                except:
                    pass
            ws_nuevo.column_dimensions[col_letter].width = max_length + 2

        # Guardar con mismo nombre pero como .xlsx
        ruta_xlsx = os.path.join(carpeta_csv, archivo.replace(".csv", ".xlsx"))
        wb_nuevo.save(ruta_xlsx)

        print(f"‚úÖ Archivo procesado con formato: {archivo.replace('.csv', '.xlsx')}")

print("üéâ Todos los archivos se procesaron con encabezado formateado correctamente.")


---
# paso1_macro.py

import xlwings as xw

ruta_excel = "/Users/Arturo/AGRICULTURA/INFORMES/INFORME_2025.xlsm"

app = xw.App(visible=False)
wb = xw.Book(ruta_excel)

try:
    macro = wb.macro("ExportarImagenGuerrero")
    macro()
    print("‚úÖ Macro 'ExportarImagenGuerrero' ejecutada con √©xito desde Python.")
except Exception as e:
    print(f"‚ùå Error al ejecutar la macro: {e}")

wb.close()
app.quit()



---
# paso1_mas_paso_2.py

import xlwings as xw
from PIL import ImageGrab
import os

# Rutas
ruta_excel = "/Users/Arturo/AGRICULTURA/INFORMES/INFORME_2025.xlsm"
carpeta_imgs = "/Users/Arturo/AGRICULTURA/INFORMES/Img_Temporales"
ruta_img1 = os.path.join(carpeta_imgs, "grafico_1.png")
ruta_img2 = os.path.join(carpeta_imgs, "grafico_2.png")

# Asegurarse que la carpeta exista
os.makedirs(carpeta_imgs, exist_ok=True)

# Iniciar app de Excel
app = xw.App(visible=False)
wb = xw.Book(ruta_excel)

def ejecutar_macro_y_guardar(nombre_macro, ruta_imagen):
    try:
        macro = wb.macro(nombre_macro)
        macro()
        print(f"‚úÖ Macro '{nombre_macro}' ejecutada.")

        # Espera ligera para asegurar que la imagen llegue al portapapeles
        import time; time.sleep(1)

        imagen = ImageGrab.grabclipboard()
        if imagen:
            imagen.save(ruta_imagen, "PNG")
            print(f"‚úÖ Imagen guardada como: {ruta_imagen}")
        else:
            print(f"‚ùå No se encontr√≥ imagen en el portapapeles tras ejecutar '{nombre_macro}'.")

    except Exception as e:
        print(f"‚ùå Error ejecutando '{nombre_macro}': {e}")

# Ejecutar macros y guardar im√°genes
ejecutar_macro_y_guardar("ExportarImagenDesdeRango", ruta_img1)
ejecutar_macro_y_guardar("ExportarImagenGuerrero", ruta_img2)

# Cerrar archivo y Excel
wb.close()
app.quit()



---
# paso2_guardar_imagen.py

from PIL import ImageGrab
import os

ruta_img = "/Users/Arturo/AGRICULTURA/imagen_temp_avance.png"

# Capturar imagen desde el portapapeles
imagen = ImageGrab.grabclipboard()

if not imagen:
    print("‚ùå No se encontr√≥ imagen en el portapapeles. Aseg√∫rate de ejecutar la macro antes.")
else:
    imagen.save(ruta_img, "PNG")
    print(f"‚úÖ Imagen guardada en: {ruta_img}")



---
# paso3_insertar_imagen_en_ppt.py

from pptx import Presentation
from pptx.util import Cm
from pptx.enum.shapes import MSO_SHAPE_TYPE
import os

# Rutas
carpeta_imgs = "/Users/Arturo/AGRICULTURA/INFORMES/Img_Temporales"
ruta_pptx = "/Users/Arturo/AGRICULTURA/INFORMES/Avances Fertilizantes 2025.pptx"

# Lista completa de im√°genes y sus slides (√≠ndice = slide - 1)
imagenes = [
    ("avance_nacional.png", 1),
    ("guerrero_avances.png", 9),
    ("durango_avances.png", 7),
    ("michoacan_avances.png", 11),
    ("morelos_avances.png", 13),
    ("tlaxcala_avances.png", 15),
    ("veracruz_avances.png", 18),
    ("chiapas_avances.png", 20),
    ("tabasco_avances.png", 23),
    ("colima_avances.png", 24),
    ("edomex_avances.png", 25),
    ("cdmx_avanes.png", 26),
    ("guanajuato_avances.png", 27),
    ("jalisco_avances.png", 28),
    ("puebla_avances.png", 29),
    ("hidalgo_avances.png", 30),
    ("campeche_avances.png", 31),
    ("yucatan_avances.png", 32),
    ("quintana_roo_avances.png", 33),
    ("oaxaca_avances.png", 34)
]

# Validar presentaci√≥n
if not os.path.exists(ruta_pptx):
    print("‚ùå No se encontr√≥ la presentaci√≥n.")
else:
    ppt = Presentation(ruta_pptx)

    for nombre_archivo, slide_idx in imagenes:
        ruta_img = os.path.join(carpeta_imgs, nombre_archivo)

        if not os.path.exists(ruta_img):
            print(f"‚ùå No se encontr√≥ la imagen: {ruta_img}")
            continue

        if len(ppt.slides) <= slide_idx:
            print(f"‚ùå La presentaci√≥n no tiene un slide en el √≠ndice {slide_idx + 1}.")
            continue

        slide = ppt.slides[slide_idx]

        # üßΩ Eliminar im√°genes anteriores del slide
        for shape in list(slide.shapes):
            if shape.shape_type == MSO_SHAPE_TYPE.PICTURE:
                sp = shape._element
                sp.getparent().remove(sp)

        # üìå Insertar imagen con posici√≥n exacta
        slide.shapes.add_picture(
            ruta_img,
            Cm(1.15),
            Cm(4.03),
            width=Cm(31.57),
            height=Cm(11)
        )

        print(f"‚úÖ Imagen '{nombre_archivo}' insertada en slide {slide_idx + 1}.")

    ppt.save(ruta_pptx)
    print("üíæ Presentaci√≥n actualizada correctamente.")




---
# pdf_a_excel.py

import pdfplumber
import pandas as pd
import os

# Ruta al archivo PDF en tu Mac
archivo_pdf = "/Users/Arturo/Downloads/Distribuci√≥n Fertilizantes 2025 UR-partidas.pdf"

# Ruta de salida del archivo Excel (en el mismo directorio)
archivo_salida = "/Users/Arturo/Downloads/Distribucion_Fertilizantes_2025_UR-partidas.xlsx"

# Lista para almacenar todas las filas de todas las p√°ginas
todas_las_filas = []

# Abrir el PDF y extraer tablas
with pdfplumber.open(archivo_pdf) as pdf:
    for i, pagina in enumerate(pdf.pages):
        tablas = pagina.extract_tables()
        for tabla in tablas:
            for fila in tabla:
                if any(fila):  # Evitar filas completamente vac√≠as
                    todas_las_filas.append(fila)

# Crear un DataFrame
df = pd.DataFrame(todas_las_filas)

# Intentar establecer encabezados si se detectan (ajusta el √≠ndice si es otra fila)
if len(df) > 1:
    df.columns = df.iloc[1]  # Encabezado en la segunda fila
    df = df.drop([0, 1]).reset_index(drop=True)

# Guardar como archivo Excel
df.to_excel(archivo_salida, index=False)

print(f"‚úÖ Archivo Excel generado con √©xito: {archivo_salida}")



---
# recibido_por_semana_2025.py

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Autor: Arturo Herrera G.
Descripci√≥n: Calcula las toneladas recibidas por semana a partir de
             RECIBIDO POR SEMANA 2025_12062025.xlsx
"""

import os
import sys
import pandas as pd
from pathlib import Path
from datetime import date

# üîß 1) Par√°metros -----------------------------------------------------------------
RUTA_EXCEL = Path("/Users/Arturo/Downloads/RECIBIDO POR SEMANA 2025_12062025.xlsx")
RUTA_SALIDA = Path("/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/"
                   "toneladas_recibidas_por_semana_2025.csv")

COLUMNAS_REQUERIDAS = {
    "folio_del_flete",
    "abreviaci√≥n producto",
    "toneladas_en_el_destino",
    "cdf_destino_final",
    "fecha_de_entrega",
    "estado_llegada"
}

# üõ†Ô∏è 2) Funciones auxiliares -------------------------------------------------------
def validar_columnas(df: pd.DataFrame) -> None:
    """Verifica que el DataFrame contenga todas las columnas requeridas."""
    faltantes = COLUMNAS_REQUERIDAS - set(df.columns.str.lower())
    if faltantes:
        msg = (f"‚ùå Columnas faltantes: {', '.join(sorted(faltantes))}. "
               "Revisa el archivo o la lista COLUMNAS_REQUERIDAS.")
        sys.exit(msg)

def leer_archivo_excel(ruta: Path) -> pd.DataFrame:
    """Lee el Excel y devuelve un DataFrame con nombres de columna normalizados."""
    if not ruta.exists():
        sys.exit(f"‚ùå No se encontr√≥ el archivo: {ruta}")
    df = pd.read_excel(ruta, engine="openpyxl")
    df.columns = df.columns.str.lower().str.strip()
    return df

def agregar_semana(df: pd.DataFrame) -> pd.DataFrame:
    """A√±ade columna 'semana_iso' con a√±o-semana ISO (YYYY-WW)."""
    df["fecha_de_entrega"] = pd.to_datetime(df["fecha_de_entrega"], errors="coerce")
    if df["fecha_de_entrega"].isna().any():
        print("‚ö†Ô∏è Hay fechas no v√°lidas que ser√°n ignoradas.")
    df = df.dropna(subset=["fecha_de_entrega", "toneladas_en_el_destino"])
    df["toneladas_en_el_destino"] = pd.to_numeric(
        df["toneladas_en_el_destino"], errors="coerce"
    )
    df = df.dropna(subset=["toneladas_en_el_destino"])
    # a√±o ISO + semana ISO ‚Üí p.ej. 2025-24
    iso = df["fecha_de_entrega"].dt.isocalendar()
    df["semana_iso"] = iso["year"].astype(str) + "-" + iso["week"].astype(str).str.zfill(2)
    return df

def exportar_csv(df_agrupado: pd.DataFrame, ruta: Path) -> None:
    """Guarda el DataFrame en CSV UTF-8."""
    ruta.parent.mkdir(parents=True, exist_ok=True)
    df_agrupado.to_csv(ruta, index=False, encoding="utf-8-sig")
    print(f"‚úÖ Archivo exportado a: {ruta}")

# üöÄ 3) Ejecuci√≥n principal --------------------------------------------------------
def main() -> None:
    print("üìñ Leyendo archivo Excel‚Ä¶")
    df = leer_archivo_excel(RUTA_EXCEL)

    print("üîé Validando columnas‚Ä¶")
    validar_columnas(df)

    print("üìÖ Agregando columna de semana ISO‚Ä¶")
    df = agregar_semana(df)

    print("üßÆ Calculando toneladas recibidas por semana‚Ä¶")
    resumen = (
        df.groupby("semana_iso", as_index=False)["toneladas_en_el_destino"]
          .sum()
          .rename(columns={"toneladas_en_el_destino": "toneladas_recibidas"})
          .sort_values("semana_iso")
    )

    # Muestra r√°pida por consola
    print("\n=== Toneladas recibidas por semana ===")
    print(resumen.to_string(index=False))

    print("\nüíæ Exportando CSV‚Ä¶")
    exportar_csv(resumen, RUTA_SALIDA)

    print("\nüöÄ Proceso finalizado con √©xito.")

if __name__ == "__main__":
    print(f"üóìÔ∏è  Ejecuci√≥n: {date.today().isoformat()}")
    main()



---
# revisar_fechas_fletes_02042025.py

import pandas as pd
import glob
import os

# Ruta al archivo
ruta_base = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/"
archivo_fletes = glob.glob(os.path.join(ruta_base, "*FERTILIZANTES-FLETES-NACIONAL-ANUAL*.csv"))[0]

# Cargar archivo sin transformar fechas
df = pd.read_csv(archivo_fletes, dtype=str, encoding="utf-8")

# Mostrar columnas que contienen la palabra 'fecha'
columnas_fecha = [col for col in df.columns if "fecha" in col.lower()]
print(f"\nüìÖ Columnas que contienen fechas:\n{columnas_fecha}\n")

# Mostrar los primeros valores √∫nicos en cada columna de fecha
for col in columnas_fecha:
    print(f"üîé Ejemplos en columna '{col}':")
    print(df[col].dropna().unique()[:10])  # Mostrar solo los primeros 10 valores no nulos
    print("-" * 50)

print("\n‚úÖ Revisi√≥n de formatos de fecha terminada.")



---
# revisar_fechas_remanentes.py

import os
import glob
import pandas as pd

# Ajusta seg√∫n necesites
archivo_original_pattern = "*_rem.csv"            # Patron para el CSV original
archivo_corregido = "remanentes_corregido.csv"    # Nombre exacto o patr√≥n para el CSV corregido

# Columna a revisar
col_fecha = "fecha_de_salida"

def revisar_formato_fecha(df, col, date_format):
    """
    Intenta parsear la columna 'col' de df con 'date_format'.
    Devuelve un DF con las filas que NO se pudieron parsear.
    """
    # Convertimos a datetime con errors='coerce'
    parsed = pd.to_datetime(df[col], format=date_format, errors='coerce')
    
    # Mask de filas que quedaron en NaT => no se pudo parsear
    mask_falla = parsed.isna()
    df_fallas = df[mask_falla].copy()
    return df_fallas

def main():
    # 1) Buscar el archivo original con el patron archivo_original_pattern
    ruta_base = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/"
    archivos_originales = glob.glob(os.path.join(ruta_base, archivo_original_pattern))
    
    if not archivos_originales:
        print(f"No se encontr√≥ ning√∫n archivo con el patr√≥n {archivo_original_pattern}.")
        return
    
    archivo_original = archivos_originales[0]
    print(f"‚úÖ Archivo ORIGINAL encontrado: {archivo_original}")
    
    # 2) Leer el original
    df_original = pd.read_csv(archivo_original, dtype=str, encoding="utf-8")
    
    # 2A) Imprimir primeras filas de la columna fecha
    print("\n=== Valores iniciales de la columna en archivo ORIGINAL ===")
    if col_fecha in df_original.columns:
        print(df_original[col_fecha].head(20))
    else:
        print(f"La columna '{col_fecha}' no existe en el CSV original.")
    
    # 2B) Revisar formato DD/MM/YYYY
    print(f"\nRevisando formato '%d/%m/%Y' en archivo ORIGINAL para la columna '{col_fecha}'...")
    if col_fecha in df_original.columns:
        df_fallas_ori = revisar_formato_fecha(df_original, col_fecha, "%d/%m/%Y")
        
        print(f"Filas que NO pudieron parsearse en '{archivo_original}' con formato '%d/%m/%Y': {len(df_fallas_ori)}")
        if not df_fallas_ori.empty:
            # Mostramos algunas filas problem√°ticas
            print(df_fallas_ori[[col_fecha]].head(20))
    else:
        print("No se revisa el formato pues la columna no existe.")
    
    # 3) Leer el archivo corregido
    ruta_corregido = os.path.join(ruta_base, archivo_corregido)
    if not os.path.exists(ruta_corregido):
        print(f"\n‚ö†Ô∏è No se encontr√≥ el archivo corregido: {ruta_corregido}")
        return
    
    print(f"\n‚úÖ Archivo CORREGIDO encontrado: {ruta_corregido}")
    df_corregido = pd.read_csv(ruta_corregido, dtype=str, encoding="utf-8")
    
    # 3A) Imprimir primeras filas de la columna fecha
    print("\n=== Valores iniciales de la columna en archivo CORREGIDO ===")
    if col_fecha in df_corregido.columns:
        print(df_corregido[col_fecha].head(20))
    else:
        print(f"La columna '{col_fecha}' no existe en el CSV corregido.")
    
    # 3B) Revisar formato DD/MM/YYYY en archivo corregido
    print(f"\nRevisando formato '%d/%m/%Y' en archivo CORREGIDO para la columna '{col_fecha}'...")
    if col_fecha in df_corregido.columns:
        df_fallas_cor = revisar_formato_fecha(df_corregido, col_fecha, "%d/%m/%Y")
        
        print(f"Filas que NO pudieron parsearse en '{archivo_corregido}' con formato '%d/%m/%Y': {len(df_fallas_cor)}")
        if not df_fallas_cor.empty:
            # Mostramos algunas filas problem√°ticas
            print(df_fallas_cor[[col_fecha]].head(20))
    else:
        print("No se revisa el formato pues la columna no existe en el archivo corregido.")
    
    print("\n‚úÖ Revisi√≥n terminada. Si hay filas con problemas, revisa su formato o datos vac√≠os.")

if __name__ == "__main__":
    main()



---
# revisar_formato_fecha.py

import pandas as pd

archivo_csv = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/derechohabientes/1051-FERTILIZANTES-BENEFICIARIOS-NACIONAL-PRIMER CORTE_2025-03-09 14_45_47_.csv"

df = pd.read_csv(archivo_csv, encoding="utf-8", delimiter=",")

# Muestra las primeras filas y revisa los tipos de datos
print(df.head())
print(df.dtypes)




---
# revisar_formato_fecha_fletes.py

import os
import unicodedata
import pandas as pd

# Ajusta la ruta a tu archivo, por ejemplo:
ruta_csv = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/1051-FERTILIZANTES-FLETES-NACIONAL-ANUAL_2025-03-09 18_52_01_.csv"

def remover_acentos(cadena: str) -> str:
    if not isinstance(cadena, str):
        return cadena
    # Normaliza la cadena separando los acentos
    nfkd_form = unicodedata.normalize('NFD', cadena)
    # Elimina los caracteres de combinaci√≥n (acentos)
    sin_acentos = nfkd_form.encode('ascii', 'ignore').decode('utf-8', 'ignore')
    return sin_acentos

# 1) Leer el CSV
df = pd.read_csv(ruta_csv, dtype=str, encoding='utf-8')

print("\n--- Nombres de columnas ORIGINALES ---")
for col in df.columns:
    print(repr(col))

# 2) Normalizar nombres (remover acentos, min√∫sculas, _ en vez de espacio)
nuevos_nombres = []
for col in df.columns:
    col_sin_acentos = remover_acentos(col).lower().strip()
    col_sin_acentos = col_sin_acentos.replace(" ", "_")
    nuevos_nombres.append(col_sin_acentos)

df.columns = nuevos_nombres

print("\n--- Nombres de columnas DESPUES de remover acentos y espacios ---")
for col in df.columns:
    print(repr(col))

# 3) Imprimir primeros 10 valores de las columnas de fecha
for fecha_col in ["fecha_de_salida", "fecha_de_llegada", "fecha_de_entrega"]:
    if fecha_col in df.columns:
        print(f"\n--- Primeros 10 valores de la columna '{fecha_col}' ---")
        print(df[fecha_col].head(10))



---
# revisar_formato_fecha_incidentes.py

import pandas as pd
import os
import glob
import unicodedata
import re

# Recomendado: instalar dateutil si a√∫n no lo tienes
# pip install python-dateutil
from dateutil import parser

# Ajusta la ruta a tu archivo CSV de incidentes
ruta_base = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/"
file_list = glob.glob(os.path.join(ruta_base, "*-INCIDENTES-NACIONAL-ANUAL_*"))
if not file_list:
    print("‚ùå No se encontr√≥ ning√∫n archivo con la m√°scara '*-INCIDENTES-NACIONAL-ANUAL_*'")
    exit()

file_path = file_list[0]

def normalizar_columna(col):
    col = col.strip().lower()
    col = ''.join(
        c for c in unicodedata.normalize('NFD', col)
        if unicodedata.category(c) != 'Mn'
    )
    col = re.sub(r'[^a-z0-9]+', '_', col)
    col = re.sub(r'[_]+', '_', col).strip('_')
    return col

print(f"‚ÑπÔ∏è Revisando archivo: {file_path}")

# 1) Leer el CSV con todo como texto
df = pd.read_csv(file_path, dtype=str, encoding='utf-8', delimiter=',')

# 2) Normalizar nombre de columnas
df.columns = [normalizar_columna(c) for c in df.columns]

col_fecha = "fecha_incidente"
if col_fecha not in df.columns:
    print(f"‚ö†Ô∏è La columna '{col_fecha}' no existe en el CSV.")
    exit()

# 3) Ver cu√°ntos valores hay y muestra algunos
num_filas = len(df)
print(f"‚úÖ Se leyeron {num_filas} filas total.")
print(f"üéØ Valores √∫nicos en '{col_fecha}':\n{df[col_fecha].unique()}")
print("\n--- INTENTO DE PARSEO CON 'dateutil.parser.parse' ---")

errores = []
exitos = 0

# 4) Recorrer todas o un subset (si tu archivo es muy grande, usa .head(50) o similar)
for i, valor_str in enumerate(df[col_fecha]):  
    if i >= 50:
        # Muestra solo los primeros 50 para no saturar la salida
        break
    
    try:
        if pd.isna(valor_str):
            # Si la celda es NaN (pandas) o None, no se puede parsear
            print(f"Fila {i}: valor='{valor_str}' -> Es nulo/NaN, no se puede parsear.")
            errores.append(valor_str)
        else:
            dt = parser.parse(valor_str)  # dateutil intenta distintos formatos
            print(f"Fila {i}: valor='{valor_str}' -> Se parsea correctamente como: {dt}")
            exitos += 1
    except Exception as e:
        print(f"Fila {i}: valor='{valor_str}' -> ‚ùå Error: {e}")
        errores.append(valor_str)

print("\n--- Resumen del intento de parseo ---")
print(f" - Filas parseadas con √©xito: {exitos}")
print(f" - Filas con error/nulo en '{col_fecha}': {len(errores)}")
if errores:
    print("‚ùå Ejemplos de valores problem√°ticos:")
    print(errores[:10])  # muestra los primeros 10 con error



---
# revisar_formato_fecha_remanentes.py

import os
import unicodedata
import pandas as pd

# Ajusta la ruta a tu archivo, por ejemplo:
ruta_csv = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/1051-FERTILIZANTES-PEDIDOS DESGLOSE-NACIONAL-ANUAL_2025-03-22 15_26_45_.csv"

def remover_acentos(cadena: str) -> str:
    if not isinstance(cadena, str):
        return cadena
    # Normaliza la cadena separando los acentos
    nfkd_form = unicodedata.normalize('NFD', cadena)
    # Elimina los caracteres de combinaci√≥n (acentos)
    sin_acentos = nfkd_form.encode('ascii', 'ignore').decode('utf-8', 'ignore')
    return sin_acentos

# 1) Leer el CSV
df = pd.read_csv(ruta_csv, dtype=str, encoding='utf-8')

print("\n--- Nombres de columnas ORIGINALES ---")
for col in df.columns:
    print(repr(col))

# 2) Normalizar nombres (remover acentos, min√∫sculas, _ en vez de espacio)
nuevos_nombres = []
for col in df.columns:
    col_sin_acentos = remover_acentos(col).lower().strip()
    col_sin_acentos = col_sin_acentos.replace(" ", "_")
    nuevos_nombres.append(col_sin_acentos)

df.columns = nuevos_nombres

print("\n--- Nombres de columnas DESPUES de remover acentos y espacios ---")
for col in df.columns:
    print(repr(col))

# 3) Imprimir primeros 10 valores de las columnas de fecha
for fecha_col in ["fecha"]:
    if fecha_col in df.columns:
        print(f"\n--- Primeros 10 valores de la columna '{fecha_col}' ---")
        print(df[fecha_col].head(10))



---
# revisar_formato_fecha_transferencias.py

import os
import unicodedata
import pandas as pd

# Ajusta la ruta a tu archivo, por ejemplo:
ruta_csv = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/1051-FERTILIZANTES-TRANSFERENCIAS-NACIONAL-ANUAL_2025-03-08 14_33_22_rem.csv"

def remover_acentos(cadena: str) -> str:
    if not isinstance(cadena, str):
        return cadena
    # Normaliza la cadena separando los acentos
    nfkd_form = unicodedata.normalize('NFD', cadena)
    # Elimina los caracteres de combinaci√≥n (acentos)
    sin_acentos = nfkd_form.encode('ascii', 'ignore').decode('utf-8', 'ignore')
    return sin_acentos

# 1) Leer el CSV
df = pd.read_csv(ruta_csv, dtype=str, encoding='utf-8')

print("\n--- Nombres de columnas ORIGINALES ---")
for col in df.columns:
    print(repr(col))

# 2) Normalizar nombres (remover acentos, min√∫sculas, _ en vez de espacio)
nuevos_nombres = []
for col in df.columns:
    col_sin_acentos = remover_acentos(col).lower().strip()
    col_sin_acentos = col_sin_acentos.replace(" ", "_")
    nuevos_nombres.append(col_sin_acentos)

df.columns = nuevos_nombres

print("\n--- Nombres de columnas DESPUES de remover acentos y espacios ---")
for col in df.columns:
    print(repr(col))

# 3) Imprimir primeros 10 valores de las columnas de fecha
for fecha_col in ["fecha_de_salida", "fecha_de_llegada"]:
    if fecha_col in df.columns:
        print(f"\n--- Primeros 10 valores de la columna '{fecha_col}' ---")
        print(df[fecha_col].head(10))



---
# revisar_json_mapa_mexico.py

import json

geojson_path = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/MAPAS/mexico.json"

with open(geojson_path, "r", encoding="utf-8") as f:
    data = json.load(f)

# Mostrar las primeras claves
print("Claves principales:", data.keys())

# Ver cu√°ntos elementos hay
print("N√∫mero de features:", len(data.get("features", [])))

# Mostrar las claves de la primera entidad
if "features" in data and len(data["features"]) > 0:
    print("Ejemplo de propiedades del primer estado:")
    print(data["features"][0]["properties"])



---
# script_generico_reusable.py

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import pandas as pd
from datetime import datetime

# Ruta y nombre base (sin extensi√≥n)
RUTA_CARPETA = "/Users/Arturo/AGRICULTURA/2026/ANALISIS SECRETARIO/PFB"
BASE_NOMBRE = "Beneficiarios PPB y FPB 2024 VS GAMSA"

# Configuraci√≥n de salida
CODIFICACION_CSV = "utf-8-sig"  # UTF-8 con BOM, compatible con Excel es-MX
SEPARADOR = ","                  # Cambia a ";" si tu Excel lo requiere
EXTENSIONES = (".xlsx", ".xlsm", ".xls")

def encontrar_archivo(carpeta: str, base_nombre: str) -> str:
    candidatos = []
    for ext in EXTENSIONES:
        ruta = os.path.join(carpeta, base_nombre + ext)
        if os.path.exists(ruta):
            candidatos.append(ruta)
    if not candidatos:
        raise FileNotFoundError(f"No se encontr√≥ '{base_nombre}' con extensiones {EXTENSIONES} en {carpeta}")
    if len(candidatos) == 1:
        return candidatos[0]
    # Si hay varias versiones, usar la m√°s reciente por fecha de modificaci√≥n
    candidatos.sort(key=lambda p: os.path.getmtime(p), reverse=True)
    return candidatos[0]

def main():
    carpeta = os.path.abspath(os.path.expanduser(RUTA_CARPETA))
    ruta_excel = encontrar_archivo(carpeta, BASE_NOMBRE)

    # Leer primera hoja
    xls = pd.ExcelFile(ruta_excel)
    if not xls.sheet_names:
        raise ValueError("El archivo no contiene hojas.")
    df = pd.read_excel(xls, sheet_name=xls.sheet_names[0], dtype=object)

    # Guardar CSV con el mismo nombre base
    ruta_csv = os.path.join(carpeta, BASE_NOMBRE + ".csv")
    df.to_csv(ruta_csv, index=False, encoding=CODIFICACION_CSV, sep=SEPARADOR)

if __name__ == "__main__":
    main()


---
# separar_pdf_por_estado.py

from pathlib import Path
from pypdf import PdfReader, PdfWriter
import re
from collections import defaultdict
import os
from tqdm import tqdm

# Rutas
PDF_INPUT = Path("/Users/Arturo/AGRICULTURA/FERTILIZANTES/TABLAS_DINAMICAS/MANIFESTACION_PROTESTA.pdf")
OUTPUT_DIR = PDF_INPUT.parent / "manifestaciones_por_estado"
OUTPUT_DIR.mkdir(exist_ok=True)

# Expresi√≥n regular para detectar el estado antes de la fecha
patron_estado = re.compile(r"([A-Z√Å√â√ç√ì√ö√ëa-z√°√©√≠√≥√∫√± ]+?) a 01 de abril de 2025")

# Lectura del PDF
reader = PdfReader(str(PDF_INPUT))
writers = defaultdict(PdfWriter)

for i, page in enumerate(tqdm(reader.pages, desc="üìÑ Procesando p√°ginas")):
    texto = page.extract_text() or ""
    match = patron_estado.search(texto)
    estado = match.group(1).strip().title().replace(" ", "_") if match else "estado_desconocido"
    writers[estado].add_page(page)

# Guardar archivos por estado
for estado, writer in writers.items():
    archivo = OUTPUT_DIR / f"{estado}_protestas.pdf"
    with open(archivo, "wb") as f:
        writer.write(f)
    print(f"‚úÖ {estado}: {len(writer.pages)} p√°ginas ‚Üí {archivo}")



---
# test_conection.py

import psycopg2

# Configuraci√≥n de la base de datos
DB_HOST = "localhost"
DB_NAME = "fertilizantes"
DB_USER = "postgres"
DB_PASSWORD = "Art4125r0"
DB_PORT = "5432"  # Puerto por defecto de PostgreSQL

try:
    # Intentar conexi√≥n a la base de datos
    conn = psycopg2.connect(
        host=DB_HOST,
        database=DB_NAME,
        user=DB_USER,
        password=DB_PASSWORD,
        port=DB_PORT
    )
    print("‚úÖ Conexi√≥n exitosa a PostgreSQL")
    conn.close()
except Exception as e:
    print(f"‚ùå Error en la conexi√≥n: {e}")


---
# test_sheets.py

#!/usr/bin/env python3
import pandas as pd
import hashlib
import datetime
import re
import unicodedata
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
from sqlalchemy import text
from conexion import engine  # tu conexi√≥n centralizada

# --- Configuraci√≥n ------------------------------------------------------
CREDS_FILE     = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/SCRIPTS/inventarios-2025-pf-69b1417ea0df.json"
SCOPES         = ["https://www.googleapis.com/auth/spreadsheets.readonly"]
SPREADSHEET_ID = "1YLmnolQ4TPab8LcVcAHTTqgYECddMfNAJGqohvBPZLM"
# Ahora incluye hasta la columna AS para observaciones y otra_persona
RANGE          = "'Respuestas de formulario 1'!A1:AS"
TABLE_NAME     = "inventarios_diarios_ceda_2025_campo"


def normalize_columns(cols):
    new_cols = []
    for col in cols:
        c = col.lower().strip()
        c = unicodedata.normalize('NFD', c).encode('ascii', 'ignore').decode('utf-8')
        c = re.sub(r"[^\w\s]", '', c)
        c = re.sub(r"\s+", '_', c)
        c = re.sub(r"_+", '_', c)
        new_cols.append(c)
    return new_cols


def main():
    # 1) Autenticaci√≥n con Google Sheets API
    creds = Credentials.from_service_account_file(CREDS_FILE, scopes=SCOPES)
    service = build("sheets", "v4", credentials=creds)

    # 2) Obtener datos brutos
    resp = service.spreadsheets().values().get(
        spreadsheetId=SPREADSHEET_ID,
        range=RANGE
    ).execute()
    values = resp.get("values", [])
    if not values:
        print("‚ö†Ô∏è No hay datos para importar.")
        return

    # 3) Encabezados y filas
    headers, rows = values[0], values[1:]
    n_cols = len(headers)
    padded_rows = [row + [''] * (n_cols - len(row)) for row in rows]
    df = pd.DataFrame(padded_rows, columns=headers)

    # 4) Normalizar nombres de columnas
    df.columns = normalize_columns(df.columns)

    # 5) Consolidar columnas de selecci√≥n de CEDA
    centro_cols = [col for col in df.columns if col.startswith('selecciona_tu_centro_de_distribucion')]
    df['id_ceda_agricultura'] = df[centro_cols].replace('', pd.NA).bfill(axis=1).iloc[:, 0]
    df.drop(columns=centro_cols, inplace=True)

    # 6) Renombrar campos fijos y nuevos
    rename_map = {
        'marca_temporal': 'fecha_y_hora',
        'direccion_de_correo_electronico': 'correo_electronico',
        'selecciona_la_fecha_del_dia_a_registrar': 'fecha_registro',
        'selecciona_tu_estado': 'estado',
        'quien_o_quienes_fueron_los_responsables_de_realizar_el_levantamiento_del_inventario': 'quien_reporta',
        'nombre_del_coz': 'nombre_de_coz',
        'nombre_del_rc': 'nombre_del_rc',
        'nombre_del_ce': 'nombre_del_ce',
        'numero_de_bultos_de_dap_en_el_centro_de_distribucion': 'bultos_dap',
        'numero_de_bultos_de_urea_en_el_centro_de_distribucion': 'bultos_urea',
        'observaciones': 'observaciones',
        'nombre_de_la_persona_que_realizo_el_levantamiento': 'otra_persona'
    }
    df.rename(columns=rename_map, inplace=True)

    # 7) Convertir tipos de bultos
    if 'bultos_dap' in df.columns:
        df['bultos_dap'] = pd.to_numeric(df['bultos_dap'], errors='coerce').fillna(0).astype(int)
    if 'bultos_urea' in df.columns:
        df['bultos_urea'] = pd.to_numeric(df['bultos_urea'], errors='coerce').fillna(0).astype(int)

    # 8) Convertir fechas
    if 'fecha_y_hora' in df.columns:
        df['fecha_y_hora'] = pd.to_datetime(df['fecha_y_hora'], errors='coerce')
    if 'fecha_registro' in df.columns:
        df['fecha_registro'] = pd.to_datetime(df['fecha_registro'], errors='coerce').dt.date

    # 9) Seleccionar columnas finales
    cols_to_keep = [
        'fecha_y_hora', 'correo_electronico', 'fecha_registro', 'estado',
        'id_ceda_agricultura', 'bultos_dap', 'bultos_urea',
        'quien_reporta', 'nombre_de_coz', 'nombre_del_rc', 'nombre_del_ce',
        'observaciones', 'otra_persona'
    ]
    missing = [c for c in cols_to_keep if c not in df.columns]
    if missing:
        print(f"‚ö†Ô∏è Columnas faltantes tras procesamiento: {missing}")
    df_final = df[cols_to_keep]

    # 10) Generar hash de fila
    df_final['hash_respuesta'] = (
        df_final.astype(str).agg('|'.join, axis=1)
          .apply(lambda x: hashlib.md5(x.encode()).hexdigest())
    )

    # 11) Insertar en PostgreSQL y eliminar duplicados
    with engine.begin() as conn:
        df_final.to_sql(
            name=TABLE_NAME,
            con=conn,
            if_exists='append',
            index=False,
            method='multi',
            chunksize=5000
        )
        conn.execute(text(f"""
            DELETE FROM {TABLE_NAME} a
            USING {TABLE_NAME} b
            WHERE a.ctid < b.ctid AND a.hash_respuesta = b.hash_respuesta;
        """))

    # 12) Log de ejecuci√≥n
    ahora = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"‚úÖ Importadas {len(df_final)} filas en '{TABLE_NAME}' ‚Äî {ahora}")

if __name__ == "__main__":
    main()



---
# truncar_tablas.py

from conexion import engine
from sqlalchemy import text  # solo si necesitas ejecutar SQL directamente

# Listado de tablas en orden para limpiar
tablas_ordenadas = [
    "derechohabientes",
    "fletes",
    "transferencias",
    "remanentes",
    "pedidos_desglosado",
    "pedidos_sigap",
    "incidentes",
    "red_distribucion"  # Esta debe ir al final
]

try:
    with engine.begin() as conn:
        print("üö® Iniciando limpieza de tablas...")
        for tabla in tablas_ordenadas:
            print(f"üßπ Limpiando: {tabla}...")
            conn.execute(text(f"TRUNCATE {tabla} CASCADE;"))
        print("‚úÖ Todas las tablas fueron limpiadas correctamente.")

except Exception as e:
    print(f"‚ùå Error durante la limpieza: {e}")


---
# verificar_columnas_fechas_fletes.py

import os
import glob
import unicodedata
import pandas as pd
import psycopg2
from sqlalchemy import create_engine
from sqlalchemy.types import Text, Integer, Numeric, TIMESTAMP, Date

# Funci√≥n para remover acentos
def remover_acentos(cadena):
    if not isinstance(cadena, str):
        return cadena
    nfkd_form = unicodedata.normalize('NFD', cadena)
    return nfkd_form.encode('ASCII', 'ignore').decode('utf-8', 'ignore')

# -----------------------------------------------------------------------------
# 1. Configuraci√≥n de acceso a la base de datos PostgreSQL
# -----------------------------------------------------------------------------
DB_USER = "postgres"
DB_PASSWORD = "Art4125r0"
DB_HOST = "localhost"
DB_PORT = "5432"
DB_NAME = "fertilizantes"

connection_url = f"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}"
engine = create_engine(connection_url)

# -----------------------------------------------------------------------------
# 2. Rutas de archivos
# -----------------------------------------------------------------------------
ruta_base = "/Users/Arturo/AGRICULTURA/FERTILIZANTES/BASES_ORIGINALES_SIGAP/"
archivo_fletes = glob.glob(os.path.join(ruta_base, "*FERTILIZANTES-FLETES-NACIONAL-ANUAL*.csv"))[0]
archivo_eliminar = os.path.join(ruta_base, "eliminar_fletes.xlsx")
archivo_corregidos = os.path.join(ruta_base, "fletes_corregidos.csv")

# -----------------------------------------------------------------------------
# 3. Lectura del archivo principal de fletes
# -----------------------------------------------------------------------------
fletes_df = pd.read_csv(archivo_fletes, dtype=str, encoding="utf-8")

# 3A. Normalizar nombres de columnas (quitar acentos, min√∫sculas, _ en vez de espacio)
nuevos_nombres = []
for col in fletes_df.columns:
    col_sin_acentos = remover_acentos(col).lower().strip()  # quita acentos, pasa a min√∫sculas
    col_sin_acentos = col_sin_acentos.replace(" ", "_")      # reemplaza espacios con _
    nuevos_nombres.append(col_sin_acentos)

fletes_df.columns = nuevos_nombres

# -----------------------------------------------------------------------------
# 4. Eliminar registros seg√∫n "eliminar_fletes.xlsx"
# -----------------------------------------------------------------------------
if os.path.exists(archivo_eliminar):
    eliminar_df = pd.read_excel(archivo_eliminar, dtype=str)
    # Normalizar tambi√©n nombres en eliminar_df
    cols_eliminar = []
    for col in eliminar_df.columns:
        col_norm = remover_acentos(col).lower().strip().replace(" ", "_")
        cols_eliminar.append(col_norm)
    eliminar_df.columns = cols_eliminar
    
    if "folio_del_flete" in eliminar_df.columns and "folio_del_flete" in fletes_df.columns:
        antes_eliminar = len(fletes_df)
        fletes_df = fletes_df[~fletes_df["folio_del_flete"].isin(eliminar_df["folio_del_flete"])]
        despues_eliminar = len(fletes_df)
        print(f"‚úÖ Se eliminaron {antes_eliminar - despues_eliminar} registros basados en 'eliminar_fletes.xlsx'.")
    else:
        print("‚ö†Ô∏è No se encontr√≥ la columna 'folio_del_flete' en alguno de los DataFrames.")
else:
    print("‚ö†Ô∏è No se encontr√≥ el archivo 'eliminar_fletes.xlsx'. No se eliminaron registros.")

# -----------------------------------------------------------------------------
# 5. A√±adir registros corregidos (si existen)
# -----------------------------------------------------------------------------
if os.path.exists(archivo_corregidos):
    corregidos_df = pd.read_csv(archivo_corregidos, dtype=str, encoding="utf-8")
    # Normalizar tambi√©n los nombres de columnas
    cols_correg = []
    for col in corregidos_df.columns:
        col_norm = remover_acentos(col).lower().strip().replace(" ", "_")
        cols_correg.append(col_norm)
    corregidos_df.columns = cols_correg
    
    antes_corregidos = len(fletes_df)
    fletes_df = pd.concat([fletes_df, corregidos_df], ignore_index=True)
    despues_corregidos = len(fletes_df)
    
    print(f"‚úÖ Se a√±adieron {despues_corregidos - antes_corregidos} registros corregidos.")
else:
    print("‚ö†Ô∏è No se encontr√≥ el archivo 'fletes_corregidos.csv'. No se agregaron registros corregidos.")

# -----------------------------------------------------------------------------
# 6. Manejo de fechas
#    NOTA: Los valores son 'YYYY-MM-DD HH:MM:SS' (verificado por tu script).
#    fecha_de_salida => date
#    fecha_de_llegada y fecha_de_entrega => timestamp
# -----------------------------------------------------------------------------
if "fecha_de_salida" in fletes_df.columns:
    # Parseamos con el formato correcto y nos quedamos con la parte de fecha
    fletes_df["fecha_de_salida"] = pd.to_datetime(
        fletes_df["fecha_de_salida"],
        format="%Y-%m-%d %H:%M:%S",
        errors="coerce"
    ).dt.date

if "fecha_de_llegada" in fletes_df.columns:
    fletes_df["fecha_de_llegada"] = pd.to_datetime(
        fletes_df["fecha_de_llegada"],
        format="%Y-%m-%d %H:%M:%S",
        errors="coerce"
    )

# Cuando estatus es 'autorizado', forzamos fecha_de_entrega a NaT (antes de parsear)
if "estatus" in fletes_df.columns and "fecha_de_entrega" in fletes_df.columns:
    mask_autorizado = fletes_df["estatus"].str.lower() == "autorizado"
    fletes_df.loc[mask_autorizado, "fecha_de_entrega"] = None

if "fecha_de_entrega" in fletes_df.columns:
    fletes_df["fecha_de_entrega"] = pd.to_datetime(
        fletes_df["fecha_de_entrega"],
        format="%Y-%m-%d %H:%M:%S",
        errors="coerce"
    )

# -----------------------------------------------------------------------------
# 7. Ajuste de tipos (num√©ricos, texto, etc.)
# -----------------------------------------------------------------------------
numeric_cols_3_decimals = ["toneladas_iniciales", "toneladas_en_el_destino", "toneladas_con_incidentes"]
for col in numeric_cols_3_decimals:
    if col in fletes_df.columns:
        fletes_df[col] = pd.to_numeric(fletes_df[col], errors="coerce").round(3)

if "ticket_bascula" in fletes_df.columns:
    fletes_df["ticket_bascula"] = fletes_df["ticket_bascula"].astype(str)

if "telefono_operador" in fletes_df.columns:
    fletes_df["telefono_operador"] = fletes_df["telefono_operador"].astype(str)

# -----------------------------------------------------------------------------
# 8. Creaci√≥n de 'id_ceda_agricultura'
# -----------------------------------------------------------------------------
if "id_ceda_agricultura" not in fletes_df.columns:
    fletes_df["id_ceda_agricultura"] = None

if "estatus" in fletes_df.columns:
    # ENTREGADO => cdf_destino_final
    if "cdf_destino_final" in fletes_df.columns:
        mask_entregado = fletes_df["estatus"].str.lower() == "entregado"
        fletes_df.loc[mask_entregado, "id_ceda_agricultura"] = fletes_df["cdf_destino_final"]
    # AUTORIZADO => cdf_destino_original
    if "cdf_destino_original" in fletes_df.columns:
        mask_autorizado = fletes_df["estatus"].str.lower() == "autorizado"
        fletes_df.loc[mask_autorizado, "id_ceda_agricultura"] = fletes_df["cdf_destino_original"]

# -----------------------------------------------------------------------------
# 9. Vista previa
# -----------------------------------------------------------------------------
print("\nEjemplo de algunos registros post-limpieza:")
columnas_mostrar = [
    "folio_del_flete", "estatus", "fecha_de_salida",
    "fecha_de_llegada", "fecha_de_entrega", "cdf_destino_original",
    "cdf_destino_final", "id_ceda_agricultura"
]
cols_existen = [c for c in columnas_mostrar if c in fletes_df.columns]
print(fletes_df[cols_existen].head(5))

if any(c in fletes_df.columns for c in ["fecha_de_salida","fecha_de_llegada","fecha_de_entrega"]):
    print("\nConteo de valores nulos en las columnas de fecha:")
    for c in ["fecha_de_salida","fecha_de_llegada","fecha_de_entrega"]:
        if c in fletes_df.columns:
            print(f"  {c}: {fletes_df[c].isnull().sum()}")

# (Opcional) CSV temporal
archivo_limpio = os.path.join(ruta_base, "fletes_limpios_para_importar.csv")
fletes_df.to_csv(archivo_limpio, index=False, encoding="utf-8")
print(f"\nüöÄ CSV temporal guardado en: {archivo_limpio}")

# -----------------------------------------------------------------------------
# 10. Diccionario de tipos con los nombres YA normalizados
# -----------------------------------------------------------------------------
dtype_sqlalchemy = {
    "folio_del_flete": Text,
    "estatus": Text,
    "producto": Text,
    "abreviacion_producto": Text,  # <<---- SIN acento
    "toneladas_iniciales": Numeric(10, 3),
    "bultos_iniciales": Integer,
    "bultos_en_destino": Integer,
    "ticket_bascula": Text,
    "estado_procedencia": Text,
    "toneladas_en_el_destino": Numeric(10, 3),
    "fecha_de_salida": Date,
    "cdf_destino_original": Text,
    "cdf_destino_final": Text,
    "fecha_de_llegada": TIMESTAMP,
    "fecha_de_entrega": TIMESTAMP,
    "toneladas_con_incidentes": Numeric(10, 3),
    "estatus_de_recepcion_incidente": Text,
    "descripcion": Text,
    "destino_final": Text,
    "nombre_operador": Text,
    "telefono_operador": Text,
    "placas_transporte": Text,
    "tipo_transporte": Text,
    "estado_llegada": Text,
    "bultos_por_anio": Integer,   # igualmente normalizado
    "id_ceda_agricultura": Text
}

# -----------------------------------------------------------------------------
# 11. Insertar en PostgreSQL
# -----------------------------------------------------------------------------
table_name = "fletes"
fletes_df.to_sql(
    name=table_name,
    con=engine,
    if_exists="append",  # o "replace" si quieres crear de cero
    index=False,
    dtype=dtype_sqlalchemy
)

print(f"\n‚úÖ Datos insertados/actualizados en la tabla '{table_name}' de la base '{DB_NAME}'.")


---
# verificar_librerias.py

import xlwings as xw
from PIL import Image
from pptx import Presentation

print("‚úÖ xlwings est√° instalado")
print("‚úÖ pillow (PIL) est√° instalado")
print("‚úÖ python-pptx est√° instalado")

